<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ashishmathew0297.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ashishmathew0297.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-20T19:35:07+00:00</updated><id>https://ashishmathew0297.github.io/feed.xml</id><title type="html">blank</title><subtitle>Here I put down my points of view/learning processes as a data scientist. </subtitle><entry><title type="html">Deep Learning - Basics of Computer Vision (ConvNets)</title><link href="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-3/" rel="alternate" type="text/html" title="Deep Learning - Basics of Computer Vision (ConvNets)"/><published>2024-12-05T18:49:00+00:00</published><updated>2024-12-05T18:49:00+00:00</updated><id>https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-3</id><content type="html" xml:base="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-3/"><![CDATA[<p>Simple Artificial Neural Networks (ANNs), though powerful by themselves face a hard limit to how effective they can be when working with specialized domains such as computer vision, speech recognition and so on. Such implementations require completely different architectures, concepts and implementations, and, as has been discussed before, intuitions for these domains do not always cross pollinate.</p> <p>This blog will cover one such domain that has been of great interest to me for the longest time: <strong>Computer Vision</strong>. A lot of the content of this blog is based off papers written by the original authors with the main structure of the blog being highly inspired by the <a href="https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning">Convolutional Neural Networks</a> portion of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a> by Andrew Ng. The images in this blog were either made by hand, in <a href="https://app.diagrams.net/">draw.io</a>, taken from their respective research papers, or were generated through python. Without wasting any more time, let’s begin.</p> <hr/> <h1 id="what-is-computer-vision">What is Computer Vision?</h1> <p>Computer Vision (CV) is one of the most intuitive and rapidly advancing fields of deep learning in the modern day. The basic idea of computer vision is to allow devices to be able to observe and identify images and/or aspects of images and videos, trying to mimic how a human’s ocular nervous system would do the same.</p> <p>As groundbreaking as it is, it would surprise you to know that computer vision research is not as novel as one would think it to be. The ideas have been around since the early 60s, with the first foray into this field being through neurophysical <a class="citation" href="#Hoffmann1984">(Hoffmann &amp; Von Seelen, 1984)</a> experimentation son cats. It is even more intriguing to know that the ideas that worked towards bringing about the discussion of AI stems from the ideas of mechanical autonomy and intelligent machines almost 3000 years ago. Homer’s Odyssey is a prime example of literature at the time that brushes upon the idea of intelligent machines <a class="citation" href="#Lively2020">(Liveley &amp; Thomas, 2020)</a>.</p> <p>Finding use in several domains such as medical imaging, autonomous vehicles, quality control and augmented reality, the rapid advancements in computer vision have pushed us towards new horizons that we would never have fathomed before. Some popular implementations of computer vision are as follows:</p> <ul> <li><strong>Image Recognition</strong>: Image recognition is one of the most common computer vision problems. This classification problem involves training specifically designed neural network models on several examples of labelled image inputs, learning different aspects of it, with the end goal of determining the identity of new and unknown examples.</li> <li><strong>Object Detection</strong>: Image recognition and object detection are similar techniques but differ completely in their applications. While image recognition identifies an object in an image, object detection is used to detect and locate instances of objects in sample images or videos. Here, the models create bounding boxes at the position and boundaries of the objects we want to detect. Image recognition is often used alongside object detection in several applications.</li> <li><strong>Image Segmentation</strong>: Image segmentation can be seen as a more refined form of object detection and image classification. Here, the models process the visual data in the image at a pixel level. Classical image segmentation methods take into account the heuristics of the pixels in the image such as the color, position, and intensity. These are used in neural networks to generate more sophisticated results. The outputs of segmentation are segmentation masks which act as pixel-to-pixel boundaries around the objects’ shape for each class of object being identified.</li> </ul> <p>There are several other applications of computer vision in addition to the above, with neural style transfer, autonomous vehicles, augmented reality and facial recognition being just a few of their implementations.</p> <h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1> <h2 id="foundations-of-convolutional-neural-networks">Foundations of Convolutional Neural Networks</h2> <p>The main issue that computer vision problems pose when using normal deep neural networks is the fact that, as the dimensions of the images being used increases, the number of input features we will have to deal with becomes proportionally larger.</p> <p>As a result it is difficult to get enough data to prevent overfitting. In addition, this will also lead to a large computational requirement when training the network.</p> <p>The solution for this is the <strong>convolution operation</strong>. This is the most building block of a significant number of computer vision models, also known as <strong>convolutional neural networks (CNNs)</strong>.</p> <h2 id="the-convolution-operation">The Convolution Operation</h2> <p>The best explanations of the convolution operation in signal processing that I have seen are from this video by <a href="https://www.youtube.com/@3blue1brown">3Blue1Brown</a>.</p> <div class="row mt-1"> <div class="col-sm mt-1 mt-md-0"> <iframe width="560" height="315" src="https://www.youtube.com/embed/KuXjwB4LzSA?si=ozem701wKPFMVaac" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> <p><br/></p> <p>There are several ways of picturing what a convolution is and how it works. It even ties into moving averages which was discussed in the <a href="/blog/2024/deep-learning-self-study-2/">previous blog</a>.</p> <p>The official formulaic definition for the convolution of two different functions is as follows</p> \[(f*g)(t) = \int_{-\infty}^{\infty} f(x)g(t-x)\] <p>However this formula tends to scare off most people who do not know what is going on exactly. This is where different intuitions on this help.</p> <p>We will be looking at convolutions from the standpoint of computer vision, where they play the role of filtering images for various uses such as smoothing, sharpening, denoising or detecting edges. This is done so with the help of a convolution <strong>filter</strong> or <strong>kernel</strong> which moves across the pixels in the image, multiplying the values and adding them up, giving us a different version of the same image.</p> <h2 id="an-intuition-vertical-edge-detection">An intuition: Vertical Edge Detection</h2> <p>As has been mentioned before, the good way to understand how convolution works is in the field of computer vision, through the example of a <strong>filter</strong>.</p> <p>In this case we are making use of an edge detector, specifically a vertical edge detector, which we will look at after this section.</p> <p>Consider a \(6\times 6\) greyscale image, which is effectively a 2 dimensional matrix of dimensions \((6,6,1)\) as shown below</p> \[\begin{bmatrix} 3 &amp; 0 &amp; 1 &amp; 2 &amp; 7 &amp; 4 \\ 1 &amp; 5 &amp; 8 &amp; 9 &amp; 3 &amp; 1 \\ 2 &amp; 7 &amp; 2 &amp; 5 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix}\] <p>For this image we, will be applying a vertical edge detection filter which is of the form</p> \[\begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix}\] <p>to it. The convolving setup is as follows.</p> \[\begin{bmatrix} 3 &amp; 0 &amp; 1 &amp; 2 &amp; 7 &amp; 4 \\ 1 &amp; 5 &amp; 8 &amp; 9 &amp; 3 &amp; 1 \\ 2 &amp; 7 &amp; 2 &amp; 5 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ \end{bmatrix}\] <p>where the output is a \(4 \times 4\) array or image.</p> <p>Now, we want to apply the convolution to the image with the given filter, the computation process of which is as follows:</p> <p>We first perform the convolution on the upper left corner of the image by pasting the given \(3 \times 3\) filter on the upper left \(3 \times 3\) region of the original input image. We then perform the element-wise product and add up all the \(9\) resulting numbers to get the first pixel value for our convoluted result.</p> \[\begin{bmatrix} \colorbox{grey}{3^{1}} &amp; \colorbox{grey}{0^{0}} &amp; \colorbox{grey}{1^{-1}} &amp; 2 &amp; 7 &amp; 4 \\ \colorbox{grey}{1^{1}} &amp; \colorbox{grey}{5^{0}} &amp; \colorbox{grey}{8^{-1}} &amp; 9 &amp; 3 &amp; 1 \\ \colorbox{grey}{2^{1}} &amp; \colorbox{grey}{7^{0}} &amp; \colorbox{grey}{2^{-1}} &amp; 5 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} \colorbox{grey}{x} &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ \end{bmatrix}\] \[\begin{align*} \Rightarrow x = \;\; &amp; (3 \times 1) + (0 \times 0) + (1 \times (-1))\\ &amp; + (1 \times 1) + (5 \times 0) + (8 \times (-1))\\ &amp; + (2 \times 1) + (7 \times 0) + (2 \times (-1)) \end{align*}\] <p>Similarly, for the second, third and fourth elements, the same process is repeated:</p> \[\begin{bmatrix} 3 &amp; \colorbox{grey}{0^{1}} &amp; \colorbox{grey}{1^{0}} &amp; \colorbox{grey}{2^{-1}} &amp; 7 &amp; 4 \\ 1 &amp; \colorbox{grey}{5^{1}} &amp; \colorbox{grey}{8^{0}} &amp; \colorbox{grey}{9^{-1}} &amp; 3 &amp; 1 \\ 2 &amp; \colorbox{grey}{7^{1}} &amp; \colorbox{grey}{2^{0}} &amp; \colorbox{grey}{5^{-1}} &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} -5 &amp; \colorbox{grey}{-4} &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ \end{bmatrix}\] \[\begin{bmatrix} 3 &amp; 0 &amp; \colorbox{grey}{1^{1}} &amp; \colorbox{grey}{2^{0}} &amp; \colorbox{grey}{7^{-1}} &amp; 4 \\ 1 &amp; 5 &amp; \colorbox{grey}{8^{1}} &amp; \colorbox{grey}{9^{0}} &amp; \colorbox{grey}{3^{-1}} &amp; 1 \\ 2 &amp; 7 &amp; \colorbox{grey}{2^{1}} &amp; \colorbox{grey}{5^{0}} &amp; \colorbox{grey}{1^{-1}} &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} -5 &amp; -4 &amp; \colorbox{grey}{0} &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ \end{bmatrix}\] \[\begin{bmatrix} 3 &amp; 0 &amp; 1 &amp; \colorbox{grey}{2^{1}} &amp; \colorbox{grey}{7^{0}} &amp; \colorbox{grey}{4^{-1}} \\ 1 &amp; 5 &amp; 8 &amp; \colorbox{grey}{9^{1}} &amp; \colorbox{grey}{3^{0}} &amp; \colorbox{grey}{1^{-1}} \\ 2 &amp; 7 &amp; 2 &amp; \colorbox{grey}{5^{1}} &amp; \colorbox{grey}{1^{0}} &amp; \colorbox{grey}{3^{-1}} \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} -5 &amp; -4 &amp; 0 &amp; \colorbox{grey}{8} \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ * &amp; * &amp; * &amp; * \\ \end{bmatrix}\] <p>The same steps are repeated for the subsequent rows, giving us the following result</p> \[\begin{bmatrix} 3 &amp; 0 &amp; 1 &amp; 2 &amp; 7 &amp; 4 \\ 1 &amp; 5 &amp; 8 &amp; 9 &amp; 3 &amp; 1 \\ 2 &amp; 7 &amp; 2 &amp; 5 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 1 &amp; 7 &amp; 8 \\ 4 &amp; 2 &amp; 1 &amp; 6 &amp; 2 &amp; 8 \\ 2 &amp; 4 &amp; 5 &amp; 2 &amp; 3 &amp; 9 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} -5 &amp; -4 &amp; 0 &amp; 8 \\ -10 &amp; -2 &amp; 2 &amp; 3 \\ 0 &amp; -2 &amp; -4 &amp; -7 \\ -3 &amp; -2 &amp; -3 &amp; -16 \\ \end{bmatrix}\] <p>With this, we have performed a single convolution on our given image, giving us a result that detects the vertical edges in our image.</p> <h2 id="edge-detectors">Edge Detectors</h2> <p>This result obtained in the previous section is known as a Vertical Edge Detector, which is one of the many types of edge detection kernels that deep learning models inadvertently end up learning in the lower layers of their networks.</p> <p>These kernels, as the name suggests, find any parts of our image that may have vertical, or horizontal edges.</p> <p>Edge detection works by trying to find regions in an image where there is a sharp change in intensity. A higher value would indicate a steep change while a lower value indicates a shallow change.</p> <p>Consider the given example below where we have a lighter region with the value of \(10\) at each pixel and a darker region at a value of \(0\).</p> \[\begin{bmatrix} 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ \end{bmatrix}\] <div class="row justify-content-center mt-3"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/beforeConv.jpg" sizes="95vw"/> <img src="/assets/img/beforeConv.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The array above would look something like this image. </div> <p>The filter we are using here is of the form</p> \[\begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix}\] <div class="row justify-content-center mt-3"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/filterConv.jpg" sizes="95vw"/> <img src="/assets/img/filterConv.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The filter we are using looks something like this. </div> <p>Upon performing the convolution operation we obtain the following result.</p> \[\begin{bmatrix} 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ 10 &amp; 10 &amp; 10 &amp; 0 &amp; 0 &amp; 0 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} 0 &amp; 30 &amp; 30 &amp; 0 \\ 0 &amp; 30 &amp; 30 &amp; 0 \\ 0 &amp; 30 &amp; 30 &amp; 0 \\ 0 &amp; 30 &amp; 30 &amp; 0 \\ \end{bmatrix}\] <p>Which looks something like the below image</p> <div class="row justify-content-center mt-3"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/afterConv.jpg" sizes="95vw"/> <img src="/assets/img/afterConv.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In this case, when the filter is in a region where all the pixel colors are the same, for example the upper left or upper right most sections of the given image example, the vertical edge detecting convolution gives us a zero on the output as the calculations made on these pixels cancel themselves out.</p> <p>Towards the center of the image where there is a change in the color values of the pixels in the image, the filter outputs a higher value due to the difference in color intensities between the left-hand and right-hand sides.</p> <p>As a result, this emphasizes upon the change in intensity, giving lesser importance parts of the image where the color intensities of the pixels are either the same or in the similar range.</p> <p>For an image with the intensities flipped, with the darker side being on the left, the result would be something like the following.</p> \[\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ 0 &amp; 0 &amp; 0 &amp; 10 &amp; 10 &amp; 10 \\ \end{bmatrix} * \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix} = \begin{bmatrix} 0 &amp; -30 &amp; -30 &amp; 0 \\ 0 &amp; -30 &amp; -30 &amp; 0 \\ 0 &amp; -30 &amp; -30 &amp; 0 \\ 0 &amp; -30 &amp; -30 &amp; 0 \\ \end{bmatrix}\] <div class="row justify-content-center mt-3"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/afterConvInvert.jpg" sizes="95vw"/> <img src="/assets/img/afterConvInvert.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="examples-of-edge-detection">Examples of Edge Detection</h2> <ul> <li><strong>Prewitt Operator</strong>: This family of edge detectors is used to detect vertical and horizontal edges in images. It does so by placing an emphasis on the pixels towards a given side of the mas, either the left or right hand side for the vertical filter, and the top or bottom for a horizontal filter.</li> </ul> \[\text{Vertical Filter} = \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix}\] \[\text{Horizontal Filter} = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; -1 &amp; -1 \\ \end{bmatrix}\] <ul> <li><strong>Sobel Operator</strong>: This is one of the most commonly used family of filters in edge detection. Here, the filter places an empahsis on pixels close to the center of the mask as opposed to the Prewitt filters, hence making it a bit more robust.</li> </ul> \[\text{Vertical Sobel Filter} = \begin{bmatrix} 1 &amp; 0 &amp; -1 \\ 2 &amp; 0 &amp; -2 \\ 1 &amp; 0 &amp; -1 \\ \end{bmatrix}\] \[\text{Horizontal Sobel Filter} = \begin{bmatrix} 1 &amp; 2 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; -2 &amp; -1 \\ \end{bmatrix}\] <h2 id="implementing-vertical-and-horizontal-edge-filters">Implementing Vertical and Horizontal Edge Filters</h2> <p>We will now look at an implementation of vertical and horizontal edge detectors in Python to see how they work.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/filter_demonstrations.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p><br/></p> <h2 id="a-note-on-edge-detection-in-neural-networks">A Note on Edge Detection in Neural Networks</h2> <p>In deep learning, it is not required for us to handpick the values of the weights used in the filters acting on the input images. In most cases, these values are treated as parameters which are automatically learnt through the backpropagation process.</p> <p>In many cases the filters learned by the networks by themselves are much more robust than any predefined filters that we may have.</p> <h2 id="padding">Padding</h2> <p>When building neural networks, one of the modifications we have to make to our images is called <strong>padding</strong>.</p> <p>When performing convolutions on our input images, the dimensions of the output images are reduced with every layer of the neural network.</p> <p>For example, a \(6 \times 6\) image being convolved with a \(3 \times 3\) filter gives us a \(4 \times 4\) output image.</p> <p>Extrapolating this operation to different examples, for a \(n \times n\) image being convolved with a \(f \times f\) filter, the output would have the following dimensions.</p> \[\boxed{(n-f+1) \times (n-f+1)}\] <p>The downsides here are:</p> <ul> <li>Every convolution operation will lead to the image dimensions shrinking</li> <li>The pixels at the corners of the image are not given as much of an importance as a pixel in the middle as the kernel only acts on it once. As a result, the corner pixels contribute much less to the output.</li> </ul> <p>To combat this, padding is implemented before each convolutional operation.</p> <p>The convention followed is to pad the image with layers of pixels \(p\) with the value of \(0\) around the image, changing its dimensions from \(n \times n\) to \((n + 2p) \times (n + 2p)\).</p> <p>The output resulting from convolutions being performed on this image will be</p> \[\boxed{(n+2p-f+1) \times (n+2p-f+1)}\] <p>Increasing the number of layers when padding the image ensures that the corner pixels are utilized more in the outputs.</p> <h2 id="valid-and-same-convolutions">Valid and Same Convolutions</h2> <p>A <strong>valid</strong> convolution is one where no padding has been done to the input images</p> \[(n \times n) * (f \times f) \rightarrow (n-f+1) \times (n-f+1)\] <p>A <strong>same</strong> convolution means that the image has been padded such that the size of the output is the same as the size of the input</p> \[\begin{align*} &amp; (n+2p-f+1) \times (n+2p-f+1) = (n \times n) \\ \Rightarrow &amp; p = \frac{f-1}{2} \end{align*}\] <p><strong>By convention, the value of \(f\) is always odd</strong></p> <p>An even filter size would require uneven padding. If \(f\) is odd, the padding region from the convolution will be more natural.</p> <p>When using odd dimensions of filters, this gives us a central pixel, which makes it easier to talk about its position.</p> <h2 id="strided-convolutions">Strided Convolutions</h2> <p>Strided convolutions are another building block of convolutional neural networks. The mechanism for the same will be discussed below.</p> <p>Consider the given \(7 \times 7\) image being convolved with a \(3 \times 3\) filter with a stride of \(2\)</p> \[\begin{bmatrix} 2 &amp; 3 &amp; 7 &amp; 4 &amp; 6 &amp; 2 &amp; 9 \\ 6 &amp; 6 &amp; 9 &amp; 8 &amp; 7 &amp; 4 &amp; 3 \\ 3 &amp; 4 &amp; 8 &amp; 3 &amp; 8 &amp; 9 &amp; 7 \\ 7 &amp; 8 &amp; 3 &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix}\] <p>The first step here is done in the same way as we would normally do</p> \[\begin{bmatrix} \colorbox{grey}{2^{3}} &amp; \colorbox{grey}{3^{4}} &amp; \colorbox{grey}{7^{4}} &amp; 4 &amp; 6 &amp; 2 &amp; 9 \\ \colorbox{grey}{6^{1}} &amp; \colorbox{grey}{6^{0}} &amp; \colorbox{grey}{9^{2}} &amp; 8 &amp; 7 &amp; 4 &amp; 3 \\ \colorbox{grey}{3^{-1}} &amp; \colorbox{grey}{4^{0}} &amp; \colorbox{grey}{8^{3}} &amp; 3 &amp; 8 &amp; 9 &amp; 7 \\ 7 &amp; 8 &amp; 3 &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix} = \begin{bmatrix} \colorbox{grey}{91} &amp; * &amp; * \\ * &amp; * &amp; * \\ * &amp; * &amp; * \\ \end{bmatrix}\] <p>From the second step, instead of jumping to the next pixel, we move over by the number of steps mentioned by the stride, which in this case is \(2\), and perform the next calculation.</p> \[\begin{bmatrix} 2 &amp; 3 &amp; \colorbox{grey}{7^{3}} &amp; \colorbox{grey}{4^{4}} &amp; \colorbox{grey}{6^{4}} &amp; 2 &amp; 9 \\ 6 &amp; 6 &amp; \colorbox{grey}{9^{1}} &amp; \colorbox{grey}{8^{0}} &amp; \colorbox{grey}{7^{2}} &amp; 4 &amp; 3 \\ 3 &amp; 4 &amp; \colorbox{grey}{8^{-1}} &amp; \colorbox{grey}{3^{0}} &amp; \colorbox{grey}{8^{3}} &amp; 9 &amp; 7 \\ 7 &amp; 8 &amp; 3 &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix} = \begin{bmatrix} 91 &amp; \colorbox{grey}{100} &amp; * \\ * &amp; * &amp; * \\ * &amp; * &amp; * \\ \end{bmatrix}\] <p>The same is repeated for the subsequent operation</p> \[\begin{bmatrix} 2 &amp; 3 &amp; 7 &amp; 4 &amp; \colorbox{grey}{6^{3}} &amp; \colorbox{grey}{2^{4}} &amp; \colorbox{grey}{9^{4}} \\ 6 &amp; 6 &amp; 9 &amp; 8 &amp; \colorbox{grey}{7^{1}} &amp; \colorbox{grey}{4^{0}} &amp; \colorbox{grey}{3^{2}} \\ 3 &amp; 4 &amp; 8 &amp; 3 &amp; \colorbox{grey}{8^{-1}} &amp; \colorbox{grey}{9^{0}} &amp; \colorbox{grey}{7^{3}} \\ 7 &amp; 8 &amp; 3 &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix} = \begin{bmatrix} 91 &amp; 100 &amp; \colorbox{grey}{83} \\ * &amp; * &amp; * \\ * &amp; * &amp; * \\ \end{bmatrix}\] <p>For moving down to the next row, instead of jumping down by one pixel, we jump by the stride number, 2.</p> \[\begin{bmatrix} 2 &amp; 3 &amp; 7 &amp; 4 &amp; 6 &amp; 2 &amp; 9 \\ 6 &amp; 6 &amp; 9 &amp; 8 &amp; 7 &amp; 4 &amp; 3 \\ \colorbox{grey}{3^{3}} &amp; \colorbox{grey}{4^{4}} &amp; \colorbox{grey}{8^{4}} &amp; 3 &amp; 8 &amp; 9 &amp; 7 \\ \colorbox{grey}{7^{1}} &amp; \colorbox{grey}{8^{0}} &amp; \colorbox{grey}{3^{2}} &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ \colorbox{grey}{4^{-1}} &amp; \colorbox{grey}{2^{0}} &amp; \colorbox{grey}{1^{3}} &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix} = \begin{bmatrix} 91 &amp; 100 &amp; 83 \\ \colorbox{grey}{69} &amp; * &amp; * \\ * &amp; * &amp; * \\ \end{bmatrix}\] <p>This process is then repeated till all the needed values are obtained</p> \[\begin{bmatrix} 2 &amp; 3 &amp; 7 &amp; 4 &amp; 6 &amp; 2 &amp; 9 \\ 6 &amp; 6 &amp; 9 &amp; 8 &amp; 7 &amp; 4 &amp; 3 \\ 3 &amp; 4 &amp; 8 &amp; 3 &amp; 8 &amp; 9 &amp; 7 \\ 7 &amp; 8 &amp; 3 &amp; 6 &amp; 6 &amp; 3 &amp; 4 \\ 4 &amp; 2 &amp; 1 &amp; 8 &amp; 3 &amp; 4 &amp; 6 \\ 3 &amp; 2 &amp; 4 &amp; 1 &amp; 9 &amp; 8 &amp; 3 \\ 0 &amp; 1 &amp; 3 &amp; 9 &amp; 2 &amp; 1 &amp; 4 \end{bmatrix} * \begin{bmatrix} 3 &amp; 4 &amp; 4 \\ 1 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 3 \end{bmatrix} = \begin{bmatrix} 91 &amp; 100 &amp; 83 \\ 69 &amp; 91 &amp; 127 \\ 44 &amp; 72 &amp; 74 \\ \end{bmatrix}\] <h4 id="input-and-output-dimensions">Input and Output Dimensions</h4> <p>The input and output dimensions here are governed by</p> \[\underset{\text{padding p}}{(n \times n)} *\underset{\text{stride s}}{(f \times f)} \rightarrow \left\lfloor \frac{n+2p-f}{s}+1 \right\rfloor \times \left\lfloor\frac{n+2p-f}{s}+1\right\rfloor\] <p>Here, we only the boxed numbers being multiplied in the previous steps if it is fully contained within the image including the padding. For any parts outside the image, we do not perform any computation.</p> <h2 id="convolutions-over-several-layers">Convolutions over several layers</h2> <p>In the real world most of the images we work with are in the form of 3D volumes, with two of the dimensions being the height and width of the image, and the third dimension being the channels of the image.</p> <p>Consider a case where we want to detect features in an RGB image. In this example the dimensions of the image is \(6 \times 6 \times 3\) and we want to convolve it with a \(3 \times 3 \times 3\) filter with the third dimension corresponding to each of the color channels. Here the output we get is going to be of dimensions \(4 \times 4 \times 1\).</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RGBConv1.png" sizes="95vw"/> <img src="/assets/img/RGBConv1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here we consider the filter to be a cube with 27 parameters in it. The mechanism here is similar to what we have seen before where we first place the convolution cube on the upper left position as shown, multiplying the corresponding number from the red, green, and blue channels with the filter’s values, and adding them up to give us the first pixel of the output.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RGBConv2.png" sizes="95vw"/> <img src="/assets/img/RGBConv2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Similarly for the next step, we move the cube forward by one pixel and perform the same computations to obtain the next pixel value</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RGBConv3.png" sizes="95vw"/> <img src="/assets/img/RGBConv3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This process can be repeated for all the pixels of the image.</p> <p>The advantages here are:</p> <ul> <li>If we want to check for edges in an image as per a given channel, we can focus on the given channel particularly while setting the other channels to zero.</li> <li>Edge detectors can be made to work irrespective of the color channel.</li> <li>This increases the number of choices we have in terms of the increased number of parameters, hence helping us detect more complex features.</li> </ul> <p>We can also perform convolutions on a given image with multiple filters simultaneously to give us an output with the number of channels corresponding to the number of filters applied to the image. Using the same example as before, applying two filters to it would look something like the below</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MultilayerRGBConv.png" sizes="95vw"/> <img src="/assets/img/MultilayerRGBConv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This can be done with several filters simultaneously, giving a proportionally equal number of output channels corresponding to each feature being worked on.</p> <h2 id="the-convolutional-neural-network-layer-structure">The Convolutional Neural Network Layer Structure</h2> <p>Now that we have seen how the convolution mechanism works, the last thing that is needed to create a convolutional neural network layer is to add a bias to our convolution values, alongside a non-linearity such as the ReLU or Sigmoid functions. An example can be seen below.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ExampleLayer.png" sizes="95vw"/> <img src="/assets/img/ExampleLayer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Comparing this to a single layer propagation in a standard neural network, the linear step equivalent is</p> \[Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\] <p>with the non-linear activation part given by</p> \[A^{[l]} = g(Z^{[l]})\] <p>The convolutional step equivalent of the same is as follows:</p> <ul> <li>The RGB image acts as the input \(A^{[0]}\)</li> <li>The filters act as the weights \(W^{[1]}\)</li> <li>The output of the convolution operation is the resultant \(Z^{[l]}\) value which is fed to the activation function to obtain the output channels \(A^{[1]}\).</li> </ul> <p>Here, the number of parameters remain the same irrespective of the size of the input. These parameters instead depend on the dimensions and number of convolutional filters used.</p> <h2 id="notations-for-cnns">Notations for CNNs</h2> <ul> <li>\(f^{[l]} =\) filter size at layer \(l\)</li> <li>\(p^{[l]} =\) amount of padding at layer \(l\)</li> <li>\(s^{[l]} =\) stride at layer \(l\)</li> <li> <p>\(n_{c}^{[l]} =\) number of filters at layer \(l\)</p> </li> <li>The input has the following properties <ul> <li>\(n\) is the number of pixels</li> <li>\(n_{c}\) is the number of channels in the previous layer</li> <li>\(n_{H}^{[l-1]} \times n_{W}^{[l-1]} \times n_{c}^{[l-1]}\) is the dimension of the input image</li> </ul> </li> <li>The output has the following dimensions</li> </ul> \[n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}\] <ul> <li>the height of the output is given by</li> </ul> \[n_{H}^{[l]} = \left\lfloor \frac{n_{H}^{[l]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \right\rfloor\] <ul> <li> <p>The same formula we apply for height also applies to the width of the output</p> </li> <li>The size of each filter is \(f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]}\)</li> <li>Activations have the dimensions \(n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}\)</li> <li>For mini batch GD, our output dimensions will be \(m \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{C}^{[l]}\)</li> <li>Weight dimensions: \(f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \times n_{c}^{[l]}\)</li> <li>Bias dimensions: \(n_{c}^{[l]}\) or \(1 \times 1 \times n_{c}^{[l]}\)</li> </ul> <h2 id="a-few-general-points-about-cnns">A few general points about CNNs</h2> <p>Most of the work that goes into designing a ConvNet involves hyperparameter selection such as</p> <ul> <li>total size</li> <li>stride</li> <li>padding</li> <li>number of filters</li> </ul> <p>In a CNN we usually start with larger images that gradually trend down in size as one goes deeper into the neural network. In turn, the number of channels increases.</p> <p>The typical ConvNet consists of three layers:</p> <ul> <li>The Convolution Layer (Conv)</li> <li>Pooling Layer (Pool)</li> <li>Fully Connected Layer (FC)</li> </ul> <p>Now that we have looked into the convolution layer, we will now look at the other layers at hand, and how they work towards building different CNN architectures.</p> <h2 id="pooling-layers">Pooling Layers</h2> <p><strong>Pooling layers</strong> are used in CNNs to reduce the size of the representations in the network, helping speed up computation alongside making some detected features more robust.</p> <h3 id="max-pooling">Max Pooling</h3> <p>Considering the example of a \(4 \times 4\) input the output would be a \(2 \times 2\) matrix.</p> \[\begin{bmatrix} \colorbox{darkblue}{1} &amp; \colorbox{darkblue}{3} &amp; \colorbox{blue}{2} &amp; \colorbox{blue}{1} \\ \colorbox{darkblue}{2} &amp; \colorbox{darkblue}{9} &amp; \colorbox{blue}{1} &amp; \colorbox{blue}{1} \\ \colorbox{green}{1} &amp; \colorbox{green}{3} &amp; \colorbox{red}{2} &amp; \colorbox{red}{3} \\ \colorbox{green}{5} &amp; \colorbox{green}{6} &amp; \colorbox{red}{1} &amp; \colorbox{red}{2} \end{bmatrix} \rightarrow \begin{bmatrix} \colorbox{darkblue}{9} &amp; \colorbox{blue}{2} \\ \colorbox{green}{6} &amp; \colorbox{red}{3} \end{bmatrix}\] <p>In this example of max pooling, the inputs are broken down into 4 distinct regions. The output here is obtained by taking the maximum of the corresponding shaded regions.</p> <p>This is similar to taking a convolution filter of size 2 and stride 2 (which are the only parameters used in this filter), with the function applied taking only the maximum of the numbers within the filter position.</p> <p>The idea of max pooling is that as long as a feature is detected in a given area, it will be preserved in the output, otherwise, the max of numbers in the region are relatively small.</p> <p>Max pooling is the only type of CNN layer that has no parameters to learn and only two hyperparameters to learn.</p> <h4 id="max-pooling-example">Max Pooling Example</h4> <p>Consider a \(5 \times 5\) image to which we want to apply a \(3 \times 3\) filter, with filter size \(f=3\) and step size \(s=1\).</p> <p>The output here will be a \(3 \times 3\) matrix.</p> \[\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 1 &amp; 3 \\ 2 &amp; 9 &amp; 1 &amp; 1 &amp; 5 \\ 1 &amp; 3 &amp; 2 &amp; 3 &amp; 2 \\ 8 &amp; 3 &amp; 5 &amp; 1 &amp; 0 \\ 5 &amp; 6 &amp; 1 &amp; 2 &amp; 9 \end{bmatrix} \rightarrow \begin{bmatrix} * &amp; * &amp; * \\ * &amp; * &amp; * \\ * &amp; * &amp; * \end{bmatrix}\] <p>The formulas for finding the <strong>output size</strong> for the Convolution layers hold true in this case too. That is</p> \[\left\lfloor \frac{n+2p-f}{s}+1 \right\rfloor\] <p>The mechanism with which the filter is applied to the image is also the same as what is done in the case of convolution layers, shown below</p> \[\begin{bmatrix} \colorbox{grey}{1} &amp; \colorbox{grey}{3} &amp; \colorbox{grey}{2} &amp; 1 &amp; 3 \\ \colorbox{grey}{2} &amp; \colorbox{grey}{9} &amp; \colorbox{grey}{1} &amp; 1 &amp; 5 \\ \colorbox{grey}{1} &amp; \colorbox{grey}{3} &amp; \colorbox{grey}{2} &amp; 3 &amp; 2 \\ 8 &amp; 3 &amp; 5 &amp; 1 &amp; 0 \\ 5 &amp; 6 &amp; 1 &amp; 2 &amp; 9 \end{bmatrix} \rightarrow \begin{bmatrix} \colorbox{grey}{9} &amp; * &amp; * \\ * &amp; * &amp; * \\ * &amp; * &amp; * \end{bmatrix}\] \[\begin{bmatrix} 1 &amp; \colorbox{grey}{3} &amp; \colorbox{grey}{2} &amp; \colorbox{grey}{1} &amp; 3 \\ 2 &amp; \colorbox{grey}{9} &amp; \colorbox{grey}{1} &amp; \colorbox{grey}{1} &amp; 5 \\ 1 &amp; \colorbox{grey}{3} &amp; \colorbox{grey}{2} &amp; \colorbox{grey}{3} &amp; 2 \\ 8 &amp; 3 &amp; 5 &amp; 1 &amp; 0 \\ 5 &amp; 6 &amp; 1 &amp; 2 &amp; 9 \end{bmatrix} \rightarrow \begin{bmatrix} 9 &amp; \colorbox{grey}{9} &amp; * \\ * &amp; * &amp; * \\ * &amp; * &amp; * \end{bmatrix}\] \[\begin{bmatrix} 1 &amp; 3 &amp; \colorbox{grey}{2} &amp; \colorbox{grey}{1} &amp; \colorbox{grey}{3} \\ 2 &amp; 9 &amp; \colorbox{grey}{1} &amp; \colorbox{grey}{1} &amp; \colorbox{grey}{5} \\ 1 &amp; 3 &amp; \colorbox{grey}{2} &amp; \colorbox{grey}{3} &amp; \colorbox{grey}{2} \\ 8 &amp; 3 &amp; 5 &amp; 1 &amp; 0 \\ 5 &amp; 6 &amp; 1 &amp; 2 &amp; 9 \end{bmatrix} \rightarrow \begin{bmatrix} 9 &amp; 9 &amp; \colorbox{grey}{5} \\ * &amp; * &amp; * \\ * &amp; * &amp; * \end{bmatrix}\] \[\vdots\] \[\begin{bmatrix} 1 &amp; 3 &amp; 2 &amp; 1 &amp; 3 \\ 2 &amp; 9 &amp; 1 &amp; 1 &amp; 5 \\ 1 &amp; 3 &amp; 2 &amp; 3 &amp; 2 \\ 8 &amp; 3 &amp; 5 &amp; 1 &amp; 0 \\ 5 &amp; 6 &amp; 1 &amp; 2 &amp; 9 \end{bmatrix} \rightarrow \begin{bmatrix} 9 &amp; 9 &amp; 5 \\ 9 &amp; 9 &amp; 5 \\ 8 &amp; 6 &amp; 9 \end{bmatrix}\] <p>In the case of 3D inputs, the outputs will have the same dimensions.</p> <h3 id="average-pooling">Average Pooling</h3> <p>Average pooling, is similar in its mechanism to max pooling but the main difference here is that the average is taken instead of the maximum value</p> \[\begin{bmatrix} \colorbox{darkblue}{1} &amp; \colorbox{darkblue}{3} &amp; \colorbox{blue}{2} &amp; \colorbox{blue}{1} \\ \colorbox{darkblue}{2} &amp; \colorbox{darkblue}{9} &amp; \colorbox{blue}{1} &amp; \colorbox{blue}{1} \\ \colorbox{green}{1} &amp; \colorbox{green}{3} &amp; \colorbox{red}{2} &amp; \colorbox{red}{3} \\ \colorbox{green}{5} &amp; \colorbox{green}{6} &amp; \colorbox{red}{1} &amp; \colorbox{red}{2} \end{bmatrix} \rightarrow \begin{bmatrix} \colorbox{darkblue}{3.75} &amp; \colorbox{blue}{1.25} \\ \colorbox{green}{4} &amp; \colorbox{red}{2} \end{bmatrix}\] <p>Average pooling isn’t as commonly used compared to max pooling.</p> <p>An area where average pooling can be used is in very deep neural networks to collapse representations from large dimensions like \(7 \times 7 \times 1000\) to \(1 \times 1 \times 1000\).</p> <h3 id="a-summary-of-pooling">A Summary of Pooling</h3> <p>The hyperparameters for pooling are:</p> <ul> <li>\(f\) : Filter size</li> <li>\(s\) : Stride</li> <li>Type of pooling: Max or Average</li> <li>\(p\) : padding (This is very rarely used)</li> </ul> <p>The common filter size and stride values used are</p> <ul> <li>filter size \(f=2\)</li> <li>stride length \(s=2\)</li> </ul> <p>Input and output of max pooling:</p> \[n_{H} \times n_{W} \times n_{C} \rightarrow \left\lfloor \frac{n_{H} - f}{s} + 1 \right\rfloor \times \left\lfloor \frac{n_{W} - f}{s} + 1 \right\rfloor \times n_{C}\] <p>Pooling has <strong>no parameters to learn</strong>. It is a <strong>fixed function</strong> that the neural network applies to the input layers.</p> <h5 id="a-concern-with-respsct-to-pooling">A concern with respsct to pooling</h5> <p>Max-pooling layers can result in the loss of accurate spacial information.</p> <h2 id="why-are-convolutions-used">Why are convolutions used?</h2> <p>If we were to work with an image of dimensions \(32 \times 32 \times 3\) implementing \(6\) filters on it, this would give us a \(28 \times 28 \times 6\) output.</p> <p>In this case \(32*32*3=3072\) and \(28*28*6=4704\).</p> <p>Using these values directly in a neural network with \(3072\) inputs and a \(4704\) unit output would give us a total of \(3072 \times 4076 \approx 14\text{ Million}\), which is computationally expensive to train.</p> <p>Considering that this is a relatively small image, one can only imagine how infeasible it would be to use this on images of larger dimensions.</p> <p>Convolutional layers have a relatively smaller number of parameters. In the case of a \(5 \times 5\) filter, including the bias parameter, we would be working with \(26\) parameters. If we were to use six of these filters, then we would have \((5*5*3 + 1)*6 = 456\) parameters to work with.</p> <p>The number of parameters in a CNN layer can be condensed into the given equation</p> \[\text{number of parameters} = (f^{[l]} \times f^{[l]} \times n_{C}^{[l-1]} + b)\times n_{C}^{[l]}\] <p>The reasons that ConvNets have an advantage over fully connected neural networks in terms of the number of parameters are:</p> <ul> <li><strong>Parameter Sharing</strong>: A feature detector such as a vertical edge detector that is useful in one part of the input image can also be useful in other parts of the image. <ul> <li>In this case, suppose we have a \(3 \times 3\) filter for vertical edge detection, the same can be used for detecting vertical edges in other parts of the image.</li> <li>This holds true for lower level features of the network such as edges as well as high level features such as eyes, nose, etc. for facial recongition.</li> <li>As a result, the network will not need to learn different feature detectors for the same feature.</li> </ul> </li> <li><strong>Sparsity of connections</strong>: Another advantage is that for each layer, the output values depend only on a small number of inputs.</li> </ul> <p>It is mainly through these two mechanisms that a CNN is able to have fewer connections, hence allowing training on much smaller training sets while avoiding overfitting.</p> <p>Another advantage of CNNs are that they are capable of capturing translation invariance. Here, if a given sample input is shifted in any direction, the filter will still be able to capture and identify the object in the image.</p> <h2 id="putting-together-a-convnet">Putting together a ConvNet</h2> <p>Suppose we wanted to implement a convolutional neural network with a labelled training set \((x^{(1)},y^{(1)}), \dots , (x^{(m)},y^{(m)})\).</p> <p>A CNN structure will mainly comprise of convolution and pooling layers as the main body of the hidden layers.</p> <p>These convolution and pooling layers are then followwed by fully connected layers that are formed by flattening the last convolution-like layer into a 1-dimension layer.</p> <p>The final layer of the network, that is, the output layer will be a softmax output given by \(\hat{y}\).</p> <p>Each of the convolutional (conv) and fully connected (FC) layers will have various parameters in the form of weights \(W\) and biases \(b\). This is used to define the cost function.</p> \[J = \frac{1}{m} \sum\limits_{i=1}^{m}L(\hat{y^{(1)}},y^{(i)})\] <p>The training process in this case is similar to how we would perform it for an ANN, which is through using a gradient based root finding algorithm like gradient descent or any of its optimized versions to reduce the cost function \(J\).</p> <h1 id="classical-convnet-case-studies">Classical ConvNet Case Studies</h1> <p>There has been several decades of research done in the field of computer vision since the mid 1960s. The idea of convolutional operations was introduced in the 1980s via a paper by Toshiteru Homma, Les Atlas and Robert Marks II (missing reference).</p> <p>This led to further research into the field, finding rudimentary uses in handwriting detection and slowly branching out into other domains like medical image segmentation and breast cancer detection.</p> <p>All this research snowballed into what we now recognize to be ConvNets, in the early 2000s, which forms the basis of modern computer vision.</p> <p>Even though we are already aware of ConvNets and how they work it’s always a good idea to explore different classical implementations of them to gain an idea of how they</p> <p>In this section we will look at some well known implementations of classical CNN architectures and how they work.</p> <h2 id="lenet-5">LeNet-5</h2> <p>The <strong>LeNet-5</strong> architecture was introduced by Yann LeCun in a collaboratively written paper <a class="citation" href="#LeNet5">(Lecun et al., 1998)</a> at AT&amp;T Bell Labs back in 1998, and is one of the earliest neural networks developed.</p> <p>This architecture was introduced as a better alternative to already existing pattern recognition systems that depended on hand-crafted feature extraction.</p> <p>This paper introduces the concepts of gradient based learning for convolution filters to help them learn low level features, slowly moving up to more complex features when moving towards the output nodes.</p> <p>The LeNet-5 architecture was trained on digitized \(32 \times 32\) greyscale images of handwritten digits.</p> <p>The main architecture of LeNet-5 is given below. Here, consider \(Cx\) to be a convolutional layer, \(Sx\) to be a pooling/subsampling layer and \(Fx\) to be a fully connected layer. \(x\) here refers to the layer index. While the original paper used average pooling for the sampling layers, the more modern variants of this architecture use max pooling.</p> <table> <thead> <tr> <th>Layer</th> <th>Inputs</th> <th>No. of Filters</th> <th>Kernel Size</th> <th>Stride</th> <th>Activation</th> <th>Outputs</th> </tr> </thead> <tbody> <tr> <td>C1</td> <td>(32,32,1)</td> <td>6</td> <td>5</td> <td>1</td> <td>Sigmoid</td> <td>(28,28,6)</td> </tr> <tr> <td>S2 (Max pool)</td> <td>(28,28,6)</td> <td>1</td> <td>2</td> <td>2</td> <td>–</td> <td>(14,14,6)</td> </tr> <tr> <td>C3</td> <td>(14,14,6)</td> <td>16</td> <td>5</td> <td>1</td> <td>Sigmoid</td> <td>(10,10,16)</td> </tr> <tr> <td>S4 (Max pool)</td> <td>(10,10,16)</td> <td>1</td> <td>2</td> <td>2</td> <td>Flatten</td> <td>(5,5,16)</td> </tr> <tr> <td>F5</td> <td>(5,5,16) = 400</td> <td>120</td> <td>1</td> <td>NA</td> <td>Sigmoid</td> <td>120</td> </tr> <tr> <td>F6</td> <td>120</td> <td>NA</td> <td>84 units</td> <td>NA</td> <td>Sigmoid</td> <td>84</td> </tr> <tr> <td>Output</td> <td>84</td> <td>NA</td> <td>10 units</td> <td>NA</td> <td>–</td> <td>1 of 10</td> </tr> </tbody> </table> <p><br/></p> <p>In the second convolution layer C3, only 10 of the 16 convolution feature maps are connected to each of the inputs at a time. This is similar to dropout in a sense, playing a similar role of disrupting the network symmetry.</p> <p>The nonlinearities used in the original paper were sigmoid and tanh. Modern implementations of this architecture make use of ReLU activations.</p> <p>The original architecture did not implement a softmax layer and instead directly output values corresponding to each input.</p> <p>By modern standards this network is relatively small with \(60,000\) parameters. Modern neural networks work with 10 or 100 million parameters, with networks going upto a thousand times the size of this network.</p> <p>The main pattern followed here is</p> \[\boxed{\text{Convolution Layer} \rightarrow \text{Pooling Layer}} \times 2 \rightarrow \boxed{\text{Fully Connected Layers}} \rightarrow \boxed{\text{Output}}\] <p>and, a trend here as one goes down the layers starting from the inputs, the height and width of each of the channels tend to decrease while the number of channels increases. There is no padding applied to any of the inputs.</p> <h2 id="alexnet">AlexNet</h2> <p><strong>AlexNet</strong> <a class="citation" href="#AlexNet">(Krizhevsky et al., 2012)</a> is one of the most influential papers in computer vision and was the first one to have an error less than 25% at the time. As of the writing of this blog, the paper for this network architecture</p> <p>This network was designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton as a part of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012.</p> <p>This architecture here resembles LeNet-5 to a certain extent but has a significantly larger number of layers.</p> <p>The network architecture is as follows</p> <table> <thead> <tr> <th>Layer</th> <th>Inputs</th> <th>No. of Filters</th> <th>Kernel Size</th> <th>Stride</th> <th>Activation</th> <th>Outputs</th> </tr> </thead> <tbody> <tr> <td>C1</td> <td>(227,227,3)</td> <td>96</td> <td>11</td> <td>4</td> <td>ReLU</td> <td>(55,55,96)</td> </tr> <tr> <td>S2 (Max pool)</td> <td>(55,55,96)</td> <td>1</td> <td>3</td> <td>2</td> <td>–</td> <td>(27,27,96)</td> </tr> <tr> <td>C3 (Same Conv.)</td> <td>(27,27,96)</td> <td>256</td> <td>5</td> <td>1</td> <td>ReLU</td> <td>(27,27,256)</td> </tr> <tr> <td>S4 (Max pool)</td> <td>(27,27,256)</td> <td>1</td> <td>3</td> <td>2</td> <td>–</td> <td>(13,13,256)</td> </tr> <tr> <td>C5 (Same Conv.)</td> <td>(13,13,256)</td> <td>384</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(13,13,384)</td> </tr> <tr> <td>C6 (Same Conv.)</td> <td>(13,13,256)</td> <td>384</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(13,13,384)</td> </tr> <tr> <td>C7 (Same Conv.)</td> <td>(13,13,256)</td> <td>256</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(13,13,256)</td> </tr> <tr> <td>S8 (Max pool)</td> <td>(13,13,256)</td> <td>1</td> <td>3</td> <td>2</td> <td>Flatten</td> <td>(6,6,256)</td> </tr> <tr> <td>F9</td> <td>(6,6,256) = 9216</td> <td>NA</td> <td>4096 units</td> <td>NA</td> <td>ReLU, dropout = 0.5</td> <td>4096</td> </tr> <tr> <td>F10</td> <td>4096</td> <td>NA</td> <td>4096 units</td> <td>NA</td> <td>ReLU, dropout = 0.5</td> <td>4096</td> </tr> <tr> <td>Softmax (output)</td> <td>4096</td> <td>NA</td> <td>1000 units</td> <td>NA</td> <td>–</td> <td>1 of 1000</td> </tr> </tbody> </table> <p><br/></p> <p>This architecture utilized the ReLU activation function as opposed to the sigmoid and Tanh functions which aided in its increased performance.</p> <p>A lot of the dimensions in the paper are different from what it is supposed to be as per Andrej Karpathy. This is so that the calculations come out right.</p> <p>This network architecture was made to train using two GPUs with code written for them to communicate with each other.</p> <p>The number of parameters here is \(60,000,000\) as opposed to LeNet-5’s \(60,000\) parameters.</p> <p><strong>Local response normalization</strong> is implemented here to aid in generalization. This is done despite the fact that ReLU doesn’t need input normalization to prevent saturation.</p> <p>The structure pattern followed here is as follows</p> \[\boxed{\text{CNN} \rightarrow \text{Normalization} \rightarrow \text{Pooling}} \times 2 \rightarrow \boxed{\text{CNN}} \times 3 \rightarrow \boxed{\text{Pooling}} \rightarrow \boxed{\text{FC with dropout}} \rightarrow \boxed{\text{Output}}\] <h2 id="vgg-16">VGG-16</h2> <p><strong>VGG-16</strong> was proposed by Karen Simonyan and Andrew Zisserman in 2014 for the ILSVRC. Its name stands for <strong>Visual Geometry Group 16</strong> was introduced by the Visual Geometry Group at the University of Oxford. The number in its name stands for the number of layers in the network, 16 layers of which 13 are convolutional layers and 3 are fully connected layers.</p> <p>This model achieved a \(92.7\%\) accuracy on the ImageNet dataset, containing 14 million images with \(1000\) classes.</p> <p>The inputs in this case, similar to the AlexNet model takes in \((224,224,3)\) images to give one of \(1000\) possible outputs.</p> <p>This network is well known for having a much simpler architecture with a focus on having convolution layers at a fixed size of \(f=3\), with a stride of \(s=1\) with same padding applied at every one of them. The max pooling layers were also fixed at the dimension of \(f=2\) with a stride \(s=2\).</p> <p>The architecture here is as follows</p> <table> <thead> <tr> <th>Layer</th> <th>Inputs</th> <th>No. of Filters</th> <th>Kernel Size</th> <th>Stride</th> <th>Activation</th> <th>Outputs</th> </tr> </thead> <tbody> <tr> <td>C1 (Same Conv.)</td> <td>(224,224,3)</td> <td>64</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(224,224,64)</td> </tr> <tr> <td>C2 (Same Conv.)</td> <td>(224,224,64)</td> <td>64</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(224,224,64)</td> </tr> <tr> <td>S3 (Max Pool)</td> <td>(224,224,64)</td> <td>1</td> <td>2</td> <td>2</td> <td>–</td> <td>(112,112,64)</td> </tr> <tr> <td>C4 (Same Conv.)</td> <td>(112,112,64)</td> <td>128</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(112,112,128)</td> </tr> <tr> <td>C5 (Same Conv.)</td> <td>(112,112,128)</td> <td>128</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(112,112,128)</td> </tr> <tr> <td>S6 (Max Pool)</td> <td>(112,112,128)</td> <td>1</td> <td>2</td> <td>2</td> <td>–</td> <td>(56,56,128)</td> </tr> <tr> <td>C7 (Same Conv.)</td> <td>(56,56,128)</td> <td>256</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(56,56,256)</td> </tr> <tr> <td>C8 (Same Conv.)</td> <td>(56,56,256)</td> <td>256</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(56,56,256)</td> </tr> <tr> <td>C9 (Same Conv.)</td> <td>(56,56,256)</td> <td>256</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(56,56,256)</td> </tr> <tr> <td>S10 (Max pool)</td> <td>(56,56,256)</td> <td>1</td> <td>2</td> <td>2</td> <td>–</td> <td>(28,28,256)</td> </tr> <tr> <td>C11 (Same Conv.)</td> <td>(28,28,256)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(28,28,512)</td> </tr> <tr> <td>C12 (Same Conv.)</td> <td>(28,28,512)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(28,28,512)</td> </tr> <tr> <td>C13 (Same Conv.)</td> <td>(28,28,512)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(28,28,512)</td> </tr> <tr> <td>S14 (Max pool)</td> <td>(28,28,512)</td> <td>1</td> <td>2</td> <td>2</td> <td>–</td> <td>(14,14,512)</td> </tr> <tr> <td>C15 (Same Conv.)</td> <td>(14,14,512)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(14,14,512)</td> </tr> <tr> <td>C16 (Same Conv.)</td> <td>(14,14,512)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(14,14,512)</td> </tr> <tr> <td>C17 (Same Conv.)</td> <td>(14,14,512)</td> <td>512</td> <td>3</td> <td>1</td> <td>ReLU</td> <td>(14,14,512)</td> </tr> <tr> <td>S18 (Max pool)</td> <td>(14,14,512)</td> <td>1</td> <td>2</td> <td>2</td> <td>Flatten</td> <td>(7,7,512)</td> </tr> <tr> <td>F19</td> <td>(7,7,512) = 25088</td> <td>NA</td> <td>4096 units</td> <td>NA</td> <td>ReLU, dropout = 0.5</td> <td>4096</td> </tr> <tr> <td>F20</td> <td>4096</td> <td>NA</td> <td>4096 units</td> <td>NA</td> <td>ReLU, dropout = 0.5</td> <td>4096</td> </tr> <tr> <td>Softmax (output)</td> <td>4096</td> <td>NA</td> <td>1000 units</td> <td>NA</td> <td>–</td> <td>1 of 1000</td> </tr> </tbody> </table> <p><br/></p> <p>This network has a total of \(138,000,000\) parameters.</p> <h1 id="residual-networks-resnets">Residual Networks (ResNets)</h1> <p>When training a deep neural network the training is much slower as opposed to shallow networks.</p> <p>Imagine we have to train a neural network on a large amount of data.<a class="citation" href="#BryceResNets">(Wiedenbeck, 2022)</a> When initializing the network, we know that the weights are randomly initialized, and as the inputs are fed into each layer, they are multiplied with these randomly generated weights and fed into activations. This is done several times such that towards the output layers, we get random noise as none of the information we receive is related to the input.</p> <p>Now, when going through the backpropagation process, we compute the loss on the output and propagate it back through the network. We yet again face the same issue as before as the loss value later into the network do not inform the layers much about the inputs as their own inputs are noise. Hence, the gradients are also effectively scrambled and propagate noise back towards the input layers, even compounding it due to multiplications with weight matrices.</p> <p>This leads to a lot of problems when training the network as the backprop algorithm struggles to learn from the inputs. In addition the problem here isn’t even something like overfitting which would have been diagnosed easily.</p> <p>This is where <strong>Residual Networks</strong> or <strong>ResNets</strong> come into the picture.</p> <p>ResNets <a class="citation" href="#BruntonResNets">(Brunton, 2024)</a> are one of the most important deep neural network architectures in the modern day. They find uses in several fields like image classification and ordinary differential equations. They are also one of the main ingredients used in transformers, which find use in large language models such as ChatGPT.</p> <p>The main idea here is to create a way for data to arrive at later layers, hence making their inputs more meaningful, and also for loss gradients and their updates to have some meaning.</p> <p>This is done by introducing the idea of <strong>skip connections</strong> which allow for data to be propagated deeper into the network.</p> <h2 id="the-residual-block">The Residual Block</h2> <p>ResNets are built upon a <strong>residual block</strong>, where a few layers of the network are grouped up into blocks, and the input activations of these blocks are both fed through each of its layers and also around it.</p> <p>Consider a neural network modelling an input/output function with inputs \(x\) and outputs \(y\). The relationships can be shown as</p> \[y = f(x)\] <p>In ResNets, instead of trying to learn the direct relationship mapping, we create a copy of the input \(x\) and only model the difference between them and \(y\) given as</p> \[y = x + f(x)\] <p>This is because most of the features in the output \(y\) is already present in the input and there is a only a small part in the form of the residual \(f(x)\) capturing all the features in the input.</p> <p>Here, the <strong>residual</strong> refers to the <em>minute changes</em> that occur to the inputs as one traverses down the network as opposed to the whole feature itself.</p> <p>Consider the layers below taken from a several layer deep neural network.</p> <div class="row justify-content-center mt-3"> <div class="col-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/two_layers.png" sizes="95vw"/> <img src="/assets/img/two_layers.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The activations in these layers would look like</p> \[a^{[l]} \rightarrow \underbrace{Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}}_{\text{linear operator}} \rightarrow \underbrace{g(Z^{[l+1]}) = a^{[l+1]}}_{\text{Activation}} \rightarrow \underbrace{Z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}}_{\text{linear operator}} \rightarrow \underbrace{g(Z^{[l+2]}) = a^{[l+2]}}_{\text{Activation}}\] <p>Here we see that the information flow goes all the way through from \(a^{[l]}\) to \(a^{[l+2]}\). We call this the <strong>main path</strong> for the given set of layers.</p> <p>In the case of a residual network, the value for \(a^{[l]}\) is copied much further into the neural network and added to \(Z^{[l+2]}\), before the non-linearity. This is known as the <strong>shortcut</strong> as shown below.</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/calculation_shortcut.png" sizes="95vw"/> <img src="/assets/img/calculation_shortcut.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Looking at the neural network architecture, this would look like</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/two_layer_res.png" sizes="95vw"/> <img src="/assets/img/two_layer_res.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A slightly more explicit look into this would be something like</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/skip_connection.jpg" sizes="95vw"/> <img src="/assets/img/skip_connection.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, the skip connections here give the data two paths to follow, one through the block and one skipping these intermediate layers and finding use deeper in the network.</p> <p>This essentially helps train deeper neural networks without forgetting what the original inputs are towards the final stages of the network.</p> <p>ResNets are built using several residual blocks, stacked together to form a deep network</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/plain_conn.png" sizes="95vw"/> <img src="/assets/img/plain_conn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/res_conn.png" sizes="95vw"/> <img src="/assets/img/res_conn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A neural network made with plain connections and a residual network with skip connections. </div> <p>The outputs of the block are combined with the input to the block in the form of a simple function that passes gradients along undisturbed by either</p> <ul> <li><strong>Adding</strong> the tensor of inputs and tensor of outputs element-wise.</li> <li><strong>Concatenating</strong> the tensors</li> </ul> <p>Here, the latest activation acts as a residual added on to the previous information passed by the skip layer. We only focus on only modelling these residuals which capture the changes from the inputs and outputs of the residual layer.</p> <h2 id="how-resnets-work">How ResNets Work</h2> <p>Consider a plain network as shown with \(X\) being the inputs to a big neural network with \(a^{[l]}\) as the output activations.</p> \[X \rightarrow \boxed{\text{Big NN}} \rightarrow a^{[l]}\] <p>Consider two more layers to be added to it , making it deeper.</p> \[X \rightarrow \boxed{\text{Big NN}} \xrightarrow{a^{[l]}} \text{layer }[l+1] \rightarrow \text{layer }[l+2] \rightarrow a^{[l+2]}\] <p>According to the ResNets <a class="citation" href="#DBLP:journals/corr/HeZRS15">(He et al., 2015)</a> paper, there exists a solution <em>by construction</em> to the deeper model</p> <p>The added layers are <em>“identity mapping”</em> and the other layers are copying from the learned shallower model which is acting as the “identity” here.</p> <p>Now, considering a skip connection from \(a^{[l]}\) to \(layer[l+2]\), we compare the equations for both cases</p> <table> <tr> <th> Plain Neural Network </th> <th> ResNet </th> </tr> <tr> <td> $$ \begin{align*} Z^{[l]} &amp;= W^{[l]}A^{[l-1]} + b^{[l]} \\ A^{[l]} &amp;= g(Z^{[l]}) \\ Z^{[l+1]} &amp;= W^{[l+1]}A^{[l]} + b^{[l+1]} \\ A^{[l+1]} &amp;= g(Z^{[l+1]}) \\ Z^{[l+2]} &amp;= W^{[l+2]}A`^{[l+1]} + b^{[l+1]} \\ A^{[l+2]} &amp;= g(Z^{[l+2]}) \\ \end{align*} $$ </td> <td> $$ \begin{align*} Z^{[l]} &amp;= W^{[l]}A^{[l-1]} + b^{[l]} \\ A^{[l]} &amp;= g(Z^{[l]}) \text{ (Skip connection created)}\\ Z^{[l+1]} &amp;= W^{[l+1]}A^{[l]} + b^{[l+1]} \\ A^{[l+1]} &amp;= g(Z^{[l+1]}) \\ Z^{[l+2]} &amp;= W^{[l+2]}A^{[l+1]} + b^{[l+1]} \text{ (Skip connection added)}\\ A^{[l+2]} &amp;= g(Z^{[l+2]} + A^{[l]}) \end{align*} $$ </td> </tr> </table> <p><br/></p> <p>If some amount of regularization or weight decay was applied here, the values of \(Z^{[l+2]}\) would shrink down to zero or values close to zero. The effects of this would have are:</p> <ul> <li> <p>For the plain model, the output activation \(A^{[l+2]}\) would be set to a value biased towards zero as opposed to a value that would be more representative of the previous inputs. Hence, the nodes in the given layer would take a longer time to train and also have a lower accuracy.</p> </li> <li> <p>In the case of ResNets, we focus on building information on top of the “identity” formed by pre-existing layers of the network. So, instead of being shrunk to zero, the given layer’s output would consist of the additional information, or “residuals” learned by the layers in between the skip layers and the output.</p> </li> </ul> <p>The idea is that if we have a shallower model learning a function, we should also be able to train a deeper model to learn the same function by copying the original five layers’ values and using the additional layers to learn residuals that are added on top of the identity function.</p> <p>The plain model does not learn the identity function because most of the weights are initialiized close to zero, hence causing any sort of regularization to bias the weights towards zero.</p> <p>Hence, the idea of identity functions makes it easy for our residual blocks to train, making it so that adding the additional layers to the network will not have as much of an impact on its ability to learn.</p> <h4 id="note">Note</h4> <p>When building ResNets it is assumed that the dimensions of the skip connection and the output of the residual block have the same dimension.</p> <p>Hence, we <strong>must</strong> use same convolutions in the residual network with \(1 \times 1\) strides to ensure the same input and output dimensions.</p> <p>In the occasion that the dimensions are different, we must add a matrix \(W_{s}\) to transform the dimensions of the output to be the same as the input.</p> <p>Here, \(W_{s}\) can be a set of parameters to be learned or a matrix that implements zero padding.</p> <p>If we also want to match up the number of channels, we use \(1 \times 1\) convolutions.</p> <h2 id="advantages-of-resnets">Advantages of ResNets</h2> <h5 id="1-skip-blocks-augment-the-existing-data">1. Skip blocks augment the existing data</h5> <p>As the skip connection passes the input down to the later layers of the neural network, the main network can focus on figuring out what information it has to add on top of the inputs as opposed to the whole input itself. This makes subsequent processing easier.</p> <ul> <li>If we are concatenating the inputs, it is passed through unchanged along with the outputs. If added, the value being sent is mostly unchanged as the weights are centered around zero.</li> <li>As a result of this each block learns a simpler task and has better access to information to learn from.</li> </ul> <h5 id="2-shorter-gradient-paths">2. Shorter gradient paths</h5> <p>Due to skip connections, the gradients have shorter paths to follow to get to each layer of the network. This is because each block has one path going through each of the layers and one path around them, and the gradients go back through both of them. Hence, any layer in the network will have a shorter path which the loss gradients can arrive to and usefully update the given layer’s computation</p> <ul> <li>As a result, in the initial epochs when the training results are not informative, we can still get useful updates from the shorter paths and obtain decreasing loss right away.</li> <li>In addition, over time, as the later blocks in the network start computing more useful functions, the information along the direct path becomes more informative, hence leading to a continued decrease in the gradients, outperforming the plain network.</li> <li>To sum it up, this leads to <strong>faster training</strong>.</li> </ul> <h5 id="3-modularity">3. Modularity</h5> <p>An additional advantage that ResNets have is modularity. Since each of the blocks have similar structures, we can short circuit around blocks during training, making it easier to add more blocks and deepen the network.</p> <ul> <li>Most papers have variations on the same network with different numbers of blocks with different tradeoffs between the resources needed to train and the resulting model’s predicting power.</li> </ul> <h2 id="concerns">Concerns</h2> <h5 id="1-shape-mismatch">1. Shape Mismatch</h5> <p>If we want to combine the inputs and outputs of a ResNet, we need to ensure that the input and output shapes match up. For normal activation layers like dense networks, we might end up with different numbers of neurons per layer.</p> <p>Hence, when adding the inputs to the outputs of the first block, we need an operation to reshape the inputs or only add the inputs to a part of the block’s output.</p> <p>With ConvNets, we will have to ensure that the height and width of the convolutional layers can be combined with the input height and width.</p> <h5 id="2-parameter-explosion">2. Parameter Explosion</h5> <p>Another concern with ResNets is that with repeated concatenation at the output blocks, we can get a large activation tensor, hence leading to an explosion in the number of parameters. To counteract this, we prefer using addition to concatenation with matching input shapes.</p> <h1 id="shape-matching-convolutional-blocks">Shape Matching Convolutional blocks</h1> <p>A lot of implementations of ConvNets require for us to ensure that the output shapes of convolutional blocks are in a particular shape. One way to ensure this is to use \(1 \times 1\) strides and same padding to ensure the width and height of the image are preserved.</p> <p>However, if we want to match up the number of channels, we use \(1 \times 1\) convolutions.</p> <h1 id="1-times-1-convolutions-networks-in-networks">\(1 \times 1\) Convolutions (Networks in Networks)</h1> <p>\(1 \times 1\) convolutions are useful when designing ConvNet architectures.</p> <p>A \(1 \times 1\) filter would be something like</p> \[\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 6 &amp; 5 &amp; 8 \\ 3 &amp; 5 &amp; 5 &amp; 1 &amp; 3 &amp; 4 \\ 2 &amp; 1 &amp; 3 &amp; 4 &amp; 9 &amp; 3 \\ 4 &amp; 7 &amp; 8 &amp; 5 &amp; 7 &amp; 9 \\ 1 &amp; 5 &amp; 3 &amp; 7 &amp; 4 &amp; 8 \\ 5 &amp; 4 &amp; 9 &amp; 8 &amp; 3 &amp; 5 \\ \end{bmatrix} * \begin{bmatrix} 2 \end{bmatrix} = \begin{bmatrix} 2 &amp; 4 &amp; 6 &amp; \dots &amp; &amp; \\ \\ \\ \\ \\ \\ \end{bmatrix}\] <p>Here we are effectively multiplying every pixel of the given image by \(2\).</p> <p>For a \(6 \times 6 \times 32\) image, the filter goes through each of the \(36\) pixels of the image, taking the element-wise product with the \(32\) pixels in the same position along the channels, hence giving us a real number output to which we apply our non-linearities.</p> <p>This is equivalent to a single neuron with \(32\) nodes, multiplying each of them in place with \(32\) weights, adding them up and applying our non-linearity to them.</p> <p>For multiple filters. it is equivalent to having multiple units, taking all the numbers in a slice and building them up to an output.</p> <p>This is called a \(1 \times 1\) convolution or network in network.</p> <h3 id="alternate-explanation">Alternate explanation</h3> <p>Using a \(1 \times 1\) kernel on a multi-channel input gives an output block with the same height and width as the input. Each neuron in the output receives inputs from just one pixel of the input but also receives information from the entire depth of the particular channel it is pointed to.</p> <p>Setting the number of filters helps us specify the depth of the output, and each neuron of the output can gets its inputs from the neurons across the depth of the previous layer. This is similar to adding a single dense layer to make the input and output shapes match up.</p> <h2 id="uses-of-1-times-1-convolutions">Uses of \(1 \times 1\) convolutions</h2> <p>Consider the given case below</p> \[28 \times 28 \times 192 \xrightarrow[CONV 1 \times 1 \;\; (32)]{ReLU} 28 \times 28 \times 32\] <p>To shrink the height and width of the volume above we would normally use a pooling layer, but we have no straightforward way to reduce the number of channels.</p> <p>Hence, in this case, to shrink the number of filters down to \(32\), we use \(32\) \(1 \times 1\) filters with each filter having a dimensions of \(1 \times 1 \times 192\). This will effectively result in shrinking the number of layers down to \(28 \times 28 \times 32\), hence helping save on computations in the networks.</p> <p>The network in network effect is the addition of a non-linearity, allowing the learning of more complex functions by adding yet another layer.</p> <h1 id="inception-networks">Inception Networks</h1> <p>Consider us building a neural network from scratch. We would pick our layers in the form of convolution filters of varying dimensions or pooling layers.</p> <p>An <strong>inception network</strong> does the job of all of the discussed layers til now while making the network architecture more complicated and performing well.</p> <p>Inception networks were developed by Google with the initial name of GoogleNet. Its name is inspired by the “We need to go deeper” meme from the movie Inception, and the meme is actually referenced in the paper, which is pretty funny.</p> <p>Consider the case of a \(28 \times 28 \times 192\) input. In an inception layer we do all of the operations at once.</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inception_example.png" sizes="95vw"/> <img src="/assets/img/inception_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> An Inception layer </div> <p>In the image above the inception network is performing the following operations</p> <ul> <li>A \(1 \times 1\) convolution to give a \(28 \times 2 \times 64\) output.</li> <li>A \(3 \times 3\) same convolution to give a \(28 \times 28 \times 128\) output.</li> <li>A \(5 \times 5\) same convolution to give a \(28 \times 28 \times 32\) output.</li> <li>A max pooling layer with padding to give us a \(28 \times 28 \times 32\) output.</li> </ul> <p>All of these are stacked one on top of the other to give us a composite output of dimensions \(28 \times 28\times 256\).</p> <p>The idea of an inception network is to be a swiss army knife to which we can put in our inputs to obtain the outputs exactly as per our needs. This network performs all the operations needed at once and concatenates them, allowing the network to learn the parameters to use alongside the filter sizes and pools needed.</p> <p>The main drawback of inception layers is that the <strong>computation cost</strong> tends to be higher.</p> <h2 id="inception-v1">Inception V1</h2> <p>The first iteration of the Inception network <a class="citation" href="#DBLP:journals/corr/SzegedyLJSRAEVR14">(Szegedy et al., 2014)</a> was created by Christian Szegedy as a part of the ILSVRC14 challenge.</p> <p>It was mainly developed as an efficient neural network architecture compared to previous implementations, using upto 12 times fewer parameters than AlexNet.</p> <p>As suggested by the meme it cites, the main idea here is to allow even deeper neural networks to be built with minimal effect on the network’s performance.</p> <p>The idea here is that given the rise of embedded and mobile computing, algorithm efficieny in terms of power and energy use are crucial alongside accuracy itself.</p> <p>The inception network heavily makes use of the Network-in-Network approach to increase the representational power of the CNN. This serves two purposes:</p> <ul> <li>Dimension reduction to remove computational bottlenecks, which would limit network size.</li> <li>This also allows increasing the width of the network without significant impact on the performance.</li> </ul> <h4 id="motivations">Motivations</h4> <p>The main way to increase the performance of a DNN is through increasing its size, both in depth and width considering the availability of data to work with. However this has some major drawbacks:</p> <ul> <li>Larger networks will need a larger number of parameters, hence leading to more chances for overfitting in case the number of training examples is limited.</li> <li>Increased network sizes also increase the amount of computation power needed, needing efficient distribution of computation resources. as opposed to directtly increasing the network size.</li> </ul> <p>To tackle this, the solution followed is to implement a sparsely connected architecture (Including inside the convolutions) and to exploit the hardware to efficiently compute dense matrices.</p> <h4 id="inception-architecture">Inception Architecture</h4> <p>The main idea followed here is finding a local sparse structure that can be approximated and covered by readily available dense components. This is built using convolutional building blocks.</p> <p>Once the optimal local construction is found, it is repeated spatially. A layer by layer construction is followed where correlation statistics of the previous layer are analyzed and grouped into units with high correlation.</p> <p>These groups/clusters are then used as units for the next layer and are connected to the units of the previous layer.</p> <p>Using the same example as before, we could make a composition of the form</p> <ul> <li>A \(1 \times 1\) convolution to give a \(28 \times 2 \times 64\) output.</li> <li>A \(3 \times 3\) same convolution to give a \(28 \times 28 \times 128\) output.</li> <li>A \(5 \times 5\) same convolution to give a \(28 \times 28 \times 32\) output.</li> <li>A max pooling layer with padding to give us a \(28 \times 28 \times 32\) output.</li> </ul> <p>And then repeat this structure multiple times as per our need.</p> <p>As the inception modules are stacked, the output correlation statistics will vary as features with higher abstraction are captured deep into the network. This leads to lesser spatial concentration and as a result, the computations become much more expensive. Max pooling, when added to the mix, leads to an increase in the number of outputs at each stage.</p> <p>To combat this, <strong>dimension reduction</strong> is practiced at points where computational requirements go high. This is done with the help of \(1 \times 1\) convolutions before the more expensive \(3 \times 3\) and \(5 \times 5\) convolutions. The resultant inception models are as follows.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/naive_inception.png" sizes="95vw"/> <img src="/assets/img/naive_inception.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source, "Going Deeper With Convolutions" (Szegedy et al., 2014) </div> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dimension_reduced_inception.png" sizes="95vw"/> <img src="/assets/img/dimension_reduced_inception.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source, "Going Deeper With Convolutions" (Szegedy et al., 2014) </div> <p>Hence, as a result we have a network architecture that alows us to build deep models without an increase in computational complexity and, in addition, allows visual information to be processed at various scales and aggregated so that the next layers can abstract features from them parallelly.</p> <h4 id="googlenet">GoogLeNet</h4> <p>GoogLeNet made use of this Inception architecture to perform well under strict memory and computational budget, using only 5 million parameters, as opposed to AlexNet and VGGNet.</p> <p>The resultant neural network architecture that was proposed by the team using these inception modules is shown below.</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/googleNet.jpg" sizes="95vw"/> <img src="/assets/img/googleNet.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source, "Going Deeper With Convolutions" (Szegedy et al., 2014) </div> <p>As we can see here, there are two extra “auxillary networks” added to the architecture here aside from the inception layers. These serve the purpose of helping reduce the <strong>vanishing gradient</strong> problem that is introduced when training deep neural networks.</p> <p>The auxillary classifiers make use of the outputs of the intermediate inception net layer outputs to generate softmax predictions of their own, based on which auxillary losses are obtained. These auxillary losses are used to augment the loss function during the training process. This is in the form</p> \[J_{\text{total}} = J + (0.3 \times J_{\text{aux1}}) + (0.3 \times J_{\text{aux2}})\] <p>These auxillary networks only serve the purpose of training the model and are removed during the inference process.</p> <h2 id="inception-v2">Inception V2</h2> <p>The second version of the inception network <a class="citation" href="#DBLP:journals/corr/SzegedyVISW15">(Szegedy et al., 2015)</a> acknowledged the effectiveness of its older version, touting its effectiveness in solving big data problems with a lower effective cost. However, it also points out a major drawback of Inception v1 quoted directly from the paper:</p> <blockquote> <p>The complexity of the inception network makes it more difficult to make changes to the network.</p> </blockquote> <p>The main aim of this paper was to upgrade this model such to increase its accuracy while also reducing the computational complexity.</p> <p>Naively scaling up inception networks eliminates any computational gains that one would expect from such an architecture. In addition there was no explanation as to why certain design decisions were made with GoogLeNet, hence making it difficult to adapt to new use cases.</p> <p>The main things tackled here are:</p> <ul> <li><strong>Avoiding representational bottlenecks:</strong> The idea here is that instead of directly jumping from larger dimensions to smaller ones with large amounts of compression, the network must, instead, work towards gradually decreasing the representation size from the inputs to the outputs. Large jumps in the dimensions of the layers leads to a loss of information crucial for the network to train on, which is known as a <strong>representational bottleneck</strong>.</li> <li><strong>Using smart factorization methods</strong> such as increasing the activations per tile, spatial aggregation and balancing the width and depth of the network to make the training process more efficient.</li> </ul> <h3 id="fixes-proposed">Fixes proposed</h3> <h5 id="1-factorizing-convolutions-with-large-filter-sizes">1. Factorizing convolutions with large filter sizes</h5> <p>The first change that was made to the network architecture was factorizing convolutions with larger filter sizes such as \(5 \times 5\) or \(7 \times 7\) to smaller sizes.</p> <p>In this case the \(5 \times 5\) convolutions are replaced with mini networks consisting of two \(3 \times 3\) convolution layers as seen below.</p> <div class="row justify-content-center mt-3"> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/original_inception.jpg" sizes="95vw"/> <img src="/assets/img/original_inception.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5x5_factorized_inception.jpg" sizes="95vw"/> <img src="/assets/img/5x5_factorized_inception.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The original inception architecture alongside the new change proposed (Szegedy et al., 2015). Note the filter marked by a red boundary. This is "Figure 5" </div> <h5 id="2-futher-factorization-of-n-times-n-filters-to-a-combination-of-1-times-n-and-n-times-1-convolutions">2. Futher factorization of \(n \times n\) filters to a combination of \(1 \times n\) and \(n \times 1\) convolutions</h5> <p>The inception module can further be changed by using asymmetric convolutions on the images, which would make each of them behave like a single layer fully connected with one another.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/1xn_nx1_variant.jpg" sizes="95vw"/> <img src="/assets/img/1xn_nx1_variant.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The proposed (1,n) and (n,1) architecture (Szegedy et al., 2015). This is "Figure 6" </div> <p>Taking the case of a \(3 \times 3\) filter, each can be replaced with a \(3 \times 1\) convolution followed by a \(1 \times 3\) convolution.</p> <p>This solution is \(33\%\) cheaper than the full \(3 \times 3\) convolution. For \(n \times n\) filters, as \(n\) increases in value, this can lead to drastic improvements in computational cost saving.</p> <p>This idea is, however, not effective at the earlier layers of the network, but works well on medium grid sizes.</p> <h5 id="3-using-higher-dimension-representations-to-increase-the-activations-per-tile">3. Using higher dimension representations to increase the activations per tile</h5> <p>It was found that using higher dimension representations would allow for disentanglement of the features being worked on, hence leading to faster training. As a result another inception layer type was introduced with expanded filter bank outputs.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/expanded_filter_banks.jpg" sizes="95vw"/> <img src="/assets/img/expanded_filter_banks.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The proposed expanded filter banks architecture (Szegedy et al., 2015). This is "Figure 7" </div> <p>This helped with removing representational bottlenecks.</p> <h3 id="network-architecture">Network Architecture</h3> <p>Using the architectures proposed above, the final proposed neural network architecture is given below</p> <table> <thead> <tr> <th>type</th> <th>patch size/stride</th> <th>input size</th> </tr> </thead> <tbody> <tr> <td>conv</td> <td>\(3 \times 3/2\)</td> <td>\(299\times 299 \times 3\)</td> </tr> <tr> <td>conv</td> <td>\(3 \times 3/1\)</td> <td>\(149 \times 149 \times 32\)</td> </tr> <tr> <td>conv padded</td> <td>\(3 \times 3/1\)</td> <td>\(147 \times 147 \times 32\)</td> </tr> <tr> <td>pool</td> <td>\(3 \times 3/2\)</td> <td>\(147 \times 147 \times 64\)</td> </tr> <tr> <td>conv</td> <td>\(3 \times 3/1\)</td> <td>\(73 \times 73 \times 64\)</td> </tr> <tr> <td>conv</td> <td>\(3 \times 3/2\)</td> <td>\(71 \times 71 \times 80\)</td> </tr> <tr> <td>conv</td> <td>\(3 \times 3/1\)</td> <td>\(35 \times 35 \times 192\)</td> </tr> <tr> <td>\(3 \times\) Inception</td> <td>As in figure \(5\)</td> <td>\(35 \times 35 \times 288\)</td> </tr> <tr> <td>\(5 \times\) Inception</td> <td>As in figure \(6\)</td> <td>\(17\times 17 \times 768\)</td> </tr> <tr> <td>\(2 \times\) Inception</td> <td>As in figure \(7\)</td> <td>\(8\times 8 \times1280\)</td> </tr> <tr> <td>pool</td> <td>\(8 \times 8\)</td> <td>\(8 \times 8 \times 2048\)</td> </tr> <tr> <td>linear</td> <td>logits</td> <td>\(1 \times 1 \times 2048\)</td> </tr> <tr> <td>softmax</td> <td>classifier</td> <td>\(1 \times 1 \times 1000\)</td> </tr> </tbody> </table> <p><br/></p> <h2 id="inception-v3">Inception V3</h2> <p>This version of Inception was proposed in the same paper as Inception V2 and has more or less the same upgrades that were implemented in Inception V2. The main improvements made here were:</p> <ul> <li>RMSProp Optimizer was implemented.</li> <li>\(7 \times 7\) convolutions were factorized into three \(3 \times 3\) convolutions.</li> <li><strong>Batch Normalization in the auxillary classifiers:</strong> If we look back at the first version of inception networks, there were two auxillary classifiers introduced that were believed to help augment the stability, improve the convergence and mitigate the vanishing gradient problem. <ul> <li>However, it was later found that these did not necessarily help much in improving the convergence earlier in the training process. Instead they acted more as regularization for their respective layers.</li> <li>Batch Normalizing these auxillary classifiers was found to lead to a better performance of the main network.</li> </ul> </li> <li>Label smoothing was implemented as a regularizing component in the loss formula to prevent overfitting.</li> </ul> <h2 id="other-implementations">Other implementations</h2> <p>There have been more implementations of the inception network such as Inception V4 and Inception-ResNet, which were introduced in the same paper <a class="citation" href="#DBLP:journals/corr/SzegedyIV16">(Szegedy et al., 2016)</a> in 2016. These dealt with the addition of residual connections to inception blocks to prevent the loss of computed values deeper into the networks.</p> <p>As of now we will not look into these architectures as there is much more to talk about in computer vision, and focusing on these will take up a lot of time on the blog (which is long enough as is). It is always advised to read the related paper as it is well written and does a really good job at explaining what is going on.</p> <h1 id="mobilenet">MobileNet</h1> <p>Deep neural networks which distinguish between a wide variety of labels generate a large amount of features, especially as we go deeper into the layers. This becomes computationally expensive because the number of channels of the filters also increase as one uses more filters while going` deeper into the network.</p> <p>MobileNet <a class="citation" href="#DBLP:journals/corr/HowardZCKWWAA17">(Howard et al., 2017)</a> is another foundational neural network architecture designed with a focus on solving the computational scalability issues mentioned before while being lightweight and efficient for computer vision based applications in low compute environments like mobile phones and microprocessors.</p> <p>The idea here is that each filter need not necessarily look at every single channel of the input at once. We can instead make each filter work on the input channels one by one and combine them into a single output.</p> <p>The main motivations for this architecture are:</p> <ul> <li>Low computational costs</li> <li>Usage of models with this architecture in mobile phones and embedded systems.</li> <li>Introducing the idea of depth-wise separable convolutions.</li> </ul> <p>The idea of this class of network architecture is to allow a deep learning model developers to choose a small network according to the constraints of the machine it is being run on. This is meant to optimize for latency (speed) while yielding small networks.</p> <p>This architecture implements the idea of “depthwise separable convolutions” which were previously used in inception networks but were never given a proper name. These help reduce the computation costs at the lower layers of the network.</p> <h2 id="what-mobilenet-aims-to-solve">What MobileNet aims to solve</h2> <p>Looking back at normal convolutions as seen from the example below</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/RGBConv1.png" sizes="95vw"/> <img src="/assets/img/RGBConv1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Assuming input image dimensions to be \(n \times n \times n_{c}\) being put through a convolution filter of dimension \(f \times f \times n_{c}\). Each side of the output obtained here, taking \(p\) as the padding and \(s\) as the stride, would be:</p> \[\frac{n + 2p -f}{s}\] <p>If we were to compute the cost here, it would be proportional to the form</p> \[\text{Computational cost} = \text{# filter params } \times \text{ # filter positions } \times \text{ # of filters}\] <p>with the computational costs depending multiplicatively on the number of input channels, number of output channels, kernel size and feature map/image size.</p> <p>So, taking the number of filters to be \(5\) with \(0\) padding and stride \(1\), the computational cost would be</p> \[\text{Computational Cost} = (3 \times 3 \times 3) \times (4 \times 4) \times 5 = 2160\] <p>Though this may seem small, when building deeper networks, the computational costs of these calculations add up, hence impacting the performance of the network.</p> <p>The <strong>depthwise separable convolutions</strong> were introduced to help with this issue.</p> <h2 id="depthwise-separable-convolutions">Depthwise Separable Convolutions</h2> <p>The depthwise separable convolution consists of two stages</p> <ul> <li>The <strong>depthwise</strong> convolution stage, followed by</li> <li>The <strong>pointwise</strong> convolution stage</li> </ul> <p>and the order of computing these are shown below</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/depthwise_separable_convolution.png" sizes="95vw"/> <img src="/assets/img/depthwise_separable_convolution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The depthwise part applies a single filter to each input channel and the pointwise convolution applies a \(1 \times 1\) convolution to combine the outputs of the depthwise convolution.</p> <p>The idea here is to split up the interaction between the number of output channels and kernel size.</p> <p>We will now look at what occurs during each stage.</p> <h4 id="depthwise-convolution">Depthwise Convolution</h4> <p>Here, for a \(n \times n \times n_c\) input, where \(n_c\) is the number of channels, we make use of a filter of dimensions \(f \times f\) applied to each channel of the input.</p> <p>Here, one of each of the filters is applied to each of the corresponding input channels such that, the red channel is convolved with one filter, the green with another one and so on, till we get a \(n_{out} \times n_{out} \times n_c\) output.</p> <p>The computation cost here is proportional to</p> \[(3 \times 3) \times (4 \times 4) \times 3 = 432\] <p>As we can see here, this depthwise convolution action is much more efficient than a standard convolution, however it is only filtering the input channels in certain manners. It isn’t combining these to create a feature map. This is where <strong>pointwise convolutions</strong> come into the picture.</p> <h4 id="pointwise-convolution">Pointwise Convolution</h4> <p>Naively concatenating the outputs of the depthwise convolution layer leads to an issue where each layer of the output would only depend on one of the convolution action of the input. This pattern will repeat deeper into the network, hence not giving us as effective of a result as a normal convolution layer.</p> <p>To combat this we make use of \(1 \times 1\) convolutions known as <strong>pointwise convolutions</strong> corresponding to the number of filter we need to act on the input.</p> <p>The pointwise convolution takes the \(n_{out} \times n_{out} \times n_{c}\) convolution obtained from the depthwise convolution part and applies number of \(1 \times 1\) convolutions equal to the number of filters being applied to the input.</p> <p>Taking the same example as before where we have a \(6 \times 6 \times 3\) input with a filter size of \(3\), stride of \(1\), padding \(0\) and number of filters as \(5\), the computational cost here would be proportional to</p> \[(1 \times 1 \times 3) \times (4 \times 4) \times 5 = 240\] <h4 id="comparing-the-cost-with-normal-convolutions">Comparing the cost with normal convolutions</h4> <p>Adding up the depthwise and pointwise convolution parts, we get the total computational costs to be proportional to</p> \[\underbrace{((3 \times 3) \times (4 \times 4) \times 3)}_{\text{depthwise}} + \underbrace{((1 \times 1 \times 3) \times (4 \times 4) \times 5)}_{\text{pointwise}} = 672\] <p>which is much lower compared to that of a normal convolution which is</p> \[(3 \times 3 \times 3) \times (4 \times 4) \times 5 = 2160\] <p>Here we see that the depthwise separable convolutions are \(31 \%\) as computationally expensive as a normal convolution layer.</p> <h2 id="generalized-formulae-for-computational-costs-and-their-ratios">Generalized formulae for computational costs and their ratios</h2> <p>Generalizing the formula for computation costs, we take the following:</p> <ul> <li>\(n_c\) as <strong>number of channels</strong></li> <li>\(f\) as <strong>filter dimension</strong></li> <li>\(n_f\) as <strong>number of filters</strong> being used</li> <li>\(n_{out}\) as the <strong>output dimensions</strong>, which can also be seen as the <strong>number of filter positions</strong> on the input image.</li> </ul> <p>and get</p> \[\begin{align*} &amp; \boxed{\text{Computations}_{\text{MobileNet}} = (f \times f \times n_c \times n_{out} \times n_{out}) + (n_c \times n_{out} \times n_{out} \times n_f)} \\ &amp; \boxed{\text{Computations}_{\text{ConvNet}} = f \times f \times n_c \times n_{out} \times n_{out} \times n_f} \\ &amp; \boxed{\text{Computations Ratio} = \frac{\text{Computations}_{\text{MobileNet}}}{\text{Computations}_{\text{ConvNet}}} = \frac{1}{n_f} + \frac{1}{f^2}} \end{align*}\] <h2 id="mobilenet-architecture">MobileNet Architecture</h2> <p>The MobileNet architecture is fairly straightforward. Wherever we would have particularly heavy computations involving convolution operations, we use depthwise separable convolutions.</p> <p>The MobileNet V1 paper implemented this block 13 times from the raw image inputs to the final classification prediction vis a pooling layer, fully connected layer and a softmax output.</p> <h2 id="experiments-done-on-the-mobilenet-architecture">Experiments done on the MobileNet architecture</h2> <h3 id="model-shrinking-hyperparameters-the-width-multiplier">Model Shrinking Hyperparameters: The Width Multiplier</h3> <p>A concept introduced with this hyperparameter to make the network much less computationally expensive was the <strong>width multiplier</strong>, \(\alpha\).</p> <p>The width multiplier’s job is to ensure that at each layer, the network remains thin. It’s value is set such that \(\alpha \in (0,1]\).</p> <p>So, considering the number of input channels to be \(n_c\), the computations become proportional to</p> \[\text{Computations}_{\text{MobileNet}} = (f \times f \times \alpha n_c \times n_{out} \times n_{out}) + (\alpha n_c \times n_{out} \times n_{out} \times \alpha n_f)\] <p>The computational costs here are reduced by \(\alpha^2\) times.</p> <p>The \(\alpha\) values lead to a computation and size tradeoff with a smooth accuracy dropoff until \(\alpha = 0.25\) after which the model becomes too small.</p> <h3 id="model-shrinking-hyperparameters-the-resolution-multiplier">Model Shrinking Hyperparameters: The Resolution Multiplier</h3> <p>The <strong>resolution multiplier</strong> is another hyperparameter here which helps reduce the computational costs of the neural network. It is denoted by \(\rho\) such that \(\rho \in [0,1)\).</p> <p>The resolution multiplier is applied to the image inputs to effectively decrease their resolution. The effective computation considering this hyperparameter along with the width multiplier is:</p> \[\text{Computations}_{\text{MobileNet}} = (f \times f \times \alpha n_c \times \rho n_{out} \times \rho n_{out}) + (\alpha n_c \times \rho n_{out} \times \rho n_{out} \times \alpha n_f)\] <p>The effective reduction in computational cost owing to this paramparameter is proportional to \(\rho^{2}\).</p> <h2 id="mobilenet-v2">MobileNet V2</h2> <p>The second version of the MobileNet architecture, MobileNet V2 <a class="citation" href="#DBLP:journals/corr/abs-1801-04381">(Sandler et al., 2018)</a> was introduced by a group of researchers in 2017, which made two changes to its structure:</p> <ul> <li>It added a resudual connection from each of the units’ input layer, passing the information directly to the later layers.</li> <li>It included an expansion layer before the depthwise separable convolution pair, known as a projection.</li> </ul> <p>Ever since depthwise seprarble convolutions became a thing, many neural network architectures have adopted this idea as a drop-in replacement for standard convolutions. This led to a reduction in computational costs proportional to \(f^{2}\) where \(f\) is the filter dimension.</p> <p>MobileNet V2 sets the value of \(f\) to \(3\), hence leading to the computational costs being approximately \(9\) times smaller than that of normal \(3 \times 3\) convolution blocks with a minimal reduction in the accuracy.</p> <h3 id="model-architecture">Model Architecture</h3> <p>The name of the block used here is the <strong>bottleneck block</strong>, and it is the basis of our model architecture. The main components of this block are a depthwise separable convolution with a residual connection and an additional Expansion layer before the depthwise separable layer. The structure of this block is shown below</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MobileNetV2.png" sizes="95vw"/> <img src="/assets/img/MobileNetV2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The <a class="citation" href="#Paul2018">(Pröve, 2018)</a> original MobileNet depthwise separable convolution block implements a \(1 \times 1\) filter followed by a depthwise convolution of size \(n \times n\) (in this case \(n = 3\)) and a pointwise convolution of dimensions \(1 \times 1\). This effectively follows a \(\text{Wide} \rightarrow \text{Narrow} \rightarrow \text{Wide}\) pattern when we look at the number of channels we are dealing with.</p> <p>In MobileNet V2 the block follows a \(\text{Narrow} \rightarrow \text{Wide} \rightarrow \text{Narrow}\) approach, where the first step is to widen the network using \(1 \times 1\) convolutions after which we apply the \(3 \times 3\) depthwise convolution. This is because, by virtue of the behavior of the depthwise separable block, we are reducing the number of parameters we work with. The final \(1 \times 1\) convolution layer, known as the <strong>projection</strong> layer, helps bring the network back to the same number of initial channels.</p> <p>The last step brings down the dimensions of inputs \(k\) to \(k'\) due to which it gets the name <strong>projection</strong> layer.</p> <h4 id="the-introduction-of-relu6">The introduction of ReLU6</h4> <p>An interesting change that was implemented in this architecture was the implementation of ReLU6 as opposed to normal ReLU.</p> \[\begin{align*} &amp; ReLU = max(0,x) \\ &amp; ReLU6 = min(max(0,x),6) \end{align*}\] <p>a This function ensures that the output is linear as long as it lies between \(0\) and \(6\). In contrast, normal ReLU ensures that the output is linear for values greater than equal to \(0\).</p> <h3 id="what-the-bottleneck-block-accomplishes">What the Bottleneck Block Accomplishes</h3> <p>The main things that the bottleneck block aims to accomplish are:</p> <ul> <li>The pointwise convolution/projection reducces the dimensionality of the input features to a smaller set of values before extracting new features from them. Hence, this leads to an increase in efficiency of the computations while maintaining a good level of accuracy.</li> <li>Residual connections here help improve the flow of information in the network.</li> <li>The effective number of matrix multiplication actions being performed is reduced significantly</li> </ul> <p>The final network proposed by Google had an efficiency comparable to NASNet, the state of the art model at the time whose building blocks were developed by another neural network, while also being easily explainable in its working as opposed to NASNet.</p> <h1 id="efficientnet">EfficientNet</h1> <p>The EfficientNet paper <a class="citation" href="#DBLP:journals/corr/abs-1905-11946">(Tan &amp; Le, 2019)</a>, rather than introducing super novel and complex architectures, focuses more on model scaling methods to see how one can balance the model depth, width and resolution to improve the overall performance of the network.</p> <p>The idea of how one must go about scaling up convolutional neural networks was never set in stone and there are many ways one can go about it. The main question this paper poses is:</p> <blockquote> <p>Is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?</p> </blockquote> <p>The authors of this paper proposed a <em>compound scaling method</em> where the various dimensions of the network, namely the</p> <ul> <li><strong>Width</strong> of the layers</li> <li><strong>Depth</strong> of the layers</li> <li><strong>Resolution</strong> of tje images were uniformly scaled by fixed ratios as opposed to arbitrary scales.</li> </ul> <p>So, based on the resources available, these dimensions would have to be scaled up based on a grid search using the initial model.</p> <p>This compound scaling method is used on already existing networks like MobileNet and ResNets to obtain the <strong>EfficientNet</strong> family of models, which outperform other ConvNets by a large margin.</p> <h1 id="using-convnets-in-the-real-world">Using ConvNets in the Real World</h1> <h2 id="using-open-source-implementations-of-cnn-architectures">Using Open Source Implementations of CNN Architectures</h2> <p>Most of the CNN architectures discussed here are finnicky to work with and difficult to replicate as they involve a ton of hyperparameter tuning which can affect the performance. I addition they also implement several tricks to ensure a high amount of efficiency on different CPU/GPU architectures. Due to this, in most cases it wouldn’t even make sense to implement these from scratch unless you’re masochistic to a certain extent.</p> <p>It is a good idea to use open source implementations from sources like GitHub, and Huggingface to build up on already existing work. This helps us get going with our ideas much faster than if we were to implement these from scratch.</p> <h2 id="implementing-transfer-learning">Implementing Transfer Learning</h2> <p>There has been loads of work done in the field of computer vision, due to which there is always a chance that someone has already pre-trained a model to perform a task similar to what you are working on. Like for instance, if we look at AlexNet, its uses aren’t fixed just to identify stuff from the ImageNet dataset. It has been widely used in several domains of research and development.</p> <p>The weights and biases of a pre-trained model, owing to the predefined patterns learned from the previous datasets trained on, can now be trained on our current data to suit our use case. This is known as <strong>transfer learning</strong> and is widely used to help reduce computational costs.</p> <p>In most cases the pretrained model has the original output classifier layer replaced with the softmax classifier for the current use case and trained a bit till the desired level of accuracy is obtained.</p> <p>For larger datasets, it is not as straightforward. We would need to freeze some of the lower layer values and only focus on training the later layers in the network. The larger the amount of data we have, the more the number of later layers we will have to train. In the case of a massive dataset, the whole network implementation itself with the pre-trained weights and biases can be used as the initialization (If you notice, this is a stand in for random initialization) and the whole network can be trained using these.</p> <h2 id="data-augmentation">Data Augmentation</h2> <p>CV tasks require a lot of data owing to the complexity of the functions that need to be learned. In a lot of cases, it is not easy to procure new data from our sources. Hence, this is where data augmentation comes into the picture to help improve the CV system.</p> <p>Some common data augmentation methods used in computer vision are</p> <ul> <li>Mirroring on the vertical axis</li> <li>Random cropping of the input images (assuming that the croppings are a reasonably large subset of the parent image)</li> <li>Rotation, shearing, local warping, etc.</li> <li>Color shifting: adding/subtracting to and from the RGB color channels of the image. These changes can be from a miniscule distribution of possible values. This makes the algorithm more robust to changes in color of the images. <ul> <li>PCA color augmentation is one method used and has been implemented in AlexNet.</li> </ul> </li> </ul> <p>These all have their own set of hyperparameters to choose from.</p> <h1 id="conclusion">Conclusion</h1> <p>In this blogpost we have been introduced to the basic mathematical and architectural concepts behind a lot of famous computer vision applications, their implementations, and all the innovations and improvements that have been made on them over the years. All the concepts covered here are only just scratching the surface when it comes to this field of study. Computer vision is a really nuanced field with several subdomains that are actively being researched, and we have delved a bit into one of the domains here, image recognition.</p> <p>In the following blog posts we will cover object detection, which is another subdomain of computer vision which is often paired with image recognition for various real-world applications.</p> ]]></content><author><name></name></author><category term="deep-learning"/><category term="machine-learning"/><category term="deep-learning"/><category term="computer-vision"/><summary type="html"><![CDATA[Topics covered: Convolutional Neural Network Foundations, Case Studies.]]></summary></entry><entry><title type="html">Deep Learning - Improving Deep Neural Networks</title><link href="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-2/" rel="alternate" type="text/html" title="Deep Learning - Improving Deep Neural Networks"/><published>2024-10-22T10:58:00+00:00</published><updated>2024-10-22T10:58:00+00:00</updated><id>https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-2</id><content type="html" xml:base="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-2/"><![CDATA[<p>This blog post is the second of a series of posts covering deep learning from the fundamentals up to more complex applications.</p> <p>Here we will look at the practical aspects of deep learning when training deep networks, covering topics such as regularization, optimization algorithms, hyperparameter tuning, batch normalization and more, which will equip us with knowledge needed when implementing deep neural networks in the real world.</p> <hr/> <h1 id="practical-aspects-of-deep-learning">Practical Aspects of Deep Learning</h1> <p>The performance of our machine learning and neural network models depend a lot on how our dataset is set up.</p> <p>In practice the development of machine and deep learning models is an <strong>iterative process</strong> starting with an <strong>idea</strong>, which is then implemented in <strong>code</strong> using which we <strong>experiment</strong> on our idea to see how our implementation works. We can than refine the idea we have from the outputs of our experiment, leading to a positive feedback loop with the goal of improving our neural network.</p> <p>Some of the factors we work with in deep learning include the depth of the network, number of hidden units per layer, the learning rates, regularization terms, activation functions and so on.</p> <p>Given that deep learning has found success in many different domains, it has been found that the knowledge transferability between different domains is limited. For example, someone working in natural language processing would find that the intuitions from that field would not work the best in computer vision. This could be due to the difference in the parameter values being used used as well as the model architectures being used in the fields, with sequential models being used for NLP and spatial hierarchies being used in computer vision.</p> <h2 id="splitting-up-our-data">Splitting Up Our Data</h2> <p>The traditional machine learning approach to split data is to split it such that one part acts as the <strong>training set</strong>, one as the <strong>hold-out cross validation</strong> set (also known as the <strong>development set</strong>) and the <strong>test set</strong>.</p> <p>The training set, as the name suggests, is used to train our algorithms, the dev set helps us see which model performs the best and improve it accordingly, and the test set is used to get the final unbiased estimate on how well the algorithm is doing.</p> <p>The common split used to be a <strong>70/30</strong> train/test split or a <strong>60/20/20</strong> train/dev/test split.</p> <p>However, with the advent of Big Data, with millions of data points to work with, using a much smaller percentage of the total dataset for the dev and test sets is preferred. This split would be more in the vein of a <strong>98/1/1</strong> split or even lower, like <strong>99.5/0.25/0.25</strong> for much larger datasets.</p> <p>This is because the dev sets’ use is only to check which model works well and to make a choice. Using a large amount of the data for this would lead to us missing out on information that could be used to train our model.</p> <p>Similarly the test set’s job is only to tell us how our final model is working.</p> <h2 id="ensuring-consistency-in-devtest-distributions">Ensuring Consistency in Dev/Test Distributions</h2> <p>The data we train our model on can come from various sources such as images from the web and user submitted images for a computer vision application. These images can have vast differences in terms of their content and quality, hence leading to a huge difference in the distributions of the data.</p> <p>The general rule of thumb is to ensure that the dev and test sets are are from the same distributions to ensure that the models’ performance metrics obtained on the dev set are representative of the its performance on the test set.</p> <h2 id="the-test-set">The Test Set</h2> <p>The test set gives us an unbiased view into how our model performs. However, in some cases, it is alright not use it altogether.</p> <p>However, not using a test set leads to the risk of reduced performance of the model due to overfitting. Hence, it is better to use the test set whenever necessary and avoid using it excessively when tuning our parameters.</p> <p>It is important that the model we are training is iterated upon rapidly. The faster a model can be trained, the quicker we can reach the optimal performance.</p> <p>This can be done so through effectively setting up train/dev/test sets.</p> <h2 id="bias-variance">Bias/ Variance</h2> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dataset.jpg" sizes="95vw"/> <img src="/assets/img/dataset.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Examples of bias and variance effects on the mode (This image was drawn referencing Coursera) </div> <p><strong>Bias</strong>: A high bias error occurs due to an overly simple model fitting the data, such as a simple linear or logistic regression curve, leading to it not capturing the true patterns in the data, hence performing poorly. This can also be seen as <strong>underfitting</strong>.</p> <p><strong>Variance</strong>: A high variance error occurs when the model developed has a high level of complexity, leading to a large amount of noise being included in the models’ predictions. This leads to a model highly tailored to the training data, leading to poor performance on the testing data. This is also known as <strong>overfitting</strong>.</p> <p>With the advent of deep learning and the ability to build deep networks, it has become possible to reduce both the bias and variance. This leads us away from the classical machine learning notion of there being an unavoidable trade-off between the bias and variance.</p> <h3 id="diagnosing-bias-and-variance-issues-using-errors">Diagnosing bias and variance issues using errors</h3> <p>A low <strong>training set error</strong> can be an indicator that the model fits the training data well but, by itself it doesn’t tell us whether our model is overfitting the data.</p> <p>This is where the <strong>development set</strong> comes in. This set helps is understand whether our model does a good job at generalizing over unknown data.</p> <p>A few scenarios we can look at regarding the training and dev set errors are:</p> <ul> <li>A <strong>low training and high dev error</strong> is indicative of our model <em>overfitting</em> the data, having a <em>high variance</em>.</li> <li>A <strong>high training and dev error</strong> indicates that our model is <em>underfitting</em> the data suggesting a <em>high bias</em>.</li> <li>If <strong>both errors are high and close</strong> then the model may have <em>both a high bias and variance</em>.</li> <li>If <strong>both errors are low</strong>, this is the ideal outcome which tells us that our model is <em>performing well</em>.</li> </ul> <p>The analyses of these errors are based on the assumption that the human error or the lower limit of possible error for the model, a.k.a <strong>Bayes Error</strong> is \(0\).</p> <p>If the Bayes error is higher then we can say that some of our created models are reasonable to use.</p> <p>The main takeaway here is that recognizing and diagnosing whether our model suffers from a high bias or variance is crucial to ensuring our model fits the training data and generalizes well to new data. We can easily diagnose these from the training and dev errors alongside the Bayes and human errors.</p> <h3 id="addressing-bias-and-variance-issues-in-neural-networks">Addressing bias and variance issues in neural networks</h3> <p>A <strong>high bias</strong> suggests that our model is too simple and is underfitting the data. The solutions for this could be:</p> <ul> <li><strong>Using a bigger network</strong>: This can be achieved by adding more layers or hidden units to our network. This makes it possible to capture more complex patterns in our data but also makes our model more computationally expensive to train.</li> <li><strong>Prolonged training</strong>: Training our model for a larger number of iterations gives it more time to capture additional information from the data, hence leading to a better fit. However, one must be careful as a model left to train for too long could potentially lead to overfitting.</li> <li><strong>Advanced Optimization Algorithms</strong>: These are methods that can speed up the process of training our model, helping it converge and fit the data better. <a href="#optimization-algorithms">Click here</a> to learn more about them.</li> <li><strong>Trying other network architectures</strong>: Some problems may call for the use of different neural network structures. It is always a good practice to experiment with different architectures to see which ones fit.</li> </ul> <p>A <strong>high variance</strong> tells us that our model is too complex and is capturing noise instead of the underlying pattern in the data, hence leading to overfitting. Some solutions for this could be:</p> <ul> <li><strong>Procuring more data</strong>: Obtaining more data for our model to work on is one of the most effective ways to combat a high variance. This is because it gives the model more examples to generalize from.</li> <li><strong>Regularization</strong>: Regularization penalizes our model when it becomes overly complex, hence reducing overfitting.</li> <li><strong>Network Architecture Tweaking</strong>: Similar to solving the bias problem, tweaking the network architecture is also useful in reducing the variance.</li> </ul> <h3 id="bias-variance-trade-off">Bias-Variance Trade-off</h3> <p>Classical machine learning and statistics sticks to the belief that the bias and variance are intrinsically linked, and a decrease in bias would increase the variance and vice versa. This is known as the <strong>bias-variance trade-off</strong>.</p> <p>In the era of deep learning, a large network is enough to reduce the bias when working with larger volumes of data without affecting the variance as, long as regularization is implemented properly.</p> <h1 id="regularization">Regularization</h1> <p><strong>Regularization</strong> is one of the fundamental techniques used in machine learning to reduce the complexity of the model being developed, hence preventing it from overfitting the data.</p> <p>Obtaining more training data is normally what one would do if they wanted to combat the variance, however, obtaining more training data in the real-world is not always feasible. This is where regularization finds its role.</p> <p>For this section we will use logistic regression as an example. We have already seen in the <a href="/blog/2024/deep-learning-self-study-1/">previous blog</a>, that the main goal of logistic regression is to minimize the cost function \(J(w,b)\) based on the model’s decisions</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})\] <p>where \(w \in \mathbb{R}^{n_{x}}\) and \(b \in \mathbb{R}\).</p> <p>We will now look at how regularization can help deal with the variance issue in this example.</p> <h2 id="l2-regularization">L2 Regularization</h2> <p>L2 regularization adds an additional term to the logistic regression cost function as follows:</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \boxed{\frac{\lambda}{2m}\|w\|_{2}^{2}}\] <p>where</p> \[\boxed{\|w\|_{2}^{2} = \sum\limits_{j=1}^{n_{x}}w_{j}^{2} = w^{T}w}\] <p>In this additional term, \(\lambda\) is known as the regularization parameter and \(\|w\|_{2}^{2}\) is the square of the Euclidean norm of the weight vector \(w\).</p> <p>We only normalize \(w\) as it is commonly a high dimensional vector with many more parameters while the bias \(b\) is just a single number. Hence changing the weight vector has more of a significant effect on the overall model than the bias.</p> <p>L2 regularization is the most commonly used type of regularization and is also known as <strong>weight decay</strong>.</p> <h2 id="l1-regularization">L1 Regularization</h2> <p>In the case of L1 regularization, the cost function is modified ass follows</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \boxed{\frac{\lambda}{m}\sum\limits_{i=1}^{n_{x}}\|w\|_{1}}\] <p>where</p> \[\boxed{|w||_{1} = \sum\limits_{i=1}^{n_{x}} \lvert w \rvert}\] <p>L1 regularization adds the absolute value of the weight vectors to the weight parameters as opposed to L2 regularization. This leads to sparse weight vectors as many of the parameters are set to zero.</p> <p>As a result, the overall model produced is compressed due to many of the nodes effectively being removed.</p> <p>L1 regularization is not as effective in comparison to L2 regularization due to which it is not used as much in comparison.</p> <h4 id="the-role-of-lambda">The role of \(\lambda\)</h4> <p>The <strong>regularization parameter</strong> helps balance the trade-off we make between fitting the training data and preventing overfitting by keeping the weights small.</p> <h2 id="regularization-in-neural-networks">Regularization in Neural Networks</h2> <p>In a neural network, the cost function is a function that encompasses all the parameters across all the layers in our model. It is given by</p> \[J(w^{[1]},b^{[1]},\dots,w^{[L]},b^{[L]}) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}\|w^{l}\|^{2}\] <p>Where</p> \[\|w^{[l]}\|_{F} = \sum_{i=1}^{n^{[l]}}\sum\limits_{j=1}^{n^{[l-1]}}(w^{[l]}_{ij})^{2}\] <p>The squared norm here, which is the sum of squares of all the entries, is known as the <strong>Frobenius norm</strong>.</p> <h4 id="gradient-descent-with-regularization">Gradient descent with regularization</h4> <p>Similar to how we would normally go about it, we calculate <code class="language-plaintext highlighter-rouge">dw</code> using the backpropagation, taking the partial derivatives of the cost function with respect to \(w\) for any given \(l\) layer. The only difference is that in this case, we add the regularization term to the resultant term.</p> \[dW^{[l]} = \frac{\partial{J}}{\partial{W^{[l]}}} + \frac{\lambda}{m}W^{[l]}\] <p>using which we update the parameters as follows</p> \[W^{[l]} := W^{[l]} - \alpha \left[ dW^{[l]} + \frac{\lambda}{m}W^{l} \right]\] \[\Rightarrow W^{[l]} := W^{[l]} \left[ 1 - \alpha\frac{\lambda}{m}W^{l} \right] - \alpha dW^{[l]}\] <p>Here, we <strong>effectively reduce the values of the matrix</strong> a bit as we are reducing the \(W^{[l]}\) values by a factor of \([1 - \alpha\frac{\lambda}{m}W^{l}]\). This justifies the name “weight decay” for L2 normalization.</p> <h2 id="how-regularization-works">How Regularization works</h2> <p>Consider a large neural network which is overfitting with the cost function</p> \[J(W^{[l]},b^{[l]}) = \frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})\] <p>When using the regularization term, the cost function becomes</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||w^{l}||^{2}\] <p>The way regularization works is, if we crank our regularization parameter \(\lambda\) to be really big, we are incentivized to set the weight matrices \(W\) close to 0.</p> <p>The idea is that if the weights are set extremely low, a lot of the hidden units will effectively be zeroed out, hence leading to them having not much of an impact on the output received.</p> <p>This effectively gives us a much smaller neural network, hence taking us from overfitting the data more towards increasing the bias.</p> <h2 id="dropout-regularization">Dropout Regularization</h2> <p>In addition to L2 regularization, another regularization method used is called dropout regularization.</p> <h3 id="intuition-behind-dropout">Intuition behind dropout</h3> <p>Consider a neural network that is overfitting its data.</p> <p>During the forward pass, dropout goes through each of the layers, setting a probability to each of the nodes to temporarily remove/disable it, or “drop it out”. After this, backpropagation is performed on the diminished network. This leads to training being done on a slightly different network on every pass.</p> <p>Looking at it from the point of view from a single unit taking in \(4\) inputs to generate a meaningful output, dropout randomly eliminates some of the inputs at every iteration.</p> <p>Due to this, with every iteration some of the input features can be present or absent at random. This leads to the node not relying on any one feature as it may or may not be present in subsequent iterations of the training process.</p> <p>This leads to an overall shrinkage in the squared norm of the weights.</p> <p>Dropout can be seen as an adaptive form of L2 regularization but the L2 penalty on different weights are different depending on the size of activations being multiplied into the weight.</p> <p>Dropout is done with the help of a parameter called the <strong>keep probability</strong>, based on which a coin toss is done on every node during the training and used to decide whether a given node will be kept or dropped from the network during a given iteration of training.</p> <h3 id="downsides-of-dropout">Downsides of Dropout</h3> <ul> <li>Despite every iteration of dropout learning being faster, we will need to train the network for a larger number of epochs to converge. This is because on every epoch, the training algorithm works on a subset of the whole network.</li> <li>Dropout only works well for larger networks and big datasets where overfitting is a huge possibility. For smaller networks and datasets dropout is not of much benefit and can harm the performance.</li> <li>Dropout adds an extra tuning parameter to work with, that is, the keep probability, which is used as the probability for dropping a node in the network.</li> </ul> <h2 id="other-regularization-methods">Other Regularization Methods</h2> <h3 id="data-augmentation">Data Augmentation</h3> <p><strong>Data Augmentation</strong> is the process of generating new data from pre-existing data to train our deep learning models. This is usually implemented when the data at hand is extremely limited in quantity and procuring new data is not feasible.</p> <p>This adds a bit of redundancy to our dataset as it is not as effective as collecting new data, but it is cost efficient.</p> <p>Some examples of data augmentation are:</p> <ul> <li>Flipping images horizontally</li> <li>Randomly cropping the image and distorting it</li> <li>Adding random rotations and distortions to images in the case of OCR.</li> </ul> <h3 id="early-stopping">Early Stopping</h3> <p>When training a machine learning algorithm or neural network with an iterative algorithm like gradient descent, the general trend we see is that the cost function decreases up to a certain point after which it begins to increase, which is indicative of the model overfitting the data.</p> <p><strong>Early stopping</strong> involves stopping the training process when the error begins to increase.</p> <h4 id="why-early-stopping-works">Why early stopping works</h4> <p>Early stopping works because, during the beginning of the training process, the weights are close to zero, giving us a simple model.</p> <p>As the training process increases these values and is eventually stopped, the value of the parameters will be relatively small, which leads to an effect similar to L2 regularization.</p> <h4 id="downsides-of-early-stopping">Downsides of early stopping</h4> <ul> <li>One of the downsides of early stopping is <strong>orthogonalization</strong>. It is always good to have tools that focus on performing a single task at a time. early stopping combines the task of reducing the cost function and preventing overfitting.</li> <li>Early stopping already has an alternative, which is using L2 regularization and training the neural network extensively.</li> </ul> <p>The advantage of early stopping is that we get to try our the training process for varying network sizes without having to practice L2 regularization which involves an extra hyperparameter (the regularization parameter \(\lambda\)).</p> <h1 id="normalizing-the-input-features">Normalizing the Input Features</h1> <p><strong>Normalization</strong> is one of the methods which we can use to speed up the training process of out neural network.</p> <p>It is used when the input features have highly differing scales which can lead to the cost function being distorted, hence causing the optimization algorithm’s path to be inefficient.</p> <h2 id="steps-to-normalize-a-dataset">Steps to normalize a dataset</h2> <p>Consider a dataset with input features as shown</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unnormalized_data.jpg" sizes="95vw"/> <img src="/assets/img/unnormalized_data.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We aim to normalize this dataset this data such that it is suitable for our deep learning model to work on. The steps to do so are as follows.</p> <ul> <li><strong>Zero centering</strong>: Here the mean of the features in question in training set is calculated and then subtracted from every instance of that feature in the training dataset. The resulting dataset has a <strong>zero mean</strong>.</li> </ul> \[\text{mean }(\mu) = \frac{1}{m}\sum\limits_{i=1}^{m}X^{(i)}\] \[\text{Zero centered data }(X) := X - \mu\] <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zeroed_mean.jpg" sizes="95vw"/> <img src="/assets/img/zeroed_mean.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The example dataset with zero mean. </div> <ul> <li><strong>Normalizing the variances</strong>: The variance \((\sigma^2)\) of the zero center dataset is now computed, using which we can normalize the data. The process is as follows:</li> </ul> \[\sigma^{2} = \frac{1}{m}\sum\limits_{i=1}^{m}X^{(i)}**2 \;\;(\text{ **} \Rightarrow \text{element-wise squaring})\] \[X := \frac{X}{\sigma}\] <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/variances_normalized.jpg" sizes="95vw"/> <img src="/assets/img/variances_normalized.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The dataset after normalizing the variances </div> <p>When doing this, the same \(\mu\) and \(\sigma^{2}\) should be used to normalize the test set. This is because we want the training and test set to go through the exact same transformations defined by the same \(\mu\) and \(\sigma^{2}\) calculated on our training data.</p> <h2 id="reasons-for-normalizing-the-inputs">Reasons for Normalizing the Inputs</h2> <p>Without normalization, the cost function would resemble an elongated bowl as a result of features possessing extremely different scales.</p> <p>Normalizing the input data will lead to the cost function being more uniform. This leads to the model being easier to train with relatively larger learning rates.</p> <p>Normalizing the input features is crucial when the input features have extremely different ranges.</p> <h1 id="vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</h1> <p><strong>Vanishing and Exploding Gradients</strong> are some of the most common issues faced when training deep learning models. Here, the slopes/derivatives can either get very big or very small as the network gets more complex, making the training process difficult.</p> <p>When training a neural network, with every subsequent activation, the outputs can either grow (explode) or reduce (shrink) exponentially due to the weight matrices being scaled slightly smaller or larger with each layer that is passed. The same is observed during the backpropagation process.</p> <p>A potential solution to this is careful <strong>weight initialization</strong>.</p> <h2 id="weight-initialization-in-deep-neural-networks">Weight Initialization in Deep Neural Networks</h2> <p>Taking a single neuron as an example with \(n\) inputs such that the linear part is the weighted sum, ignoring the bias, given by</p> \[Z = W_{1}X_{1} + W_{2}X_{2}+ W_{3}X_{3}+ \dots + W_{n}X_{n}\] <p>To ensure that \(Z\) does not blow up or become too small, the larger the \(n\) is, the smaller the weights \(W_{i}\) are.</p> <p>In practice, using python this is done so as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W_</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">n[l-1]</code> is the number of units from the (l-1)th layer being fed into the current layer.</p> <p>A reasonable thing to do would be to set the variance of the weights to \(1/n\) where \(n\) is the number of input features going into the neuron.</p> <p>For a ReLU activation function, a variance of \(2/n\) works a bit better. This is done by taking a Gaussian random variable and multiplying it with \(\sqrt{2/n}\), hence setting the variance to \(2/n\).</p> <p>If the input activations are roughly mean \(0\) and standard variance \(1\), \(Z\) will also take on a similar scale, hence helping reduce the issue with vanishing and exploding gradients problem as it sets each of the weight matrices \(W\) so that it is not too much bigger than 1 and not too less than 1, hence preventing vanishing or exploding too quickly.</p> <h3 id="other-variants-of-random-initialization">Other variants of random initialization</h3> <p>The previous section assumes the use of the ReLU activation function. However, in the case of other activation functions we use different values.</p> <p>For the tanh activation function \(\sqrt{1/n}\) is more suitable. This is called <strong>Xavier Initialization</strong>.</p> <p>Another value proposed by Yoshua Bengio and his colleagues is</p> \[\sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}\] <p>These all give us a starting point for the variance of the initialization of the weight matrices.</p> <p>The variance is another hyperparameter that can be tuned, which can have a moderate effect on the models’ performance.</p> <h1 id="gradient-checking">Gradient Checking</h1> <p><strong>Gradient Checking</strong> is a method used to verify that the gradients calculated during the backpropagation process are correct.</p> <p>In this process we approximate the gradient of each parameter and tweak the value slightly to observe the change in the cost function. If the computed gradient and its approximation have close values, then the backpropagation process done is correct.</p> <p>Consider a cost function where instead of the it being \(J(W^{[1]},b^{[1]}, \dots)\), we have \(J(\theta)\).</p> <p>Similarly, the derivatives \(dW^{[1]}, db^{[1]}, dW^{[2]}, db^{[2]} \dots\) can be reshaped into a big vector \(d\theta\).</p> <p>Now, the main question we have is whether \(d\theta\) the slope of cost function \(J(\theta)\). This is where gradient checking is useful.</p> <p>The process is as follows</p> <p>We first calculate</p> \[\begin{align*}\\ &amp;for \; each \; i:\\ &amp;\;\;\;\; d\theta_{approx}[i] = \frac{J(\theta_{1},\theta_{2},\dots, \theta_{i}+\epsilon,\dots) - J(\theta_{1},\theta_{2},\dots, \theta_{i}-\epsilon,\dots)}{2\theta} \end{align*}\] <p>This gives us two vectors to work with, \(d\theta_{approx}\) and \(d\theta\), both of which have the same dimensions. We now check to see whether both these vectors are approximately equal to one another. This is mainly done through the Euclidean distance between the two</p> \[\frac{||d\theta_{approx} - d\theta||_{2}}{||d\theta_{approx}||_{2} + ||d\theta||_{2}}\] <p>In practice, we could use \(\epsilon=10^{-7}\), and if the above gives us a value approximately equal to \(10^{-7}\), our derivative approximation is very likely correct. If it is \(10^{-5}\) or higher, we have to check if any components are too large and if some of the components’ difference are very large, we might have a bug somewhere.</p> <p>If the value obtained goes up to \(10^{-3}\), then there is definitely a reason for concern. We will have to check the individual components to see whether a specific value of \(i\) for which \(d\theta_{i}\) is very small and use that to track down if some of our derivatives are incorrect.</p> <h2 id="practical-tips-when-working-with-gradient-checking">Practical Tips when working with Gradient Checking</h2> <ul> <li><strong>Do not use grad checking during training</strong>: Computing the approximated gradients \(d\theta_{approx}[i]\) for every \(i\) is computationally intensive. We only use this to confirm that our approximated gradients are close to the actual backprop gradients \(d\theta\) after which it is deactivated.</li> <li><strong>Debug the algorithm with component analysis</strong>: If \(d\theta_{approx}\) is very far from \(d\theta\), we must analyze each of the components in our network to see which values of \(d\theta_{approx}\) differ to a large extent from \(d\theta\). This doesn’t identify the bug right away but it helps us home in on the location of the bug.</li> <li><strong>Regularization</strong>: We must not forget the regularization term if we are regularizing our cost function during backpropagation. The gradients should be calculated for the entire cost function including the regularization term.</li> <li><strong>Issues working with Dropout</strong>: Gradient checking will not work with dropout. This is because every iteration of dropout randomly eliminates different subsets of hidden units. As a result there is no easy cost function \(J\) that the dropout can do gradient descent on. A good practice is to turn off dropout when performing gradient checking. We can also fix the pattern of nodes dropped and verify that grad check for the pattern of units dropped is correct.</li> <li><strong>The need for more training iterations</strong>: We might see that backpropagation is more accurate when the weights and biases’ values are close to zero. To counter this we have to run gradient checking for several iterations, allowing the weights and biases to move away from their initial values.</li> </ul> <h1 id="optimization-algorithms">Optimization Algorithms</h1> <p>Optimization algorithms are methods that allow us to train our neural networks faster.</p> <p>Given that machine and deep learning are highly empirical, we have the need to train a lot of models to find one that works well. Hence, it is helpful to be able to train models quickly. However this process is slow owing to the size of the datasets usually used when training deep learning models. Here we will be looking at some optimization methods that help with hastening the training process.</p> <h2 id="mini-batch-gradient-descent">Mini Batch Gradient Descent</h2> <p>In the real world, training neural networks is slow due to the sheer size of the dataset being worked with. Performing normal gradient descent on this whole dataset would not be feasible as it would take forever for even a single iteration to complete.</p> <p>To combat this we divide the training set into smaller sets called <strong>mini batches</strong> and perform updates on each batch.</p> <h4 id="implementing-gradient-descent">Implementing Gradient Descent</h4> <ul> <li>Split the training data into mini batches of 1000 data points each such that <ul> <li>Mini batch 1: \(X_{1}, \dots, X_{1000}\)</li> <li>Mini batch 2: \(X_{1001}, \dots, X_{2000}\)</li> <li>and so on</li> </ul> </li> <li>For each mini-batch perform the forward propagation process, keeping it isolated from the other batches</li> <li>Compute the cost function for the mini batch.</li> <li>Compute the gradients through backpropagation</li> <li>Update the weights and biases accordingly</li> </ul> <h4 id="important-points-on-mini-batch-gradient-descent">Important points on mini batch gradient descent</h4> <ul> <li>Mini batch gradient descent allows us to effectively train our model despite being partially through the dataset as one single pass takes multiple gradient descent steps.</li> <li>Every iteration of this process should lead to a decrease in the cost function.</li> <li>Different training batches may not lead to a decrease in the cost function due to various factors such as the the learning rate, the dataset itself being trained on and other such difficulties.</li> </ul> <h4 id="behavior-of-gradient-descent-algorithms">Behavior of Gradient Descent Algorithms</h4> <p>Mini batch size is one of the many hyperparameters we will have to consider when designing our neural network architecture.</p> <p>When the size of the mini batch \(m\) is equal to the size of the training set, then the process is known as <strong>batch gradient descent</strong>.</p> <p>When the size of the mini batch is 1, then we end up with <strong>stochastic gradient descent</strong>.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/contour.jpg" sizes="95vw"/> <img src="/assets/img/contour.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Effects of different batch sizes on finding the minimum during the training. </div> <p>Batch gradient descent would take relatively low noise and large steps when moving towards the minimum. However, it is also the slowest method owing to the fact that it has to process the whole dataset in one go at each iteration.</p> <p>Stochastic gradient descent takes a gradient descent step with a single training example. Hence, most of the time we head towards the global minimum, but sometimes we can head in the wrong direction. As a result, it is an extremely noisy process but takes us in the correct direction eventually, while sometimes going in the wrong direction.</p> <p>Stochastic gradient won’t ever converge and will always oscillate around the region of the minimum.</p> <h4 id="choosing-mini-batch-size">Choosing mini batch size</h4> <p>For smaller training sets of less than 2000 examples, we use batch gradient descent.</p> <p>For larger sets, sizes between 64-512 (powers of 2) are used to ensure that it is compatible with the memory layout of the computer.</p> <p>We must always ensure that the mini batch size taken fits in the memory of the CPU or GPU being used.</p> <h1 id="exponentially-weighted-averages">Exponentially Weighted Averages</h1> <p>There exist a large number of algorithms that can perform better than gradient descent. We will now look at a few of them, but before that, we need to know about the underlying concept they are based on, which is known as <strong>exponentially weighted averages</strong>.</p> <p>Consider the temperature values in a town over a year. The data would normally be noisy. If we want to compute the trends then, as opposed to normal average, we have to make use of a <em>moving average</em> of the temperature to keep track of the day to day changes.</p> <p>If we took the initial weighted average to be \(v_{0}=0\), and on every day, we average it with a weight of \(0.9\) times the previous \(v\) value that appears and \(0.1\) times the given day’s temperature, we would get the weighted average calculated as follows</p> \[\begin{align*} v_{0} &amp;= 0\\ v_{1} &amp;= 0.9v_{0} + 0.1\theta_{1}\\ v_{2} &amp;= 0.9v_{1} + 0.1\theta_{2}\\ \vdots \end{align*}\] <p>The generalized formula for this is</p> \[v_{t} = 0.9v_{t - 1} + 0.1\theta_{t}\] <p>which is known as a moving average or an <strong>exponentially weighted average</strong> of the daily temperatures.</p> <p>Considering the reading for a given day \(t\) as \(\theta_{t}\) and the weighted average for a given day as \(v_{t}\), the general formula for weighted average is</p> \[\boxed{v_{t} = \beta v_{t - 1} + (1-\beta)\theta_{t}}\] <p>\(v_{t}\) can be thought as approximately averaging over \(\frac{1}{1-\beta}\) days’ temperature.</p> <p>For \(\beta = 0.9\), we think of this process as averaging over the last 10 days’ temperature.</p> <p>Similarly, if we had \(\beta = 0.98\), we would be averaging over, roughly the last 50 days.</p> <p>Hence, as is seen below, for a high beta value the average curve (green) is smoother as we are averaging over more days of temperature. On the flip side, this curve is also shifted towards the right as we are averaging over a much larger temperature window. Hence, this exponentially weighted average formula adapts more slowly.</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/year_temp.jpg" sizes="95vw"/> <img src="/assets/img/year_temp.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Temperature distributions over the year with the weighted averages plotted at different values of beta </div> <p>This leads to more latency as at a \(\beta\) value of \(0.98\) we are giving a lot of weight to the previous value and a smaller weight to the current value, hence leading to this average adapting more slowly.</p> <p>If we set \(\beta=0.5\), we would be averaging over just 2 days, giving us the yellow curve in the graph above which is much more noisy and susceptible to outliers, but also adapts much quickly to temperature changes.</p> <p>Hence, the \(\beta\) value here is yet another hyperparameter which can give us slightly different effects based on its value, and it is usually some value in between that would work best.</p> <h2 id="another-explanation-of-weighted-averages">Another explanation of weighted averages</h2> <p>The Exponentially Weighted Average (EWA) or Exponentially Weighted Moving Average (EWMA)<a class="citation" href="#tobias2022">(Chavarría, 2022)</a> is used as a smoothing technique in time series data, and in this case, as a way to smooth out the cost function of neural networks when training them using mini batch gradient descent.</p> <p>This algorithm depends on a <em>weight parameter</em> \(\beta\) which helps decide how much the current observation contributes to the total when calculating the EWA.</p> <p>So taking the equation of exponential averages, we get the mean calculation as</p> \[v_{t} = \beta v_{t - 1} + (1-\beta)\theta_{t}\] <p>where</p> <ul> <li>\(v_{t}\) is the EWA for the point of time/epoch \(t\)</li> <li>\(\theta_{t}\) is the current reading at \(t\)</li> <li>\(\beta\) is the weight parameter</li> </ul> <p>The EWA combines the current reading being taken with the previous readings iteratively as (taking \(\beta=0.9\))</p> \[\begin{align*} v_{1} &amp;= 0.9v_{0}+0.1\theta_{1} \\ v_{2} &amp;= 0.9v_{1}+0.1\theta_{2} \\ \vdots \\ v_{t} &amp;= 0.9v_{t-1}+0.1\theta_{t} \\ \end{align*}\] <p>Here</p> <ul> <li>\(\beta v_{t - 1}\) is the <strong>trend</strong></li> <li>\((1-\beta)\theta_{t}\) is the <strong>current value</strong></li> </ul> <p>A higher \(\beta\) value places a higher importance on the trend than the newer values being added to it, leading to the curve adapting more smoothly.</p> <p>A lower \(\beta\) value would make the EWA place more importance on tracking newer values hence making the curve more noisy.</p> <h4 id="what-is-beta">What is \(\beta\)?</h4> <p>\(\beta\) can be thought of as</p> \[n_{\beta} = \frac{1}{1-\beta}\] <p>which is the number of observations used to adapt the EWA.</p> <ul> <li>For \(\beta=0.9\), \(n\) would be 10 leading to a <strong>normally adapting</strong> curve.</li> <li>For \(\beta = 0.5\), \(n\) would be 50 leading to a <strong>slowly adapting</strong> curve.</li> <li>For \(\beta = 0.98\), \(n\) would be 2 leading to a <strong>quickly adapting</strong> curve.</li> </ul> <h2 id="implementing-exponentially-weighted-averages">Implementing Exponentially Weighted Averages</h2> <p>In pseudocode, the exponentially weighted average calculation process would be</p> \[\begin{align*} &amp;V_{\theta} = 0 \\ &amp;repeat\{ \\ &amp;\text{get next } \theta_{t} \\ &amp;\text{set } V_{\theta} := \beta V_{\theta} + (1-\beta)\theta_{t} \\ &amp;\} \end{align*}\] <h2 id="bias-correction">Bias correction</h2> <p>Bias correction can help us make the calculations of the weighted average more accurate.</p> <p>Taking the same example as before</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/year_temp.jpg" sizes="95vw"/> <img src="/assets/img/year_temp.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Temperature distributions over the year with the weighted averages plotted at different values of beta </div> <p>If we implement weighted average as</p> \[v_{t} = \beta v_{t-1} + (1 - \beta)\theta_{t}\] <p>we won’t actually get the green curve in the plot above, we would instead get a curve looking more like the purple curve shown below.</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Bias%20Correction.jpg" sizes="95vw"/> <img src="/assets/img/Bias%20Correction.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In this case the curve starts off very low.</p> <p>This is because, when we implement the moving average, we initialize the value with \(v_{0}=0\), taking \(\beta=0.98\) and then calculate</p> \[v_{1} = 0.98v_{0}+0.02\theta_{1} \Rightarrow 0.02\theta_{1}\] <p>So, if the initial value is \(\theta_{0}=40\), then \(v_{1}\) will be \(0.02 \times 40\) which is \(0.8\). This is not a good estimate of what the initial value could be. Similarly \(v_{2}\) would be</p> \[0.0196 \theta_{1} + 0.02\theta_{2}\] <p>which is again, not a good estimate.</p> <p>To modify this estimate to make it more accurate, instead of taking \(v_{t}\) we take</p> \[\frac{v_{t}}{1-\beta^t}\] <p>where \(t\) is the current data point we are observing.</p> <p>Considering the temperatures per day example from before, for \(t=2\), \(1-\beta^{t} = 1-0.98^{2}= 0.0396\). Considering the initial value to be \(40\), we get the estimate of the temperature on day 2 as</p> \[\frac{v_{2}}{0.0396} = \frac{0.0196\theta_{1} + 0.02\theta_{2}}{0.0396}\] <p>Notice that the two known values on the top add up to the denominator. This gives us the weighted average of \(\theta_{1}\) and \(\theta_{2}\) and removes the bias.</p> <p>As \(t\) becomes large \(\beta^{t}\) becomes close to \(0\), so the bias correction makes almost no difference.</p> <p>Hence, bias correction helps during the initial phases of learning when we are still warming up our estimates, to get a good estimate early on.</p> <h1 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h1> <p>This is the first optimization algorithm we will be looking at which implements weighted averages.</p> <p>Another name for this algorithm is <strong>momentum</strong>. This algorithm almost always works better than standard gradient descent.</p> <p>Here, we calculate a weighted average of the gradients. This aggregates the gradients for a given number of epochs, say, 10 epochs, into a single weighted average value which is used in place of the actual gradient value to calculate the new value of the parameters.</p> <p>This induces an effect of momentum where, when a new point is introduced to the given loss function, it won’t contribute too much to the effective average value. This prevents particularly sharp increases or decreases in the gradient values from affecting the effective trajectory of the loss function.</p> <p>As a result the overall gradient descent process is slowed down due to it preventing us from using a larger learning rate as it would diverge and overshoot the minimum.</p> <p>Another way to think of this is <em>minimizing the bowl shape of the function</em>. Consider a ball rolling down a hill. The derivative terms act as acceleration on the ball rolling downhill and the momentum terms represent velocity of the ball. \(\beta\) here acts as the friction preventing the ball from speeding up too much as the acceleration is imparted to it. As a result, rather than gradient descent taking every step independently of all the previous steps, the ball can gain momentum as a result of the aggregate velocity and the acceleration and friction acting on it.</p> <p>Consider an example where we are optimizing a cost function with the following contour.</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/contours.jpg" sizes="95vw"/> <img src="/assets/img/contours.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In the case of gradient descent, for every step, the cost function would zig zag till it gradually reaches the optimum.</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/contours_GD.jpg" sizes="95vw"/> <img src="/assets/img/contours_GD.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One way to look at this is that we want to damp the oscillations in the vertical axis by slowing down the learning in that direction while hastening the learning process in the horizontal direction.</p> <p>Each iteration of momentum goes through the following steps:</p> \[\begin{align*} &amp;\text{On iteration t:}\\ &amp;\;\;\;\;\text{Compute }dW,db\text{ on the current minibatch} \\ &amp;\;\;\;\;V_{dW} = \beta V_{dW} + (1-\beta)dW \\ &amp;\;\;\;\;V_{db} = \beta V_{db} + (1-\beta)db \\ &amp;\;\;\;\;W = W - \alpha V_{dW} \\ &amp;\;\;\;\;b = b - \alpha V_{db} \end{align*}\] <p>As a result the gradient descent steps are smoothed out</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/contour_momentum.jpg" sizes="95vw"/> <img src="/assets/img/contour_momentum.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="advantages-of-momentum">Advantages of Momentum</h2> <ul> <li>Averaging out the gradients cause the oscillations in the vertical direction to move closer to zero.</li> <li>Oscillations in the horizontal direction, the derivatives, remain aggressive, moving towards the minimum.</li> </ul> <p>As a result, the average in the horizontal direction is large.</p> <h2 id="implementing-momentum">Implementing Momentum</h2> <p>The implementation of momentum is as follows</p> \[\begin{align*} &amp;\text{On iteration t:}\\ &amp;\;\;\;\;\text{Compute }dW,db\text{ on the current minibatch} \\ &amp;\;\;\;\;V_{dW} = \beta V_{dW} + (1-\beta)dW \\ &amp;\;\;\;\;V_{db} = \beta V_{db} + (1-\beta)db \\ &amp;\;\;\;\;W = W - \alpha V_{dW}, \;\;b = b - \alpha V_{db} \end{align*}\] <p>This gives us two hyperparameters to work with: \(\alpha\) and \(\beta\).</p> <p>The most common value for \(\beta\) which controls the exponentially weighted average is \(\boxed{0.9}\). That is averaging over the last 10 iterations’ gradients.</p> <p>The bias correction, is not usually used because after 10 iterations or so, our moving average would have warmed up and would no longer need a bias estimate.</p> <p>In the above process \(V_{dW}\) and \(V_{db}\) are initialized to 0.</p> <h1 id="rmsprop">RMSProp</h1> <p>RMSProp stands for <strong>Root Mean Square Prop</strong>. This is the second optimization algorithm we are looking at which also implements a moving average.</p> <p>Taking the same example as before</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/contours.jpg" sizes="95vw"/> <img src="/assets/img/contours.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Consider the vertical direction to be parameter \(b\) and the horizontal direction to be parameter(s) \(W\).</p> <p>In RMSProp, we want to <strong>slow down the learning</strong> in the \(b\) <strong>vertical</strong> direction and speed <strong>up learning</strong> in the \(W\) <strong>horizontal</strong> direction.</p> <h2 id="implementing-rmsprop">Implementing RMSProp</h2> \[\begin{align*} &amp;\text{On iteration t:}\\ &amp;\;\;\;\;\text{Compute }dW,db\text{ on the current minibatch} \\ &amp;\;\;\;\;S_{dW} = \beta_{2} S_{dW} + (1-\beta_{2})dW^{2}\text{ (the squaring operation here is element-wise)} \\ &amp;\;\;\;\;S_{db} = \beta_{2} S_{db} + (1-\beta_{2})db^{2} \\ &amp;\;\;\;\;W = W - \alpha\frac{dW}{\sqrt{S_{dW} + \epsilon}}, \;\;b = b - \frac{db}{\sqrt{S_{db} + \epsilon}} \end{align*}\] <p>This process keeps an exponentially weighted average of the <strong>squares of the derivatives</strong>.</p> <p>Taking the weight updates here</p> \[W = W - \alpha\frac{dW}{\sqrt{S_{dW}+ \epsilon}}, \;\;b = b - \frac{db}{\sqrt{S_{db}+ \epsilon}}\] <p>We see that</p> <ul> <li>\(S_{dW}\) is meant to be relatively small to speed up updates in the horizontal direction</li> <li>\(S_{db}\) is meant to be relatively large to slow down updates in the vertical direction</li> <li>To ensure the algorithm doesn’t divide by zero in practice we add a very small \(\epsilon\) to the denominator. This enables better numerical stability and helps avoid dividing by zero and instead dividing by a very small value.</li> </ul> <h2 id="effects-of-rmsprop">Effects of RMSProp</h2> <p>Consider the image below</p> <div class="row justify-content-center mt-3"> <div class="col-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Learning_directions.jpg" sizes="95vw"/> <img src="/assets/img/Learning_directions.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Looking at the derivatives, the values are much larger in the vertical direction than the horizontal direction, that is, \(db\) is much larger than \(dW\) as the function has a steeper slope in the vertical direction.</p> <p>This algorithm divides the updates in the vertical direction with a value that is a much larger number, helping damp out the oscillations.</p> <p>The horizontal updates are divided by a smaller number. This leads to a net dampening of the updates.</p> <p>As a result of this, <strong>we can use a larger learning rate</strong>, and get a faster learning <strong>without the diverging in the vertical direction</strong>.</p> <p>RMSProp was proposed by a Coursera course by Jeff Hinton, not a paper.</p> <h1 id="adam-optimization">Adam Optimization</h1> <p>Adam optimization takes the ideas of Momentum and RMSProp and puts them together. It is widely used as it is robust and applicable to different deep learning architectures.</p> <h2 id="concepts-used">Concepts used</h2> <p>The two algorithms used here are</p> <ul> <li><strong>Momentum</strong> <ul> <li>This algorithm speeds up the optimization process by taking into account gradients from previous iterations.</li> </ul> \[V_{dW} = \beta_{1} V_{dW} + (1-\beta_{1})dW\] \[V_{db} = \beta_{1} V_{db} + (1-\beta_{1})db\] </li> <li><strong>RMSProp</strong> <ul> <li>This algorithm speeds up the optimization process by taking into account the moving average of the squared gradient.</li> </ul> \[S_{dW} = \beta_{2} S_{dW} + (1-\beta_{2})dW^{2}\] \[S_{db} = \beta_{2} S_{db} + (1-\beta_{2})db^{2}\] </li> <li><strong>Bias Correction</strong> <ul> <li>Bias correction is used in typical implementations of Adam. Look back to the <a href="#bias-correction">bias correction</a> section to learn why this is important.</li> <li>Bias correction for momentum</li> </ul> \[V_{\text{dW_corrected}} = \frac{V_{dW}}{1-\beta_1^T}\] \[V_{\text{db_corrected}} = \frac{V_{db}}{1-\beta_1^T}\] <ul> <li>Bias correction for RMSProp</li> </ul> \[V_{\text{dW_corrected}} = \frac{V_{dW}}{1-\beta_2^T}\] \[S_{\text{db_corrected}} = \frac{S_{db}}{1-\beta_2^T}\] </li> </ul> <h2 id="implementing-adam-optimization">Implementing Adam Optimization</h2> \[\begin{align*} \\ &amp;V_{dW} = 0, \;S_{dW} = 0,\; V_{db} = 0, \;S_{db} = 0 \\ &amp;\text{On iteration t:}\\ &amp;\;\;\;\;\text{Compute }dW,db\text{ on the current minibatch} \\ &amp;\;\;\;\;V_{dW} = \beta_{1} V_{dW} + (1-\beta_{1})dW,\;\; V_{db} = \beta_{1} V_{db} + (1-\beta_{1})db \\ &amp;\;\;\;\;S_{dW} = \beta_{2} S_{dW} + (1-\beta_{2})dW^{2},\;\;S_{db} = \beta_{2} S_{db} + (1-\beta_{2})db^{2} \text{ ( element-wise squaring)} \\ &amp;\;\;\;\;V_{dW}^{corrected} = V_{dW}/(1-\beta_{1}^{t}),\;\; V_{db}^{corrected} = V_{db}/(1-\beta_{1}^{t}) \\ &amp;\;\;\;\;S_{dW}^{corrected} = S_{dW}/(1-\beta_{2}^{t}),\;\; S_{db}^{corrected} = S_{db}/(1-\beta_{2}^{t}) \\ &amp;\;\;\;\;W = W - \alpha\frac{dW}{\sqrt{S_{dW}^{corrected}+\epsilon}}, \;\;b = b - \frac{db}{\sqrt{S_{db}^{corrected}+\epsilon}} \end{align*}\] <p>The hyperparameters here are</p> <ul> <li>\(\alpha\): <strong>The learning rate</strong>. This value will need to be tuned</li> <li>\(\beta_1\): <strong>Momentum term</strong>. This value is typically set to \(0.9\)</li> <li>\(\beta_2\): <strong>RMSProp term</strong>. This value is typically set to \(0.999\)</li> <li>\(\epsilon\): <strong>used to avoid division by zero</strong>. This value is typically set to \(10^{-8}\)</li> </ul> <h1 id="practically-implementing-the-optimization-algorithms">Practically Implementing the Optimization Algorithms</h1> <p>A lot of the explanations for optimization algorithms tend to get confusing if only read. It’s always good practice to see these in working. Here we will look at implementations of these learning algorithms from scratch and observe how they affect the learning process.</p> <p>In this example we make use of the matyas function given by</p> \[f(x) = 0.26(x^{2}+y^{2})-0.48xy\] <p>which has its global minimum at \((0,0)\).</p> <p>Before moving forward, we keep in mind the formulae for each of the optimization algorithms. These will be necessary to understand why these algorithms work.</p> <table> <thead> <tr> <th><strong>Optimization Algorithm</strong></th> <th><strong>Formula</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent</strong></td> <td>\(W = W - \alpha dW\) <br/> \(b = b - \alpha db\)</td> </tr> <tr> <td><strong>Momentum</strong></td> <td>\(V_{dW} = \beta V_{dW} + (1-\beta)dW\) <br/> \(V_{db} = \beta V_{db} + (1-\beta)db\) <br/> \(W = W - \alpha V_{dW}\) <br/> \(b = b - \alpha V_{db}\)</td> </tr> <tr> <td><strong>RMSProp</strong></td> <td>\(S_{dW} = \beta_2 S_{dW} + (1-\beta_2)dW^2\) <br/> \(S_{db} = \beta_2 S_{db} + (1-\beta_2)db^2\) <br/> \(W = W - \alpha \frac{dW}{\sqrt{S_{dW} + \epsilon}}\) <br/> \(b = b - \alpha \frac{db}{\sqrt{S_{db} + \epsilon}}\)</td> </tr> <tr> <td><strong>Adam</strong></td> <td>\(V_{dW} = \beta_1 V_{dW} + (1-\beta_1)dW\) <br/> \(V_{db} = \beta_1 V_{db} + (1-\beta_1)db\) <br/> \(S_{dW} = \beta_2 S_{dW} + (1-\beta_2)dW^2\) <br/> \(S_{db} = \beta_2 S_{db} + (1-\beta_2)db^2\) <br/> \(V_{dW}^{corrected} = \frac{V_{dW}}{1-\beta_1^t}\) <br/> \(V_{db}^{corrected} = \frac{V_{db}}{1-\beta_1^t}\) <br/> \(S_{dW}^{corrected} = \frac{S_{dW}}{1-\beta_2^t}\) <br/> \(S_{db}^{corrected} = \frac{S_{db}}{1-\beta_2^t}\) <br/> \(W = W - \alpha \frac{dW}{\sqrt{S_{dW}^{corrected} + \epsilon}}\) <br/> \(b = b - \alpha \frac{db}{\sqrt{S_{db}^{corrected} + \epsilon}}\)</td> </tr> </tbody> </table> <p><br/></p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/optimizer_demonstrations.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p><br/></p> <p>The animation generated from the above code is obtained as follows.</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/gradients_1.gif" sizes="95vw"/> <img src="/assets/video/gradients_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Zooming in a bit, we can observe the loss curves a bit clearer as the approach the minimum.</p> <div class="row justify-content-center mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/video/gradients_2.gif" sizes="95vw"/> <img src="/assets/video/gradients_2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="problems-faced-in-optimization">Problems faced in Optimization</h1> <h2 id="learning-rate-decay">Learning Rate Decay</h2> <p>One of the problems faced when optimizing our learning algorithm is reducing the learning rate over time, which is known as <strong>learning rate decay</strong>.</p> <h4 id="reason-for-implementation">Reason for implementation</h4> <p>Mini batch gradient descent with small mini batches tends to be noisy when moving towards the minimum due to using a fixed learning rate value.</p> <p>The solution for this is starting with a larger learning rate, which causes the algorithm to take to larger steps. We then eventually reduce the learning rate as we approach convergence.</p> <h4 id="learning-rate-decay-methods">Learning Rate Decay Methods</h4> <ul> <li>Standard decay: <ul> <li>Formula: \(\alpha = \frac{1}{1+\text{decay Rate}\times \text{epoch number}}\alpha_{0}\)</li> <li>The \(\alpha\) values generated per epoch would look something like</li> </ul> </li> </ul> <table> <thead> <tr> <th>Epoch</th> <th>\(\alpha\)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.1</td> </tr> <tr> <td>2</td> <td>0.067</td> </tr> <tr> <td>3</td> <td>0.05</td> </tr> <tr> <td>4</td> <td>0.04</td> </tr> </tbody> </table> <ul> <li>Exponential Decay <ul> <li>Formula: \(\alpha = 0.95^{epoch}\alpha_{0}\)</li> <li>The decay rate is much faster than standard decay due to use of an exponential scale</li> </ul> </li> <li>Other Formulae <ul> <li>\(\boxed{\alpha = \frac{k}{\sqrt{ epoch }}\alpha_{0}}\) where \(\alpha_{0}\) is the initial learning rate value.</li> <li>\(\boxed{\alpha = \frac{k}{\sqrt{ t }}\alpha_{0}}\) where \(t\) is the mini batch number.</li> </ul> </li> <li>Discrete Learning Rates <ul> <li>We can make it so that the learning rate decreases in discrete steps.</li> </ul> </li> <li>Manual Decay <ul> <li>If a model is taking too long, we can observe how the model is training and change the learning rate accordingly.</li> </ul> </li> </ul> <p>While learning rate decay can help speed up the training process, the primary learning rate has a more significant impact.</p> <h2 id="local-optima">Local Optima</h2> <p>When looking at cost function curves in the case of neural networks, most of the points of zero gradients are not local optima, they are in the form of <strong>saddle points</strong>.</p> <p>At higher dimensional spaces, for a zero gradient in each direction, we can have either a convex-like function or concave-like function. For a several thousand dimensional space, we would get something like a saddle shaped curve.</p> <p><strong>Plateaus</strong> in these saddle curves can slow down the learning as it leads to the derivatives in those regions being close to zero for a very long time.</p> <p>Optimization algorithms like RMSProp and Momentum are useful to help with this problem.</p> <h1 id="hyperparameter-tuning">Hyperparameter Tuning</h1> <p>Some of the common hyperparameters we work with when designing neural network architecture are:</p> <ul> <li>Learning rate \((\alpha)\)</li> <li>Momentum term \((\beta)\)</li> <li>Mini batch size</li> <li>Number of layers</li> <li>Number of hidden units</li> <li>Learning rate decay</li> <li>Adam optimizer parameters</li> </ul> <h2 id="order-of-importance-of-hyperparameters">Order of importance of hyperparameters</h2> <ul> <li>Learning rate is the most crucial parameter that we have to tune</li> <li>Secondary Importance: <ul> <li>Momentum Term</li> <li>Mini batch size</li> <li>Number of hidden units</li> </ul> </li> <li>Tertiary Importance: <ul> <li>Number of layers</li> <li>Learning Rate Decay</li> </ul> </li> <li>Seldom tuned: <ul> <li>Adam optimizer parameters</li> </ul> </li> </ul> <h2 id="hyperparameter-sampling-strategies">Hyperparameter sampling strategies</h2> <p>Earlier generations of ML algorithms used grids to systematically explore and choose the set of hyperparameter values that would work best.</p> <p>For deep learning this is not ideal. We instead choose the hyperparameter value combinations at random. The reason for this is that we cannot predict what hyperparameters are important for our use case.</p> <p>By sampling parameters at random we can explore a broad range of values for essential hyperparameters as opposed to a fixed set of values.</p> <p>Another common practice is to use coarse to fine sampling schemes. Here we find a region where the parameters work well and then zoom in and sample more densely within the given region.</p> <h2 id="hyperparameter-sampling-at-scale">Hyperparameter sampling at scale</h2> <p>When sampling hyperparameters at random, we do not randomly sample uniformly over all the possible values of the hyperparameters. The sampling is instead done at a scale.</p> <h3 id="uniform-sampling-examples">Uniform Sampling Examples</h3> <ul> <li>When selecting the number of <strong>hidden units</strong> we use a value range of \(50\) to \(100\) as a starting point.</li> <li>Similarly for the <strong>number of layers</strong>, we use a value between \(2\) to \(4\).</li> </ul> <h3 id="sampling-learning-rate-alpha">Sampling Learning Rate (\(\alpha\))</h3> <ul> <li>The sample range for learning rate is usually \([0.0001,1]\)</li> <li>Uniform sampling over this range would be inefficient as most of the values would lie between \(0.1\) to \(1\).</li> <li>In this case we make use of a <em>logarithmic scale</em>.</li> <li>So, we take a logarithmic range between, for example, \(10^a\) to \(10^b\), selecting our random value on this range as \(10^r\).</li> </ul> <h3 id="sampling-exponentially-weighted-average-hyperparameters">Sampling Exponentially Weighted Average hyperparameters</h3> <ul> <li>The hyperparameter \(\beta\) is used for calculating exponentially weighted averages as has been seen <a href="#exponentially-weighted-averages">before</a>.</li> <li>The range of values used is in \([0.9,0.999]\).</li> <li>This case, similar to learning rate, is inefficient when being linearly sampled. Hence, we make use of logarithmic scale again.</li> <li>Here we explore the values of \(1-\beta\), within a range of \(0.1\) to \(0.001\).</li> <li>We can take values of \(r\) between \(-3\) to \(-1\) and subsequently find \(beta = 1-10^r\).</li> </ul> <p>We use the following sampling process because</p> <ul> <li>When taking \(\beta\) values close to \(1\), minor variations in the value can have huge impacts on the results obtained. For example <ul> <li>a change from \(0.9\) to \(0.9005\) has a minimal change</li> <li>a change from \(0.999\) to \(0.9995\) drastically changes the results.</li> </ul> </li> <li>the formula used when implementing bias correction, \(\frac{1}{1 - \beta}\) is highly sensitive to minor changes when \(beta\) is close to \(1\).</li> <li>This sampling method allows for denser sampling as \(\beta\) approaches \(1\), hence allowing for efficient hyperparameter searching.</li> </ul> <h2 id="hyperparameter-tuning-in-practice">Hyperparameter Tuning in Practice</h2> <p>Hyperparameter tuning is particularly important in deep learning especially due to the sheer number of hyperparameters involved in the field. In addition, intuitions from a given field of deep learning will not necessarily transfer to other fields.</p> <p>Contrary to the above statement, there are some cases where we see some cross-fertilization between domains, such as ConvNets and ResNets in computer vision finding use in speech synthesis in NLP. However, such cases are a few and far in between.</p> <p>Intuitions on hyperparameters get stale over time due to several reasons such as</p> <ul> <li>Development of new algorithms for the same task</li> <li>Changes in data over time</li> <li>Infrastructure changes</li> </ul> <p>Hence, it is necessary to re-evaluate our hyperparameters from time to time.</p> <p>There are two main schools of thoughts that go into tuning hyperparameters</p> <ul> <li><strong>Babysitting a model</strong> - this is done when we have a huge dataset and not a lot of computational resources.In this case we babysit the model even as it is training. <ul> <li>Here we begin with random parameter initialization and monitor the learning curve, adjusting the hyperparameters as per the need.</li> <li>Changes are made incrementally over a period of time and the impacts on the model performance are actively observed.</li> <li>In the case of the changes performed leading to the detriment of the models’ performance, we revert back to a previous configuration</li> <li>This is an long and intensive process.</li> </ul> </li> <li><strong>Training many models in parallel</strong> - This method is useful when we have sufficient computational resources to work with. <ul> <li>Here, we run multiple models simultaneously with different parameter settings.</li> <li>Each model generates its own learning curve based on which we can make a decision on the best model to select</li> </ul> </li> </ul> <p>Making a decision between these approaches boils down to the computational resources we have at hand as well as the size of the datasets we are dealing with.</p> <h1 id="batch-normalization">Batch Normalization</h1> <p><strong>Batch Normalization</strong> is one of the most crucial ideas in the field of machine learning and deep learning. This process involves calculating the means and variances of the dataset and adjusting the data based on them. The end result is transforming our problem at hand to make it more suitable to be a gradient descent optimization problem.</p> <p>Batch normalization is meant to help make the parameter search process easier, hence making the neural network more robust. It also enables the training of very deep neural networks.</p> <h2 id="batch-normalization-mechanism">Batch Normalization Mechanism</h2> <p>As opposed to normalizing the activations, batch normalization focuses on normalizing the linear part \(z\) before the activation function. The process is as follows:</p> <ul> <li>Compute the mean as: \(\mu = \frac{1}{m} \sum\limits_{i}Z^{(i)}\).</li> <li>Compute the variance: \(\sigma^{2} = \frac{1}{m} \sum\limits_{i}(Z^{(i)} - \mu)^{2}\)</li> <li>Normalize the values: \(z^{(i)}_{norm} = \frac{Z^{(i)}-\mu}{\sqrt{ \sigma^{2} + \epsilon }}\)</li> <li>Scale and shift the normalized values: \(\tilde{Z}^{(i)} = \gamma Z^{(i)}_{norm} + \beta\)</li> </ul> <p>Here \(\gamma\) and \(\beta\) control the mean and variance of the hidden units. Setting \(\beta = \mu\) and \(\gamma = \sqrt{sigma^{2} + \epsilon}\), we get the identity function \(z^{(i)}_{norm}\).</p> <p>This process ensures that the input and hidden units have a standardized mean and variance.</p> <h2 id="batch-normalization-in-deep-neural-networks">Batch Normalization in Deep Neural Networks</h2> <p>Batch normalization is used to improve the generalization, accelerate training and allow for the use of higher learning rates in deep neural networks.</p> <p>Batch normalization is applied to each mini batch as opposed to the whole training set</p> <h2 id="implementation">Implementation</h2> <p>For each mini batch</p> <ul> <li>Compute the mean and variance of the activations (\(Z\)) alongside the normal forward propagation process.</li> <li>Normalize the activations \(Z^{[l]}\) using the statistical information calculated previously to get \(\tilde{Z}^{[l]}\).</li> <li>Scale and shift the normalized activations using the two parameters \(\beta\) and \(\gamma\)</li> <li>Use backpropagation to compute \(dW^{[l]},d\beta^{[l]},d\gamma^{[l]}\)</li> <li>Update the parameters</li> </ul> <p>Batch normalization replaces the role of the biases \(B\) with \(\beta\) and also introduces a scaling factor \(\gamma\). Hence the parameters we are working with are \(W,\beta,\gamma\).</p> <p>This process can be used for other optimization methods like RMSProp, and momentum.</p> <h2 id="advantages">Advantages</h2> <ul> <li>Batch normalization helps deal with covariate shift<a class="citation" href="#Seldon2021">(Seldon, 2021)</a>.</li> <li>It ensures that all the parameters have similar value ranges while maintaining the variances. This makes the training process more efficient.</li> <li>It centers the activations around zero mean due to which the bias term becomes redundant. Due to this they can be set to zero or removed.</li> </ul> <p>The \(\beta\) in batch normalization is not the same as the \(\beta\) in other algorithms such as momentum or Adam.</p> <h2 id="why-batch-normalization-works">Why Batch Normalization Works</h2> <p>We have already seen how <a href="#normalizing-the-input-features">normalization</a> helps improve the speed performance and stability of neural networks.</p> <p>Some important points about batch normalization are</p> <ul> <li><strong>Feature Scaling</strong>: Scaling down our features to have a mean of zero and a variance of one, as is known beforehand, can accelerate the learning process. In batch normalization, this concept is extended to the hidden layers.</li> <li><strong>Robustness to weight changes</strong>: Changes in the earlier layers of a neural network can have a significant effect on the values of the weights on the deeper layers of the network. Batch normalization helps make the network more resilient to this.</li> <li><strong>Covariate Shift</strong>: When the distribution of the data changes, it will affect the model performance, hence requiring the model to be retrained. This is known as covariate shift. In neural networks, as the early layer weights undergo changes, the values in the subsequent layers get affected, hence leading to an internal covariate shift. Batch normalization helps reduce this effect.</li> <li>Batch normalization makes it such that even if values from previous layers change, their mean and variance remain consistent, hence making the network stable. As a result each layer can learn independently, accelerating the learning process across the whole network.</li> <li>Batch norm also has a slight regularization effect due to the introduction of noise when computing the mean and variances based on mini batches. This prevents the model from relying too much on any particular hidden unit. This effect is not the main effect we are trying to find here which is why it is not given as much importance. Normalization along with dropout is preferred to this.</li> </ul> <h2 id="batch-normalization-during-testing">Batch Normalization During Testing</h2> <p>Batch normalization handles the data in mini batches, computing the means and variances on each mini batch.</p> <p>Due to this, when making predictions and evaluating the neural network, we have one example at a time instead of mini batches. Hence, we need to do something to ensure that our predictions are accurate.</p> <p>The training process is as follows:</p> <ul> <li>Calculate \(\sigma^2\) and \(\mu\)</li> </ul> \[\mu = \frac{1}{m}\sum\limits_{i}z^{(i)}\] \[\sigma^{2} = \frac{1}{m}\sum\limits_{i}(z^{(i)} - \mu)^{2}\] <ul> <li>Normalize the activations using \(\sigma^2\) and \(\mu\)</li> </ul> \[z^{(i)}_{norm} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^{2}+\epsilon}}\] <ul> <li>Scale and shift the normalized activations using the parameters \(\gamma\) and \(\beta\)</li> </ul> \[\tilde{z}^{(i)} = \gamma z^{(i)}_{norm} +\beta\] <p>The issue here is that we are calculating \(\sigma^2\) and \(\mu\) on a mini batch, but we might not have a mini batch to work with when testing.</p> <p>To solve this:</p> <ul> <li>During training we first <strong>maintain an exponentially weighted average</strong> for \(\mu\) and \(\sigma^2\).</li> <li>Then, when it comes to testing, we normalize the activations with these averaged values and apply the \(\gamma\) and \(\beta\) learned during the training to shift and scale the data as necessary.</li> </ul> <p>The advantage here is that the estimations for \(\mu\) and \(\sigma^2\) are robust and there are many other ways to approximate these values besides an exponentially weighted average.</p> <h1 id="multi-class-classification---softmax-regression">Multi-class Classification - Softmax Regression</h1> <p>The focus till now has been on binary classification through the use of logistic regression where we only have two possible outcomes to a prediction.</p> <p>Real world situations, however, will require for us to classify much more than two objects simultaneously. This is where we use <strong>Softmax Regression</strong>.</p> <p>Softmax is a generalization of Logistic Regression which handles classifying an unknown object to one of \(C\) classes. Here we use \(C\) to denote the number of classes we are trying to categorize the inputs to.</p> <h2 id="softmax-in-neural-networks">Softmax in Neural Networks</h2> <p>In a neural network, the output layer will have \(C\) output units. So, \(n^{[L]} = C\). Each of the output nodes will give us the probability of the given input belonging to each of the \(C\) classes.</p> <p>The dimensions of the output is a \((C,1)\) vector with each of the nodes being \(P(C_{1}\mid X),P(C_{2}\mid X), \dots\), all of them summing up to \(1\).</p> <p>To achieve this, the <strong>softmax</strong> layer comes into the picture.</p> <p>When computing the softmax activation on the last layer, we first compute \(Z^{[L]}\) which is the product of the weights and activations of the previous layer with the biases added to them.</p> <p>We then calculate</p> \[t = e^{Z^{[L]}}\] <p>This is also a \((C,1)\) dimension vector. This is then fed into the activation, giving us the output \(a^{[L]}\)</p> \[\boxed{a^{[L]} = \frac{e^{Z^{[L]}}}{\sum_{i=1}^{C}t_{i}}}\] <p>For the \(i^{th}\) element of the output this would be</p> \[\boxed{a^{[L]}_{i} = \frac{t_{i}}{\sum_{i=1}^{C}t_{i}}}\] <p>From this we see that the softmax activation function is used to transform a vector of numbers to a probability distribution for each class the outputs represent.</p> <h2 id="how-softmax-works">How Softmax Works</h2> <p>Suppose we have a vector</p> \[Z^{[L]} = \begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix}\] <p>we would get \(t\) as</p> \[t = \begin{bmatrix} e^{5} \\ e^{2} \\ e^{-1} \\ e^{3} \end{bmatrix} = \begin{bmatrix} 148.4 \\ 7.4 \\ 0.4 \\ 20.1 \end{bmatrix}\] <p>which is the exponent of the elements of the vector.</p> <p>Now, if we wanted to get \(a^{[L]}\), we normalize these values so that they sum up to 1, hence giving us what is effectively a probability distribution. Hence we calculate this as</p> \[\sum\limits_{j=1}^{4}t_{j} = 176.3\] <p>We then obtain the activation for this layer \(a^{[L]}\) as</p> \[a^{[L]} = \frac{t}{176.3}\] <h2 id="training-the-softmax-classifier">Training the Softmax Classifier</h2> <p>Taking the same example as before, where we have</p> \[Z^{[L]} = \begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix}\] <p>we calculate the softmax activation as</p> \[g^{[L]}(Z^{[L]}) = \begin{bmatrix} \frac{e^{5}}{e^{5} + e^{2} + e^{-1} + e^{3}} \\ \frac{e^{2}}{e^{5} + e^{2} + e^{-1} + e^{3}} \\ \frac{e^{-1}}{e^{5} + e^{2} + e^{-1} + e^{3}} \\ \frac{e^{3}}{e^{5} + e^{2} + e^{-1} + e^{3}} \end{bmatrix} = \begin{bmatrix} 0.842 \\ 0.042 \\ 0.002 \\ 0.114 \end{bmatrix}\] <p>Here we see that the biggest vector is,5, due to which it has the highest probability value of the bunch.</p> <p>Softmax is named as such because of its contrast to the <strong>Hardmax</strong> function which just shows the highest value to have a probability of \(1\) with the rest of the values being set to \(0\). Softmax, instead, is a gentle mapping of the \(Z\) values to different probabilities, hence giving us a probability distribution as the output.</p> <h2 id="softmax-cost-function">Softmax Cost Function</h2> <p>The loss function of the softmax function given the ground truth as \(y\) and the model prediction as \(\hat{y}\), is</p> \[L(\hat{y},y) = -\sum\limits_{j=1}^{C}y_{j}\log{\hat{y}_{j}}\] <p>where \(C\) is the number of possible classes that the output can have.</p> <p>The loss function here is known as the <strong>cross entropy loss function</strong>, which looking at the ground truth, tries to make the corresponding probability of the class as high as possible.</p> <p>Hence, the cost function would be</p> \[\boxed{J(W^{[I]},B^{[1]},\dots) = \frac{1}{m}\sum\limits_{i=1}^{m} L(\hat{y}^{(i)},y^{(i)})}\] <h2 id="conclusion">Conclusion</h2> <p>The methods discussed in this blog are some of the most popular methods used to improve the training speed of our neural networks along with in several ways.</p> <p>We have looked into</p> <ul> <li>Appropriate ways to split up our dataset when dealing with deep learning problems and how it differs from classical machine learning.</li> <li>Ways to prevent the model from overfitting the data through regularization methods such as L2, L1 and Dropout.</li> <li>Normalizing our datasets to help our deep learning models to learn faster.</li> <li>How appropriate weight initialization methods can help with vanishing and exploding gradients.</li> <li>Verifying that our calculated gradients are correct through gradient checking.</li> <li>Optimization algorithms other than simple gradient descent that use exponentially weighted averages to train our neural networks faster.</li> <li>Hyperparameter sampling strategies to help choose our deep learning hyperparameters effectively.</li> <li>Batch Normalization for deep neural networks and how they help prevent instability in the weights and biases when training deeper neural networks.</li> <li>Multi-class classification with the help of the <strong>softmax</strong> activation function.</li> </ul> <p>Now that we have looked into some of the ways we can improve our deep learning models, We can now shift focus to deep learning model architectures that are more tailored towards specific applications which would be beyond the capabilities of simple deep artificial neural networks.</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="machine-learning"/><category term="deep-learning"/><summary type="html"><![CDATA[Topics covered: Deep Learning in practice, Optimization Algorithms, Hyperparameter Tuning, and Batch Normalization.]]></summary></entry><entry><title type="html">Deep Learning - An Introduction</title><link href="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-1/" rel="alternate" type="text/html" title="Deep Learning - An Introduction"/><published>2024-09-17T16:40:16+00:00</published><updated>2024-09-17T16:40:16+00:00</updated><id>https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-1</id><content type="html" xml:base="https://ashishmathew0297.github.io/blog/2024/deep-learning-self-study-1/"><![CDATA[<p>This is the first of a series of posts that compile my reports for a Deep Learning independent study taken up at the University of Massachusetts Dartmouth.</p> <p>This study is loosely based on the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a> offered by <a href="https://www.deeplearning.ai/">deeplearning.ai</a> with more content included from outside sources where necessary. A lot of the visualizations here are hand-drawn or taken directly from Coursera.</p> <p>The focus here is to start from the foundations to help understand the capabilities, challenges and consequences of deep learning, gradually moving on to more complex topics and applications of these concepts. The main goal is to gain insights on the inner workings of the concepts behind cutting-edge AI models used in various fields as well as hands-on experience using these models. Equal amounts of importance will be given to the underlying logic and mathematics behind various deep learning models and their practical applications.</p> <p>This post covers the basics of deep learning and neural networks with a focus on the underlying computation.</p> <hr/> <h1 id="introduction">Introduction</h1> <p>In the world of data science <a class="citation" href="#coursera2024">(Coursera, 2024)</a>, we usually see mentions of statistics, machine learning (ML), artificial intelligence (AI), and deep learning several times. Though machine learning and deep learning are both subfields of AI and both terms are used interchangeably, they have their own meanings.</p> <p><strong>Machine learning</strong> algorithms are relatively simple models designed to adapt on input data and either find patterns or make decisions on newly provided data.</p> <p><strong>Deep learning</strong> on the other hand, is a branch of machine learning that involves training expansive <em>artificial neural networks (ANNs)</em>. An ANN consists of layers of interconnected nodes called <em>neurons</em> that, together, help process and learn patterns from the input.</p> <p>We often picture AI, deep learning and machine learning as a Venn diagram with AI taking up the outermost layer, machine learning being the intermediate layer, and the innermost layer being deep learning. A more detailed visualization of this can be seen below.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI_domains.jpg" sizes="95vw"/> <img src="/assets/img/AI_domains.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Artificial Intelligence and its Domains </div> <p>Deep learning has not only transformed how we search for content on the internet, but has also found its use in domains such as:</p> <ul> <li><strong>Image Recognition</strong>: Image recognition is one of the most common uses of deep learning, where a model is capable of identifying objects or features in image and video data. This forms the groundwork for several fields such as facial recognition, image classification, automated driving, robotics and several other fields. This domain also has a great amount of overlap in the field of healthcare in terms of medical imaging.</li> <li><strong>Object Detection</strong>: Object detection is a technique similar to image recognition. Both these techniques are often paired together when being used practically. Object detection is used on a higher level to detect instances and the positions of objects in an image or video, while image recognition is capable of assigning an identity to objects from these sources. E.g. Object detection would detect whether an image contains cats in it along with the number of cats and their positions. Image recognition would be capable of telling us what color or breed of cat is present in the given image.</li> <li><strong>Natural Language Processing</strong>: This domain is related to creating a way for computers to understand natural human languages in everyday speech. This could be in several forms such as speech, writing or even sign language. Some of the common techniques that show up in this field are sentiment analysis, keyword extraction, translation, text summarization, text generation, etc.</li> <li><strong>Generative AI</strong>: This domain involves generating various types of context such as text, images and audio, and is one of the most popular applications of deep learning as of the writing of this blog. GenAI has found various uses in boosting human productivity, such as in content creation, data analysis and research. Large Language Models (LLMs) are the most popular applications of GenAI, with active research and development being done in the domain. Some popular LLMs are Llama by Meta, ChatGPT and GPT-4 by OpenAI, and Gemini by Google.</li> </ul> <h1 id="neural-networks">Neural Networks</h1> <p>The basic idea of deep learning is the training of <strong>Neural Networks</strong>. The best way to understand what a neural network is, is to look at a real-life example.</p> <h5 id="scenario-1">Scenario 1</h5> <p>Consider a housing price prediction model where the data at hand consists of the square-foot size of the houses and their prices. We plot these points out taking the square-foot size on the X axis and price on the Y axis.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/housing_example_1.jpg" sizes="95vw"/> <img src="/assets/img/housing_example_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> All the instances of houses plotted by Price vs Square-foot size. </div> <p>Using basic machine learning, the next logical step would be to create a regression or any other suitable model to fit a line or curve that best describes the general trend of the price of the houses with relation to their size. In this case, let us take linear regression as the initial model of choice.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/housing_example_2.jpg" sizes="95vw"/> <img src="/assets/img/housing_example_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The linear regression line plotted to fit the points. Note the line taking up negative price values. </div> <p>Now, we know that the price of a house can never be negative. That would imply that the buyer is being paid a sum of money along with receiving a house, which does not make sense realistically. Pure linear regression takes into account both negative and positive values, which is why we can rule it out. Instead, we truncate our fitting line at \(y=0\), giving us the following that predicts the price as a function of size.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/housing_example_3.jpg" sizes="95vw"/> <img src="/assets/img/housing_example_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The modified linear regression line with truncation (a.k.a ReLU). </div> <p>This resulting function is known as a <strong>Rectified Linear Unit</strong> or <strong>ReLU</strong> for short, and is widely used in the development of deep learning models.</p> <h5 id="scenario-2">Scenario 2</h5> <p>Now, suppose we have more features to work with in addition to the size of the house, such as the number of <em>bedrooms</em>, <em>zip code</em> and <em>wealth</em> of the neighborhood. In this scenario we can make the following observations:</p> <ul> <li>The <em>size</em> and <em>number of bedrooms</em> gives us an idea of whether a house can accommodate for the <em>family size</em>.</li> <li>The <em>zip code</em> tells us how <em>walkable</em> the locality of the house is.</li> <li>The <em>zip code</em> along with the <em>wealth</em> of the neighborhood can give us an idea of the <em>quality of schools</em> in the area.</li> </ul> <p>All of this information can be visualized in the form of a neural network as shown.</p> <div class="row justify-content-center mt-3"> <div class="col-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/housing_example_nn_1.jpg" sizes="95vw"/> <img src="/assets/img/housing_example_nn_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The circles in the image here represent <strong>nodes</strong> which can contain a ReLU or any other function. The arrows here represent the flow of information between the input and nodes.</p> <p>All the observations made above can be seen as functions that take in the information to give the corresponding outputs for family size, walkability and school quality. In a similar fashion, all of these can together be taken in as inputs for yet another node which gives us an estimate for the price of the given house.</p> <p>This example gives us a look into how people can go about deciding how much they would be willing to spend on a house.</p> <h2 id="a-real-world-neural-network">A real-world Neural Network</h2> <p>In a real-world implementation of a neural network, the inputs are taken as a vector \(X\) with the target (price) taken as \(y\) which we aim to predict.</p> <p>We give the neural network the \(X\) values along with the target \(y\) as the training set. As opposed to the previous scenarios, the function of the intermediate layers are figured out by the neural network by itself.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/actual_nn.jpg" sizes="95vw"/> <img src="/assets/img/actual_nn.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A real-world implementation of a single layer neural network. </div> <p>A practical neural network would look like the image above. The circles here are called <strong>hidden units</strong> which are arranged in layers known as <strong>hidden layers</strong>.</p> <p>Each node here takes inputs from all of the input features. We do not have control over what each node does. The neural network itself makes the decision on what each node is supposed to do.</p> <p>In addition, a practical neural network will seldom have just one neuron or a single layer. Instead a collection of several neurons receiving differing inputs would be used, which collaborate to give an accurate and complex prediction.</p> <p>In this case, the layer is known as a <strong>densely connected layer</strong> as every input feature of a given layer is connected to every other node in its previous and next layer.</p> <p>The main advantage of neural networks is that with a large enough number of input features along with the target, and enough training examples for them, neural networks are effective at figuring out approximate functions to map the inputs to the outputs.</p> <h2 id="supervised-learning-with-neural-networks">Supervised Learning with Neural Networks</h2> <p>Supervised learning makes use of labelled datasets where we have both the features \(X\) and the target \(y\) available to us. The goal here is to learn a function that would best form a relation ship between them.</p> <p>Neural networks are widely used for supervised learning tasks due to their adaptability and capability of creating complex representations.</p> <p>A popular implementation of neural networks is in Natural Language Processing, where they are used in tasks such as sentiment analysis, machine translation and speech synthesis.</p> <p>Computer Vision is another implementation of neural networks where machines are trained to interpret images and make decisions based on the same. Some popular computer vision tasks are object detection, image recognition and facial recognition.</p> <p>Neural networks also find use in the domain of healthcare <a class="citation" href="#DLNIH">(Miotto, 2018)</a> where they are employed in fields such as biomedical informatics, genomics and public health to gain insights from high dimensional and complex data. Several challenges and opportunities are faced in this field, especially when developing data-driven tools for knowledge discovery and predictive analysis. Neural networks find their use here due to them being capable of creating models to more accurately represent the complex relationships in the data and create meaningful abstractions from it.</p> <h3 id="sequential-data">Sequential Data</h3> <p>A lot of modern implementations of deep learning involves working with <strong>sequential data</strong>. Sequential data refers to a series of data points whose order of occurrence is of importance. Examples of these are time series data, audio signals, stock price data, natural language sentences, gene sequences and weather data. In all these cases, the points in the dataset are dependent on their position in relation to other points present in the dataset.</p> <p>Traditional neural networks are not capable of handling sequential data well as they are incapable of taking into consideration of the sequence in which the data is fed into them. Instead specialized neural networks have been developed to handle these cases, some of which are:</p> <ul> <li><strong>Recurrent Neural Networks (RNNs)</strong>: RNNs are designed to work with sequential data. This is attributed to their “memory” mechanism where they can take in information from prior inputs to influence the outcome of the current input and output. As opposed to traditional neural networks which assume the inputs to be independent of each other, RNN outputs depend on the prior elements in the sequence. This makes them useful in fields such as speech recognition and language modelling.</li> <li><strong>Long Short-Term Memory (LSTM) Networks</strong>: LSTMs <a class="citation" href="#LSTM">(Hochreiter &amp; Schmidhuber, 1997)</a> are a variant of RNNs developed as a solution to several problems faced by RNNs such as the vanishing gradient problem. These models can memorize patterns over long sequences, making them useful for time series predictions with longer patterns such as machine translation, speech recognition and video analysis.</li> <li><strong>Gated Recurrent Units (GRUs)</strong>: GRUs, similar to LSTMs also address the problem of short-term memory in RNNs and simplify the overall architecture of LSTMs.</li> <li><strong>Transformers and Attention Mechanisms</strong>: These architectures while not being recurrent, handle sequences in the input data by focusing on input elements that are considered important. They prioritize focusing on relevant information, leading to an enhancement of the overall performance of the model.</li> </ul> <h3 id="non-sequential-data">Non-Sequential Data</h3> <p>Non-sequential data is more straightforward to deal with as the inputs are independent of one another. Such information can be modelled with the use of <strong>FeedForward Neural Networks (FNNs)</strong> or <strong>Multi-layer Perceptrons (MLPs)</strong>. These networks are quintessential to deep learning and aim to approximate a function mapping its inputs to the outputs. They consist of an input layer, several hidden layers and an output layer. In these networks the neurons in a given layer are fully connected with the neurons in the subsequent layer, hence enabling the learning and transformation of features. Such networks are used for tasks such as regression and classification and more complex tasks in combination with other techniques.</p> <p>In the subsequent sections we will dive deeper into how these networks work.</p> <h2 id="neural-networks-the-basics">Neural Networks: The Basics</h2> <p><strong>Deep feedforward networks</strong> <a class="citation" href="#Goodfellow-et-al-2016">(Goodfellow et al., 2016)</a> are the most basic of deep learning models with the main goal of approximating a function. Feedforward networks get their name from the flow of information through the function being evaluated from the inputs \(X\), through intermediate computations towards the output \(y\). These intermediate computations eventually define the function we are approximating.</p> <div class="row justify-content-center mt-3"> <div class="col-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DNN1.jpg" sizes="95vw"/> <img src="/assets/img/DNN1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A typical feedforward network. </div> <p>Neural networks, as can be seen form the image, compose different functions together. A good example given by Goodfellow’s book (previously cited) takes three functions \(f^{(1)}, f^{(2)}, f^{(3)}\) which are connected in a chain to form a composite function of the form \(f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))\), where the numbers in the superscript correspond to different <strong>layers</strong> of nodes. This is what gives us the term <strong>networks</strong> to describe these models.</p> <p>The very first layer of the feedforward network consists of the inputs, aptly giving it the name: the <strong>input layer</strong> with the final layer of the network being called the <strong>output layer</strong>. The layers that lie between the input and output layers are called <strong>hidden layers</strong>.</p> <p>Training a neural network has the aim of finding an approximate function, say \(f(x)\) that can match an unknown function, say \(f^{*}(x)\). The training process of a neural network aims to drive the function \(f(x)\) to match \(f^{*}(x)\) as close as possible. Each training example gives us noisy approximate examples of \(f^{*}(x)\), hence specifying directly what a layer must do at the given point. The training algorithm must decide how the layers are used together towards producing \(f\) such that it is as close as possible to \(f^{*}\).</p> <p>The most common analogy for naming these networks as <strong>neural networks</strong> is that they take inspiration from neuroscience, owing to their resemblance to human neurons. A human neuron receives electrical signals from other neurons, does a simple computation and if fired, it sends a pulse of electricity down the axon to other neurons. Similarly, each layer of a neural network is vector valued, with each vector element representing a neuron and the number of elements representing the <strong>width</strong> of the model.</p> <p>However, it is still up to question how neurons actually work as they are much more complex than the given explanation. We are not sure if a neuron uses an algorithm or any other different learning principle. Hence, deep learning can learn complex function mappings fairly easily, but does not directly compare to a neuron.</p> <p>The easiest way to understand how neural networks work is through examples, one of which we will have a look at now.</p> <h2 id="notation">Notation</h2> <p>Before we go on to building our network, here are some notations to keep in mind that we will be using.</p> <ul> <li>A single training example is \((x,y)\) where \(x \in \mathbb{R}^{n_{x}}\) and \(y \in \{0,1\}\)</li> <li>Training set is denoted by \(m\) or \(m_{train}\) : \(\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots, (x^{(m)},y^{(m)})\}\)</li> <li>Testing set is denoted by \(m_{test}\)</li> <li>The training set as a column matrix is denoted as \(X = \begin{bmatrix} \vdots &amp; \vdots &amp; &amp; \vdots\\ x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)}\\ \vdots &amp; \vdots &amp; &amp; \vdots\\ \end{bmatrix}\), with the number of columns being \(m\) and the number of training examples/rows/height is \(n_{x}\). \(X \in \mathbb{R}^{n_{x} \times m}\)</li> <li>The output labels are also in the form of a row vector corresponding to each column of \(X\) such that \(Y \in \mathbb{R}^{1 \times m}\).</li> <li>Different layers of the network are denoted with superscript square brackets. For instance, \(z^{[1]}\) and \(a^{[1]}\) correspond to computations on the first layer, \(z^{[2]}\) and \(a^{[2]}\) correspond to computations on the second layer, and so on.</li> </ul> <h2 id="logistic-regression">Logistic Regression</h2> <p>In the modern day, we use a lot of different types of artificial neurons, the most common of which is the <strong>sigmoid neuron</strong>, which is used for binary classification via <strong>logistic regression</strong>.</p> <p>The formula of the sigmoid function is given by the formula</p> \[f = \frac{1}{1 + e^{-\sum\limits\beta_nx_n}}\] <p>where \(\beta_n\) is a <strong>weight</strong> and \(x_n\) is a <strong>feature</strong>.</p> <p>Consider the example of an image that we have to classify as either a cat (1) or not a cat (0). The image is in the form of 3 matrices of red, green and blue channels. So, for a \(64 \times 64\) pixel image, we have 3 \(64 \times 64\) matrices corresponding to rgb pixel intensities for the image.</p> <p>To convert these into a feature vector, we unroll the matrices into an input feature \(x\) as</p> \[\begin{bmatrix} 255\\ 231\\ \vdots\\ 255\\ 134\\ \end{bmatrix}\] <p>which is a combination of all the rgb pixel intensity values into a single vector.</p> <p>For a \(64 \times 64\) image, the dimension of this vector will be \(64 \times 64 \times 3\) as it is the total number of elements we have which in this case is 12288. We use \(n_{x}= 12288\) to represent <strong>dimension of input features</strong> \(x\) and sometimes for brevity we just use \(n\).</p> <p>The goal here is to use an input image represented by the feature vector \(x\) to predict whether the corresponding label \(y\) is 1 or 0.</p> <p>Now, given the input images \(X\) we want to predict \(\hat{y} = P(y=1 \mid x)\).</p> <p>Here the <strong>parameters</strong> we work with are:</p> <ul> <li>The <strong>weights</strong>, \(w \in \mathbb{R}^{n_{x}}\), which are vectors and</li> <li>The <strong>biases</strong>, \(b \in \mathbb{R}\), which are scalars.</li> </ul> <p>This, is the basis of a <strong>linear model</strong> given by</p> \[\hat{y} = w^{T}x + b\] <p>The dot product of \(w^T\) and \(x\) helps us evaluate multiple classifiers parallelly where each \(x\) would have a different value of \(w\).</p> <p>The drawback of a linear classifier is the fact that it has a range of values in \((-\infty, +\infty)\), giving us output values that have a large amount of variations as opposed to a simple yes or no equivalent.</p> <p>This is where logistic regression comes in. The logistic function</p> \[\hat{y} = \sigma(\underbrace{w^{T}x + b}_{z}) = \frac{1}{1 + e^{-z}}\] <p>clamps the value of the linear function within the range \([0,1]\) such that</p> <ul> <li>for very large values of \(z\), \(e^{-z} \rightarrow 0\), hence giving us \(\sigma(z) \approx 1\)</li> <li>for very large negative values of \(z\), \(e^{-z} \rightarrow \infty\), hence giving us \(\sigma(z) \approx 0\)</li> </ul> <p>Our goal here is to learn the parameters \(w\) and \(b\) such that the given function \(\hat{y}\) is a good estimate of \(P(y=1 \mid x)\). This is done with the help of the <strong>cost function</strong>.</p> <h2 id="cost-functions">Cost Functions</h2> <p>We already know loss functions to be the measure the performance of an individual training example. The <strong>cost function</strong> is the average of the loss functions over the entire training set. Cost functions measure how well the parameters are doing over the entire set. The main goal is to minimize the cost function with the help of the appropriate weights and biases.</p> <p>When designing a neural network, the choice of cost function we use holds a great deal of importance. Cost functions in the case of neural networks are the same as those of other models parametric models.</p> <p>The most common parametric models we use define a distribution, which work on the idea of maximum likelihood, hence making use of cross-entropy between the training data and predictions as the cost function (such as is in the case of logistic regression or softmax).</p> <p>In practice, when training a neural network, the total cost function would be a combination of the primary cost function with a regularization term.</p> <h3 id="logistic-regression-cost-function">Logistic Regression Cost Function</h3> <p>Consider the data points \(\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots, (x^{(m)},y^{(m)})\}\), where we want to find \(\hat{y}^{(i)} \approx y^{((i))}\). the function being taken here is the sigmoid function given by \(\hat{y} = \sigma(w^{T}x + b)\) or</p> \[\sigma(z) = \frac{1}{1 + e^{-z}}\] <p>by finding the parameters \(w\) &amp; \(b\).</p> <p>We judge the performance of our model by using an <strong>error/loss function</strong> denoted by \(L(\hat{y}, y)\). The loss function here helps us understand how well our current approximating function’s output \(\hat{y}\) is doing in comparison to the ground truth \(y\).</p> <p>In most cases, the <strong>square error</strong> or <strong>half of the square error</strong> would be taken into consideration for a linear problem. In the case of Logistic Regression, the optimization problem would be non-convex with multiple local minima if we made use of this loss function.</p> <p>Hence, instead we make used of the loss function for Logistic regression, given by</p> \[L(\hat{y}, y) = -(y\log{\hat{y}} + (1-y)\log{ (1-\hat{y})})\] <p>For squared error, we normally want the value to be as small as possible. Similarly for logistic regression, we would want this loss function to be as small as possible.</p> <p>The mechanism of this loss function is rather interesting to look at. It is a version of <strong>Cross-Entropy Loss</strong> which is used to calculate the loss for the softmax function, which we will look into later on. Its working is as follows</p> <ul> <li>Take \(y=1\), then we get \(L(\hat{y}, y) = -\log{\hat{y}}\), which should be as small as possible. <ul> <li>This means that \(\log{\hat{y}}\) should be as large as possible.</li> <li>And hence \(\hat{y}\) should be large but never bigger than 1.</li> </ul> </li> <li>On the flip side, if \(y=0\), then we would get \(L(\hat{y}, y) = -\log{(1-\hat{y})}\), which should be as small as possible. <ul> <li>\(\log{(1-\hat{y})}\) should be as large as possible.</li> <li>hence \(\hat{y}\) should be as small as possible but never lesser than 0.</li> </ul> </li> </ul> <p>If we are working with the complete training set, instead of a loss function, we would make use of the <strong>cost function</strong> given by</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{n} L(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum\limits_{i=1}^{n} y^{(i)}\log{\hat{y}^{(i)}} + (1-y^{(i)})\log{(1-\hat{y}^{(i)})}\] <p>where we want to find \(w\) and \(b\) such that it minimizes the overall cost function \(J(w,b)\). We will be looking into the mechanism for this now.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>As opposed to most models we usually see in machine learning, neural networks are non-linear, leading to the loss functions being non-convex. Hence, neural networks need to be trained iteratively using gradient based optimizers to drive the cost function low as possible.</p> <p>Gradient descent is one of the most common algorithms used in machine learning, and in this case, it also finds its use in learning the parameters \(w\) and \(b\) on our training set.</p> <p>When training neural networks using gradient descent, it is important that all the weights are initialized to small random values with the biases being either zero or small positive values. We will look into the reason for this later in this article.</p> <p>Gradient descent can be visualized as shown</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Gradient%20Descent.png" sizes="95vw"/> <img src="/assets/img/Gradient%20Descent.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The cost function here is seen as a surface above the horizontal axes of \(w\) and \(b\) with the height giving us the value of \(J(w,b)\) at a certain point.</p> <p>We aim to find the value of \(w\) and \(b\) such that the cost of our resultant model corresponds with the minimum of this curve. In this case, the cost function we see is a convex function due to which it can be seen as a big bowl with a single optimum point.</p> <p>To find a good value for these parameters, we first initialize the \(w\) and \(b\) values to an initial value. The gradient descent algorithm takes a step in the steepest downhill direction, trying to find its way down to the minima as quickly as possible. This step is repeated till our function reaches the global optimum or a value close to it.</p> <p>Visualizing this, consider a two-dimensional representation for the cost function, ignoring the bias \(b\) for the sake of simplicity while we are at it.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD1.jpg" sizes="95vw"/> <img src="/assets/img/GD1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The cost function in 2D </div> <p>The following steps are taken to reach the minima:</p> <ul> <li>\(w := w - \alpha \frac{dJ(w)}{dw}\) : updating the value of \(w\). This is known as the <strong>inner loop</strong></li> <li>Repeat the above till it converges to a global minimum.</li> </ul> <p>Here, \(\alpha\) is the <strong>learning rate</strong>, which controls how large or small the gradient descent steps taken are.</p> <p>The value \(\frac{dJ(w)}{dw}\) is a derivative that depicts the update of the change we want to see in the parameter \(w\), and which direction it goes in. In code we write this as <code class="language-plaintext highlighter-rouge">dw</code>.</p> <p>Now, suppose we have an initial point for the cost function \(J(w)\).</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD2.jpg" sizes="95vw"/> <img src="/assets/img/GD2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Cost function with the initial cost. </div> <p>Here, the derivative is the slope of the cost function at a point. It is the height divided by the width of the lower triangle, or the tangent to the curve at the given point.</p> <p>In this case, we see that the derivative is positive, leading to \(w\) getting updated as \(w - \alpha \frac{dJ(w)}{dw}\), hence taking the cost a step to the left.</p> <p>Continuing this, our algorithm can be made to slowly decrease the parameter till we reach the minima of \(J\).</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD3.jpg" sizes="95vw"/> <img src="/assets/img/GD3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Gradient descent updating the cost such that it approaches the minimum. </div> <p>For the case of the weight \(w\) leading the cost to be on the left hand side of the minimum, we would see a similar trend with the only difference being that \(w\) is increased as \(w + \alpha \frac{dJ(w)}{dw}\) (since \(\frac{dJ(w)}{dw} &lt; 0\)) till we reach the minimum value of \(J\).</p> <p>In the case of logistic regression, we have the cost function inner loop as</p> \[w = w - \alpha \frac{dJ(w,b)}{dw}\] \[b = b - \alpha \frac{dJ(w,b)}{db}\] <p>Here, the above derivatives are partial derivatives, which in code are represented as <code class="language-plaintext highlighter-rouge">dw</code> and <code class="language-plaintext highlighter-rouge">db</code>.</p> <h2 id="computation-graphs">Computation Graphs</h2> <p>Computation graphs are a way for us to visualize mathematical expressions in a descriptive and visual manner. These are extremely useful in understanding how deep learning models work</p> <p>The computations in a neural network are of two forms:</p> <ul> <li><strong>Forward pass/propagation</strong> where the <em>output</em> of the neural network is computed.</li> <li><strong>Backward pass/propagation</strong>, which is used to calculate the <em>gradients/derivatives</em>.</li> </ul> <p>Here, they find use in describing both the forward and the backward propagation algorithm in a precise manner.</p> <p>In a computation graph, each node represents a variable. Variables can be formed from an operation or set of operations (a.k.a a function) consisting of one or more variables. An operation can only return a single output variable.</p> <h2 id="forward-propagation">Forward Propagation</h2> <p>The process of computing the output of a neural network is done through <strong>forward propagation</strong>. This process is similar to logistic regression but repeated for each node in the hidden layer.</p> <p>Neural networks, as discussed before, can be seen as multiple functions stacked together as multiple layers of computations. Each node in the network performs two actions:</p> <ul> <li>Calculating the linear value: \(z\)</li> <li>Computing a non-linear activation function \(a\) using the \(z\) value.</li> </ul> <h3 id="forward-propagation-explained-with-computation-graphs">Forward Propagation: Explained with Computation Graphs</h3> <p>Taking a simple example of computing a function</p> \[J(a,b,c) = 3(a + bc)\] <p>The computation of this function can be split into 3 different parts:</p> <ul> <li>computing \(u = bc\)</li> <li>computing \(v = a + u\)</li> <li>computing \(J = 3v\)</li> </ul> <p>In a computation graph, these steps are visualized as follows</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CG1.jpg" sizes="95vw"/> <img src="/assets/img/CG1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As we can see, computation graphs are useful when there is some output variable (\(J\) in this case) to optimize. For logistic regression, \(J\) is the cost function that we are trying to minimize.</p> <p>In this case, for a left to right pass, we compute the value of \(J\) and, in order to compute the derivatives, we see a right to left pass in the opposite direction of the above arrows.</p> <p>Summing it up, computation graphs organize our left-to-right and right-to-left computations with arrows to give us an understanding of the mathematical model of our network.</p> <h2 id="backward-propagation">Backward Propagation</h2> <p><strong>Backward propagation</strong> also known as <strong>backprop</strong> is an algorithm used for supervised learning mainly used in the training of artificial neural networks. It is a gradient descent method used to minimize the error in the predictions of our network, which it does so by adjusting the weights and biases.</p> <p>Backpropagation allows for the backward flow of information throughout the network, helping us compute the gradient.</p> <p>The process of backward propagation is as follows:</p> <ul> <li><strong>Forward Pass</strong>: This is the first step in the backprop process where the input data is passed through the layers of the network, going through all the hidden layers to get an output prediction.</li> <li><strong>Loss Calculation</strong>: Upon receiving the output predictions, we can compare it with the ground truth by taking the difference between them via a loss function.</li> <li><strong>Backward Pass</strong>: This is the main backpropagation step where the errors are calculated, passed through the network, and eventually used to update the weights and biases through gradient descent. Here: <ul> <li>The error obtained is used to calculate the partial derivatives (gradients) of the loss/cost function with respect to the weights \(w\) and biases \(b\).</li> </ul> </li> <li><strong>Updating the Weights and Biases</strong>: The gradients calculated are propagated backwards through the network starting from the output, throughout the hidden layers. This is usually done by gradient descent, or other similar optimization parameters. <ul> <li>As the errors are being propagated, the parameters at each node are updated according to how much they contribute towards the total loss. The gradients calculated tell us how much a change in the given parameter contribute towards the total loss.</li> <li>The adjusting of the parameters is done in the opposite direction of gradient so as to reduce the error obtained.</li> <li>A <strong>learning rate</strong> hyperparameter (\(\alpha\)) is used to control how much the parameters are being updated by at each step of backpropagation.</li> </ul> </li> <li><strong>Iteration</strong>: The steps above are repeated until we either the error stops reducing significantly or the loop reaches the maximum number of predefined iterations.</li> </ul> <h3 id="importance-of-backpropagation">Importance of Backpropagation</h3> <ul> <li>Backpropagation is efficient due to its calculating the gradient, directly informing the network of how the parameters should be adjusted to reduce the error.</li> <li>Backpropagation can be applied to any differentiable loss function and network architecture. This is known as universality.</li> </ul> <h3 id="challenges-faced-in-backpropagation">Challenges faced in Backpropagation</h3> <p>Some common challenges faced during the backpropagation process are:</p> <ul> <li><strong>Vanishing and Exploding gradients</strong>: These are some of the most common problems faced during backpropagation. These occur when the gradients of the weights and biases become either too small or too large as they propagate through the network, making the learning process slow or unstable respectively. These are caused by various factors such as the choice of activation function, parameter initialization or depth of the network.</li> <li><strong>Local Minima</strong>: This is another challenge faced during backprop, where the model gets stuck in a local minimum or saddle point of the loss function. These prevent the network from reaching the optimal solution. These can be combated by using techniques such as momentum, AdamProp, or adaptive learning rates.</li> <li><strong>Computational complexity</strong>: Backpropagation can turn out to be computationally expensive or lead to issues with memory limitations due to the need for storing and updating values and gradients of all the parameters and activations. This limits how complex we can make our network and the quality of the data we can use for training. For this we use techniques like mini-batch training, pruning and quantization.</li> <li><strong>Overfitting and Underfitting</strong>: This is an issue that is faced both in the sphere of deep learning and traditional machine learning. Both these problems lead to poor performance and accuracy of the network. To prevent this, some techniques we can use are regularization, data augmentation, and dropout.</li> </ul> <h3 id="backward-propagation-explained-with-computation-graphs">Backward Propagation: Explained with Computation graphs</h3> <p>In a neural network, during <strong>forward propagation</strong> the inputs \(x\) provide the initial information that propagates through the hidden units to produce the output \(\hat{y}\), hence giving us a cost function \(J\) to work with.</p> <p>Taking the previous example, we now plug in some values and take the case that we want to compute the derivative of \(J\) with respective to \(v\).</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CG2.jpg" sizes="95vw"/> <img src="/assets/img/CG2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here, we want to see how a minor change in \(v\) would affect the value of \(J\). We know that \(J = 3v\) which in this case is 33. Increasing it by an infinitesimally small value gives us</p> \[\frac{dJ}{dv} = 3\] <p>Now, suppose we want to see \(dJ/da\), we know \(a=5\) which we bump up to \(a=5.001\). This changes \(v=11\) to \(v=11.001\) This changes J from \(J=33\) to \(J=33.003\) Hence the increase in \(J\) is 3 times the increase in \(a\) hence giving us</p> \[\frac{dJ}{da}=3\] <p>Another way to see this is that a change in \(a\) will result in a change in \(v\) which will ultimately change \(J\).</p> <p>So, by changing \(a\) we end up increasing \(v\) by \(dv/da\), whose change will cause \(J\) to increase by \(dJ/da\).</p> \[\frac{dJ}{da} = \frac{dJ}{dv}\frac{dv}{da} \;\;\text{(here we see that dv/da=1)}\] <p>In calculus, this is known as the <strong>chain rule</strong>.</p> <p>When writing code with backpropagation, we usually have a final output variable we want to optimize (\(J\) in this case). A lot of these computations involve finding the derivatives of the final output variable with respect to another intermediate variable.</p> <p>In software we can name these with long variable names like <code class="language-plaintext highlighter-rouge">dJdvar</code>. But since we are always taking derivatives with respect to \(dJ\), we just use <code class="language-plaintext highlighter-rouge">dvar</code> to represent that quantity.</p> <p>So <code class="language-plaintext highlighter-rouge">da</code> would be \(dJ/da\), <code class="language-plaintext highlighter-rouge">dv</code> would be \(dJ/dv\) and so on.</p> <p>Hence, performing the same for all the variables in our computation graph, would get us the following results.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/CG3.jpg" sizes="95vw"/> <img src="/assets/img/CG3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here, computing <code class="language-plaintext highlighter-rouge">dv</code> helps us compute <code class="language-plaintext highlighter-rouge">da</code> and <code class="language-plaintext highlighter-rouge">du</code>, and computing <code class="language-plaintext highlighter-rouge">du</code> helps us compute <code class="language-plaintext highlighter-rouge">db</code> and <code class="language-plaintext highlighter-rouge">dc</code>.</p> <h2 id="gradient-descent-logistic-regression">Gradient Descent: Logistic Regression</h2> <p>Considering the case of Logistic Regression, we can explain the process of gradient descent in a relatively simple manner using computation graphs.</p> <p>So, in the case of logistic regression, we have:</p> <ul> <li>The <strong>linear part</strong> given by \(z = w^{T}x + b\)</li> <li>The non-linear <strong>activation</strong>, giving us an output: \(\hat{y} = a = \sigma (z)\)</li> <li>The Loss: \(L(a,y) = -(y \log{a} + (1-y)\log{(1-a)})\)</li> </ul> <p>where \(a\) is the output of the logistic regression model and \(y\) is the ground truth label.</p> <h3 id="gradient-descent-on-a-single-example">Gradient Descent on a Single Example</h3> <p>In this example taking 2 inputs \(x_{1},w_{1}\) and \(x_{2},w_{2}\) along with bias \(b\). The computation graph for this is seen as</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LR_GD.jpg" sizes="95vw"/> <img src="/assets/img/LR_GD.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here, we modify the parameters \(w\) and \(b\) to reduce the loss \(L\). This process is the <strong>forward propagation</strong> for a single training example.</p> <p>For <strong>backward propagation</strong>, we want to compute the derivatives with respect to loss \(L\). To do this, we first go backwards to \(a\) computing \(dL/da\) aka <code class="language-plaintext highlighter-rouge">da</code>. Computing this, we get</p> \[\begin{align*} &amp; \frac{dL}{da} = \frac{d}{da}(-(y \log{a} + (1-y)\log{(1-a)}))\\ \Rightarrow &amp; \boxed{\frac{dL}{da} = -\frac{y}{a} + \frac{1-y}{1-a}} \end{align*}\] <p>Now that we have <code class="language-plaintext highlighter-rouge">da</code>, we can compute <code class="language-plaintext highlighter-rouge">dz</code> as</p> \[\begin{align*} &amp;\frac{dL}{dz} = \frac{dL}{da}\;\frac{da}{dz} \\ \\ &amp;\frac{da}{dz} = \frac{d}{dz}\left(\frac{1}{1+e^{-z}}\right) \\ \Rightarrow&amp;\frac{da}{dz} = \frac{e^{-z}}{(1+e^{-z})^{2}} \\ \Rightarrow&amp;\frac{da}{dz} = \left(\frac{1}{1+e^{-z}}\right)\left(1-\frac{1}{1+e^{-z}}\right) \\ \Rightarrow&amp;\boxed{\frac{da}{dz} = a(1-a)} \\ \\ &amp;\frac{dL}{dz} = \frac{dL}{da}\;\frac{da}{dz}\\ \Rightarrow&amp;\frac{dL}{dz} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right)(a(1-a))\\ \Rightarrow&amp;\frac{dL}{dz} = -y(1-a) + a(1-y)\\ \Rightarrow&amp;\boxed{\frac{dL}{dz} = a-y}\\ \Rightarrow&amp;\boxed{\frac{dL}{dz} = \frac{dL}{da}*a*(1-a)} \end{align*}\] <p>The final step is to calculate how much \(w\) and \(b\) need to change. Computing <code class="language-plaintext highlighter-rouge">dw1</code>, <code class="language-plaintext highlighter-rouge">dw2</code> and <code class="language-plaintext highlighter-rouge">db</code> we get</p> \[\begin{align*} &amp;\frac{dL}{dw_{1}} = \frac{dL}{dz}\;\frac{dz}{dw_{1}} \\ \\ &amp;\frac{dz}{dw_{1}} = \frac{d}{dw_{1}}(w_{1}x_{1}+w_{2}x_{2}+b)\\ &amp;\frac{dz}{dw_{1}} = x_{1}\\ \\ \Rightarrow&amp;\boxed{\frac{dL}{dw_{1}} = x_{1}\frac{dL}{dz} = x_{1}(a-y)}\\ \\ &amp;\text{Similarly}\\ \\ &amp;\boxed{\frac{dL}{dw_{2}} = x_{2}\frac{dL}{dz} = x_{2}(a-y)}\\ \\ &amp;\text{and}\\ \\ &amp;\boxed{\frac{dL}{db} = \frac{dL}{dz} = (a - y)} \end{align*}\] <p>Here, we see that</p> <ul> <li><code class="language-plaintext highlighter-rouge">dw1 = x1 dz</code></li> <li><code class="language-plaintext highlighter-rouge">dw2 = x2 dz</code></li> <li><code class="language-plaintext highlighter-rouge">db = dz</code></li> </ul> <p>If we want to perform the gradient descent for this example, we use the above formulae to compute <code class="language-plaintext highlighter-rouge">dz</code>, using which we would compute the resultant formulae for <code class="language-plaintext highlighter-rouge">dw1</code>, <code class="language-plaintext highlighter-rouge">dw2</code> and <code class="language-plaintext highlighter-rouge">db</code> after which we update \(w_{1}\), \(w_{2}\) and \(b\) as</p> \[\begin{align*} w_{1} &amp;:= w_{1} - \alpha \; dw_{1}\\ w_{2} &amp;:= w_{2} - \alpha \; dw_{2}\\ b &amp;:= b - \alpha \; db \end{align*}\] <h3 id="gradient-descent-on-multiple-examples">Gradient Descent on Multiple Examples</h3> <p>Considering \(m\) number of examples as opposed to a single example to work with, the cost function for gradient descent is given by</p> \[J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m}L(a^{(i)},y^{(i)})\] <p>where \(a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)})= \sigma(w^{T}x^{(i)} + b)\).</p> <p>The overall cost function is the average of the losses, so the derivative with respect to \(w_{1}\) is also seen to be the average of derivatives with respect to \(w_{1}\) of the individual loss terms.</p> \[\frac{\partial}{\partial w_{1}}J(w,b) = \frac{1}{m}\sum\limits_{i=1}^{m} \underbrace{\frac{\partial}{\partial w_{1}} L(a^{(i)},y^{(i)})}_{dw^{(i)} - (x^{(i)},y^{(i)})}\] <p>Algorithmically, This would be:</p> <ul> <li>Initialize <code class="language-plaintext highlighter-rouge">J=0</code>, <code class="language-plaintext highlighter-rouge">dw1=0</code>, <code class="language-plaintext highlighter-rouge">dw2=0</code> and <code class="language-plaintext highlighter-rouge">db=0</code>.</li> <li><code class="language-plaintext highlighter-rouge">for i = 1 to m</code> over the training set <ul> <li>Compute derivative for each training example <code class="language-plaintext highlighter-rouge">z_i = w_T * x_i + b</code></li> <li><code class="language-plaintext highlighter-rouge">a_i = sigma(z_i)</code></li> <li><code class="language-plaintext highlighter-rouge">J += -((y_i * log(a_i)) + ((1 - y_i)* log(1 - a_i)))</code></li> <li><code class="language-plaintext highlighter-rouge">dz_i = a_i - y_i</code></li> <li><code class="language-plaintext highlighter-rouge">dw1 += x1_i * dz_i</code></li> <li><code class="language-plaintext highlighter-rouge">dw2 += x2_i * dz_i</code> (do this for all features, in this case we only take 2)</li> <li><code class="language-plaintext highlighter-rouge">db += dz_i</code></li> <li><code class="language-plaintext highlighter-rouge">J /= m</code></li> <li><code class="language-plaintext highlighter-rouge">dw1 /= m</code></li> <li><code class="language-plaintext highlighter-rouge">dw2 /= m</code></li> <li><code class="language-plaintext highlighter-rouge">db /= m</code></li> </ul> </li> <li>Then we update the weights accordingly <ul> <li><code class="language-plaintext highlighter-rouge">w1 := w1 - (alpha * dw1)</code></li> <li><code class="language-plaintext highlighter-rouge">w2 := w2 - (alpha * dw2)</code></li> <li><code class="language-plaintext highlighter-rouge">b := b - (alpha * db)</code></li> </ul> </li> <li>Update <code class="language-plaintext highlighter-rouge">J</code> with the new values of <code class="language-plaintext highlighter-rouge">w1</code>, <code class="language-plaintext highlighter-rouge">w2</code> and <code class="language-plaintext highlighter-rouge">b</code>.</li> <li>Repeat this for multiple steps of gradient descent</li> </ul> <p>Here <code class="language-plaintext highlighter-rouge">J</code>, <code class="language-plaintext highlighter-rouge">dw1</code>, <code class="language-plaintext highlighter-rouge">dw2</code> and <code class="language-plaintext highlighter-rouge">db</code> act as accumulators to sum over the entire training set.</p> <p>Two <em>weaknesses</em> with the calculations above are.</p> <ul> <li>If we implement Logistic Regression this way, we use two for loops. Using explicit for loops in the code makes the algorithm less efficient.</li> <li>With large datasets, we need to use techniques such as <strong>vectorization</strong> to improve the performance.</li> </ul> <h1 id="single-layer-neural-networks">Single Layer Neural Networks</h1> <p>Now that we have an idea about the basic steps of training neural networks, we can now look at single layer neural networks to understand how they work.</p> <p>The most common type of artificial neurons used in today’s day and age is the <strong>sigmoid neuron</strong> which we have discussed <a href="#logistic-regression">earlier</a>.</p> <p>We can picture the sigmoid neuron as follows</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SNN1.jpg" sizes="95vw"/> <img src="/assets/img/SNN1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A single neuron. </div> <p>Here we have the inputs \([x_1,x_2,x_3]\) which are stacked vertically. This is known as the <strong>input layer</strong>. These values are input to a node which computes the linear \(z\) value, and the non-linear activation \(a\) to give us the output prediction \(\hat{y}\) and loss \(L\).</p> <p>A neural network can be seen as a stacking of several such functional units as shown below.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SNN2.jpg" sizes="95vw"/> <img src="/assets/img/SNN2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A shallow neural network. </div> <p>This is known as a <strong>shallow neural network</strong>.</p> <p>Here, the first stack on nodes would correspond to a \(z\)-like calculation as well as an \(a\)-like calculation, the next node will correspond to another \(z\) and \(a\) like calculation, giving us a prediction \(\hat{y}\).</p> <p>By varying the weights and thresholds we get different decision making models.</p> <p>The layer of circles shown in the above image after the input is known as the <strong>hidden layer</strong>. The name hidden refers to the fact that the true value of the weights and biases of these nodes are not observed. We mainly focus on the inputs and outputs when working with neural networks.</p> <p>The final layer here is a single node, called the output layer, which is responsible for generating the predicted value \(\hat{y}\).</p> <h2 id="neural-network-notations">Neural Network Notations</h2> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SNN3.jpg" sizes="95vw"/> <img src="/assets/img/SNN3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Each layer of the neural network are represented with superscript square brackets \([1],[2],\dots\). The input features of the network are referred to as \(A^{[0]}\) which are passed to the first hidden layer</li> <li>The first hidden layer generates a set of linear functions given by \(z^{[1]}\) consisting of weights and biases \(W^{[1]}\) and \(b^{[1]}\), which are fed to an activation function giving us a set of outputs \(a^{[1]}_i\) where \(i\) refers to each node. For any subsequent hidden layers we would represent these values with the respective layer numbers in the superscript square brackets.</li> <li>The output layer in this example has only one node which generates the output \(a^{[2]}\), which in this case can be seen as \(\hat{y}=a^{[2]}\).</li> </ul> <p>The network in this case is called a <strong>two layer neural network</strong> or a <strong>single hidden layer neural network</strong> because when counting the layers in a neural network, we do not count the input layer. The hidden layer here is layer 1 and the output layer here is layer 2.</p> <p>Each of the two here layers will have parameters associated with them such as \(w^{[1]}, b^{[1]}\) for the hidden layer. \(w\) will be a \((4,3)\) matrix in this case and \(b\) will be a \((4,1)\) vector. Similarly the output layer will have \(w^{[2]}, b^{[2]}\) of dimensions \((1,4)\) and \((1,1)\).</p> <h1 id="computing-the-output-of-a-neural-network">Computing the Output of a Neural Network</h1> <p>The process of computing the output of a neural network is similar to logistic regression but repeated multiple times at each node of the neural network.</p> <p>We can visualize the calculations that take place in a node as given below</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LogRegRepresentation.jpg" sizes="95vw"/> <img src="/assets/img/LogRegRepresentation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Consider the first layer of a network. Here the computation happens in two steps</p> <ul> <li>Computing \(z^{[1]} = w^{[1]T}x + b^{[1]}\)</li> <li>Computing \(a^{[1]} = \sigma(z^{[1]})\)</li> </ul> <p>Here \(z\) is the linear part, \(a\) is the non-linear activation function output, \(w\) is the weight and \(b\) is the bias.</p> <p>For multiple nodes in a given layer, we would get the following calculations</p> \[\begin{align*} z_{1}^{[1]} &amp;= w_{1}^{[1]T}x + b_{1}^{[1]} &amp; a_{1}^{[1]} = \sigma(z_{1}^{[1]})\\ z_{2}^{[1]} &amp;= w_{2}^{[1]T}x + b_{2}^{[1]} &amp; a_{2}^{[1]} = \sigma(z_{2}^{[1]})\\ z_{3}^{[1]} &amp;= w_{3}^{[1]T}x + b_{3}^{[1]} &amp; a_{3}^{[1]} = \sigma(z_{3}^{[1]})\\ z_{4}^{[1]} &amp;= w_{4}^{[1]T}x + b_{4}^{[1]} &amp; a_{4}^{[1]} = \sigma(z_{4}^{[1]})\\ \end{align*}\] <p>In practice, these equations are vectorized to compute \(Z\) as a vector. If we had 4 logistic regression units stacked together into a layer, the calculation would look something like</p> \[Z^{[1]} = \begin{bmatrix} z_{1}^{[1]} \\ z_{2}^{[1]} \\ z_{3}^{[1]} \\ z_{4}^{[1]} \end{bmatrix} = \begin{bmatrix} \dots &amp; w_{1}^{[1]T} \dots \\ \dots &amp; w_{2}^{[1]T} \dots \\ \dots &amp; w_{3}^{[1]T} \dots \\ \dots &amp; w_{4}^{[1]T} \dots \end{bmatrix} \cdot \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \end{bmatrix} + \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \\ b_{4} \end{bmatrix}\] <p>This gives us the outputs for the first layer \(a^{[1]}\).</p> <h1 id="activation-functions">Activation Functions</h1> <p><strong>Activation functions</strong> serve the purpose of introducing a non-linearity to our model, enabling it to learn complex patterns from the data given to it.</p> <p>In the calculations being done in a neural network the activation functions look something like</p> \[\begin{align*} z^{[1]} &amp;= W^{[1]}x + b^{[1]}\\ a^{[1]} &amp;= g^{[1]}(z^{[1]})\\ z^{[2]} &amp;= W^{[2]}a^{[1]} + b^{[2]}\\ a^{[2]} &amp;= g^{[2]}(z^{[2]})\\ \end{align*}\] <p>where \(g\) is the activation function.</p> <p>We will now look at some of the commonly used activation functions.</p> <h2 id="sigmoid-function">Sigmoid Function</h2> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Sigmoid.png" sizes="95vw"/> <img src="/assets/img/Sigmoid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The sigmoid function maps all the values input into it to a binary output.</li> <li>Equation: \(a = \frac{1}{1+e^{-z}}\)</li> <li>Value Range: \([0,1]\)</li> <li>Issues: Sigmoid functions can result in vanishing gradient problems</li> </ul> <h2 id="hyperbolic-tangent-tanh-function">Hyperbolic Tangent (Tanh) Function</h2> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Tanh.png" sizes="95vw"/> <img src="/assets/img/Tanh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Equation: \(a = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\)</li> <li>Value range: \([-1,1]\)</li> <li>Pros: This function is works better than the sigmoid function as the activations have a zero mean, making the learning process easier.</li> <li>Con: This function also suffers from the vanishing gradient problem that the sigmoid function faces.</li> <li>This can be seen as a mathematically shifted version of the Sigmoid function.</li> </ul> <h2 id="rectified-linear-unit-relu-function">Rectified Linear Unit (ReLU) Function</h2> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ReLU.png" sizes="95vw"/> <img src="/assets/img/ReLU.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Equation: \(a = max(0,z)\)</li> <li>Value range: \([0,\infty]\)</li> <li>Pros: This function is fast to compute and mitigates the vanishing gradient problem.</li> <li>Con: For negative \(z\) values the gradient is zero.</li> <li>The <strong>Leaky ReLU</strong> function allows for a small non-zero gradient when the value of \(z\) is less than 0, improving the performance of the ReLU function.</li> <li>Leaky ReLU Equation: \(a = max(cz,z)\)</li> <li>Leaky ReLU Value Range: \([-\infty,\infty]\)</li> <li>The modification here can be done by changing the slope for \(z &lt; 0\).</li> </ul> <h2 id="general-rules">General Rules</h2> <ul> <li>Sigmoid functions are used for binary classification problems and are useful in the output layer</li> <li>ReLU is often the default function used for hidden layers. Tanh can sometimes be used too.</li> <li>Variants like leaky ReLU are preferred for improvements in performance of the network.</li> </ul> <p>Data science problems are diverse hence needing for a lot of experimentation in terms of using different activation functions.</p> <p>Without activation functions the neural networks’ behavior would be similar to a linear regression model, being unable to capture the complexities of the data.</p> <h1 id="derivatives-of-activation-functions">Derivatives of Activation Functions</h1> <p>When performing backpropagation through a neural network, we need to perform derivatives on the outputs of the different layers. For this reason, it is useful to know how the derivatives of different activation functions are found.</p> <h2 id="sigmoid-function-1">Sigmoid Function</h2> <p>Given the sigmoid activation \(g(z) = \frac{1}{1+e^{-z}}\), we get \(\frac{d}{dz}g(z)\) as</p> \[g'(z) = \frac{1}{1+e^{-z}}\left(1 - \frac{1}{1+e^{-z}}\right)\] \[\Rightarrow\boxed{\frac{d}{dz}g(z) = g(z) (1 - g(z))}\] <p>The complete derivation for this has been discussed previously and can be found <a href="#gradient-descent-on-a-single-example">here</a>.</p> <h2 id="tanh-function">Tanh Function</h2> <p>Given the tanh function \(g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\), We find \(g'(z)\) as</p> \[\begin{align*} &amp;g'(z) = \frac{(e^{z} + e^{-z})^{2} - (e^{z} - e^{-z})^{2}}{(e^{z} + e^{-z})^{2}}\\ &amp;g'(z) = 1 - \frac{(e^{z} - e^{-z})^{2}}{(e^{z} + e^{-z})^{2}}\\ \Rightarrow &amp;\boxed{g'(z) = 1 - g(z)^{2}} \end{align*}\] <h2 id="relu-function">ReLU Function</h2> <p>Given the ReLU function \(g(z) = max(0,z)\), we can find the derivative of it to be</p> \[g'(z) = \begin{cases} 0 &amp; \text{if } z &lt; 0 \\ 1 &amp; \text{if } z \geq 0 \\ \text{undefined} &amp; \text{if } z = 0 \end{cases}\] <p>In software, the last part won’t necessarily be correct and it works just fine at \(z = 0\).</p> <h2 id="leaky-relu">Leaky ReLU</h2> <p>Here, the activation function is \(g(z) = max(cz,z)\) hence giving us:</p> \[g'(z) = \begin{cases} c &amp; \text{if } z &lt; 0, c \in (0,1) \\ 1 &amp; \text{if } z \geq 0 \end{cases}\] <h1 id="weight-initialization-in-neural-networks">Weight Initialization in Neural Networks</h1> <p>When building a Neural Network it is important to ensure that the initial weights and biases selected are done so randomly.</p> <p>Consider a case with two input features and two hidden units. The input matrix \(w^{[1]}\) will be of \((2,2)\) dimension.</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Random_Init_Example.jpg" sizes="95vw"/> <img src="/assets/img/Random_Init_Example.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here, we take</p> \[w^{[1]} = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\] \[b^{[1]} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\] <p>Here, for any example</p> \[a^{[1]}_{1} = a^{[1]}_{2} = 0\] <p>And, when computing backpropagation we get</p> \[dz^{[1]}_{1} = dz^{[1]}_{2} = 0\] <p>We see that initializing the bias to zero is fine, but initializing the weights to zero causes issues.</p> <p>Here, the hidden units will be completely identical as they turn out to be the same function. Hence, after training, our neural network will have the two hidden units, computing the exact same function, having the same amount of influence on their outputs. Hence, the updates performed will also be exactly the same.</p> <p>In addition, zero weights make it likely that the neurons are activated in flat regions of the activation function, hence leading to a small gradient value, leading to the vanishing gradient problem.</p> <h2 id="how-random-weights-help">How Random Weights Help</h2> <ul> <li>Random initialization of the weights ensures that each neuron has a unique starting linear function, hence computing different activations, breaking the symmetry. This allows the network to learn from the errors, making updates on each neuron in the network.</li> <li>Using small initialization values ensures that the activation functions operate in ranges where the gradients aren’t flat.</li> </ul> <h2 id="weight-initialization-in-practice">Weight Initialization in Practice</h2> <ul> <li>In practice the weights are initialized to small random values multiplied by a constant like \(0.01\) to ensure the values are small. In the case of sigmoid and tanh functions, this ensures that the values taken up by the weights are within the working regions for these functions</li> <li>For biases, it is fine to initialize the values to zero as the weights themselves being initialized randomly is enough to break the symmetry.</li> <li>For deeper networks there exist advanced initialization techniques, but small random initialization values are useful with shallow networks.</li> </ul> <h1 id="deep-neural-networks">Deep Neural Networks</h1> <p><strong>Deep neural networks (DNNs)</strong>, as opposed to shallow networks are capable of learning complex functions. However, it is hard to predict how many layers deep our network would have to be. Hence, we normally start with a single layer and move on from there, gradually increasing the number of layers.</p> <p>The number of layers can be viewed as a hyperparameter which we use to evaluate our model.</p> <h2 id="notations-for-dnns">Notations for DNNs</h2> <p>Consider a 4 layer NN as shown below, with 3 hidden layers.</p> <div class="row justify-content-center mt-3"> <div class="col-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DNN1.jpg" sizes="95vw"/> <img src="/assets/img/DNN1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The notation here would be:</p> <ul> <li>\(L\) : to denote the number of layers in the network (4 in this case)</li> <li>\(n_{x}\) : gives us the number of input features</li> <li>\(n^{[l]}\) : gives us the number of nodes/units in a layer \(l\). Here \(n^{[1]} = n^{[2]} = 5, n^{[0]} = n_{x} = 3\) and so on</li> <li>\(a^{[l]}\): activations in layer \(l\)</li> <li>\(a^{[l]} = g^{[l]}(z^{[l]})\): Activation function indexed by \(l\)</li> <li>\(w^{[l]}\): weights for \(z^{[l]}\) in the given layer \(l\)</li> <li>\(b^{[l]}\): biases for the layer \(l\)</li> <li>\(x = a^{[0]}\): inputs</li> <li>\(\hat{y} = a^{[L]}\): output predictions</li> </ul> <h2 id="forward-propagation-1">Forward Propagation</h2> <p>We have already seen how <a href="#forward-propagation">forward propagation</a> works in shallow neural networks. This process passes the input data to the network, computing the activations for each layer sequentially from the input layer to the output layer which gives us a prediction, classification or any other result that a network can produce.</p> <p>The forward propagation steps for a deep neural network are as follows:</p> <ul> <li><strong>Input layer</strong>: The input layer’s data is fed into the network serving as the initial activations for the first layer of the network</li> <li><strong>Linear Combination</strong>: For each neuron in the hidden layers, a weighted sum of the inputs are calculated with the associated weights and biases associated with them. For a layer \(l\), the linear combination formula is \(z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\), where \(w^{[l]},b^{[l]}\) are the weights and biases of the current layer and \(a^{[l-1]}\) is the activation from the previous layer.</li> <li><strong>Activation Function</strong>: The linear part obtained here is input to the activation function which introduces the non-linearity in the system, helping it learn complex patterns from the data.</li> </ul> <p>For a single training example, this process would look like:</p> \[\begin{align*} z^{[l]} &amp;= w^{[l]}a^{[l-1]} + b^{[l]}\\ a^{[l]} &amp;= g^{[l]}(z^{[l]}) \end{align*}\] <p>When implementing this in code, the general rule of thumb is to avoid the use of for loops and to use vectorization for matrix multiplication on multiple examples. However, in this case for propagating over multiple layers from \(1\) to \(L\), this is unavoidable.</p> <h2 id="the-building-blocks-of-dnns">The Building Blocks of DNNs</h2> <p>We have already seen how forward propagation and backward propagation are the key components needed for a neural network to work, and how putting these together allow for it to train on the data provided to it.</p> <p>Considering a single layer \(l\) of a DNN, the process of training it with weights and biases \(W^{[l]}, b^{[l]}\) are as follows:</p> <h4 id="for-forward-propagation">For forward propagation</h4> <ul> <li>Input the previous activation \(a^{[l-1]}\)</li> <li>Calculate linear part and activation <ul> <li> \[Z^{[l]} = W^{[l]}a^{[l]}+b^{[l]}\] </li> <li> \[a^{[l]} = g^{[l]}(Z^{[l]})\] </li> </ul> </li> <li>Cache \(Z^{[l]}\) for later use</li> </ul> <h4 id="for-backward-propagation">For backward propagation</h4> <ul> <li>Input \(da^{[l]}\) <ul> <li> \[da^{[l-1]} = W^{[l]T}dZ^{[l]}\] </li> </ul> </li> <li>Output \(da^{[l-1]},dW^{[l]}\), and \(db^{[l]}\) <ul> <li> \[dZ^{[l]} = da^{[l]} \times g^{[l]'}(Z^{[l]}) \Rightarrow dZ^{[l]} = W^{[l]T}dZ^{[l]} \times g^{[l]'}(Z^{[l]})\] </li> <li> \[dW^{[l]} = dZ^{[l]}a^{[l-1]T}\] </li> <li> \[db^{[l]} = dZ^{[l]}\] </li> </ul> </li> <li>for this we use the cached \(Z^{[l]}\) value and in addition to outputting \(da^{[l-1]}\), we take the gradients we want in order to implement gradient descent for learning.</li> </ul> <p>This process can be visualized as follows</p> <div class="row justify-content-center mt-3"> <div class="col-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DNN4.jpg" sizes="95vw"/> <img src="/assets/img/DNN4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Each of the units visualized here can be chained together to give us the whole process of the forward and backward propagation processes.</p> <p>Conceptually, it is useful to think of the cache as storing the \(Z\) values for the backward functions to use. When implementing this, however, we see that the cache is a convenient way to get the \(W^{[1]},b^{[1]}\) into the backward function as well. Hence, we store the \(W\) and \(b\) values alongside \(Z\) in the cache.</p> <h1 id="parameters-and-hyperparameters">Parameters and Hyperparameters</h1> <p>Deep learning involves the careful tuning of the parameters (weights and biases) and hyperparameters.</p> <p>Parameters are intrinsic parts of the model and are learned and optimized from the input data during training.</p> <p>Hyperparameters are set manually and help us govern the process of learning the parameters.</p> <p>A rundown of some of the parameters and hyperparameters we work with when building a DNN are as follows:</p> <ol> <li>Parameters <ul> <li>Weights, \(W\) and biases, \(b\) are the main parameters in a neural network that get updated during the training process to minimize the error.</li> <li>Coefficients in the simple linear regression model(\(y=mx+c\)) are another example of parameters where the slope (\(m\)) and y-intercept (\(c\)) are the parameters.</li> <li>In support vector machines, the support vectors, which are data points closest to the decision boundary, are the parameters.</li> </ul> </li> <li>Hyperparameters <ul> <li>The <strong>Learning Rate (\(\alpha\))</strong>: This determines he size of the steps taken during the optimization process.</li> <li>The <strong>number of iterations</strong> the optimization algorithm runs for.</li> <li>The <strong>number of hidden layers (L)</strong> which add to the depth and overall complexity of the network.</li> <li>The <strong>number of hidden units</strong> in each layer, which determines the size of each layer.</li> <li>The <strong>activation functions</strong> used which add non-linearity to the model, hence allowing it to learn from the losses generated at each layer.</li> </ul> </li> </ol> <p>Hyperparameters affect the efficiency and accuracy of the model when training the parameters \(W\) and \(b\). Due to this, it is important that they are selected appropriately.</p> <h1 id="why-neural-networks-work-well">Why Neural Networks Work Well</h1> <p>Consider an example where we are building a face detection algorithm with a large number of examples to work with. The layers of the network would work something like the following:</p> <ul> <li>The <strong>first layer</strong> would work on detecting edges in several orientations like vertical, horizontal, at different angles, and so on.</li> <li>The <strong>second layer</strong> can group together features from the first layer to form slightly complex facial features like eyes, lips, ears and so on.</li> <li>The <strong>third and later layers</strong> can then take up the task of combining facial the features from the previous layer to generate examples of faces that can be used to detect faces in the output layer.</li> </ul> <p>Similarly for speech recognition</p> <ul> <li>The <strong>first layer</strong> would work on detecting low level audio features in the form of waveforms.</li> <li>The <strong>second layer</strong> can group together the waveforms from the first layer to compose the low level units of sound (phenomes).</li> <li>The <strong>third and later layers</strong> can then work on generating words, phrases and sentences based on the inputs fed to it.</li> </ul> <p>This process can be described as <strong>hierarchical feature learning</strong>, where we see that the problem being worked on is broken down into smaller units from which more and more complex features are being built. This resembles how we believe a human brain processes information, however, it is not necessarily an accurate comparison as the way the brain works is still unknown to medical professionals to this day.</p> <p>Deep learning and Artificial Intelligence have garnered a lot of public interest as of late, being a branding catchphrase used by many companies and organizations for their products. This is rightfully so as deep learning, though still being a bit rough around the edges have demonstrated superior performance in several fields of application.</p> <p>While we know that deep learning models work well, we need to first understand whether a given problem warrants the use of such an architecture. It is always a good practice to first use simpler models like decision trees, logistic regression or networks with a few layers before moving on to more complex and deep architectures.</p> <h1 id="conclusion">Conclusion</h1> <p>In this article, we have seen how deep artificial neural networks work from the ground up, gaining an understanding of the underlying mathematics and how they have the potential to learn increasingly complex functions, hence finding a great deal of use in several fields.</p> <p>Now that we have these concepts down, it would be good to know how the training process for more complex neural networks would work. The next post will highlight this topic. We will also look at an implementation of a neural network from scratch and comparing its performance with pre-made deep learning frameworks such as Tensorflow and PyTorch.</p>]]></content><author><name></name></author><category term="deep-learning"/><category term="machine-learning"/><category term="deep-learning"/><summary type="html"><![CDATA[Topics covered: Introduction to Neural Networks and the underlying mathematics behind the training process for shallow and deep Artificial Neural Networks.]]></summary></entry></feed>