{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the packages we need to make this work. Luckily since we are doing this from scratch, don't have to use too many of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib qt\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the matyas function along with its derivatives, the weighted average calculator and finally, the backpropagation function that helps us with the root finding process on the surface of the matyas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matyas(a):\n",
    "    return 0.26*(a[0]**2 + a[1]**2) - 0.48*a[0]*a[1]\n",
    "\n",
    "\n",
    "def matyas_derivatives(a):\n",
    "    dx = 0.52*a[0] - 0.48*a[1]\n",
    "    dy = 0.52*a[1] - 0.48*a[0]\n",
    "    return dx, dy\n",
    "\n",
    "def get_weighted_average(v_prev, beta, dtheta):\n",
    "    return beta*v_prev + ((1-beta)*dtheta)\n",
    "\n",
    "def descend(a, alpha=0.4, beta1 = 0.90, beta2 = 0.98, iter=5000, optimizer = 'GD'):\n",
    "    ai = []\n",
    "    f = matyas(a)\n",
    "    da = 0\n",
    "    epsilon = 10**(-1)\n",
    "    vda = 0\n",
    "    sda = 0\n",
    "\n",
    "    for i in range(1,iter+1):\n",
    "        f = matyas(a)\n",
    "        \n",
    "        # if (ai[-1][1] < 10^(-1)):\n",
    "        #     break\n",
    "\n",
    "        da = np.array(matyas_derivatives(a))\n",
    "\n",
    "        if (optimizer == 'GD'):\n",
    "            ai.append([a,f,da,vda,sda])\n",
    "            a = a - np.dot(alpha,da)\n",
    "        elif (optimizer == 'momentum'):\n",
    "            vda = get_weighted_average(vda, beta1, da)\n",
    "            ai.append([a,f,da,vda])\n",
    "            a = a - np.dot(alpha,vda)\n",
    "        elif (optimizer == 'rmsprop'):\n",
    "            sda = get_weighted_average(sda, beta2, da**2)\n",
    "            ai.append([a,f,da,sda, alpha/(np.sqrt(sda) + epsilon)])\n",
    "            a = a - (np.dot(alpha,da)/(np.sqrt(sda) + epsilon))\n",
    "        elif (optimizer == 'adam'):\n",
    "            vda = get_weighted_average(vda, beta1, da)/(1 - (beta1**iter))\n",
    "            sda = get_weighted_average(sda, beta2, da**2)/(1 - (beta2**iter))\n",
    "            ai.append([a,f,da,vda,sda, alpha/(np.sqrt(sda) + epsilon)])\n",
    "            a = a - (np.dot(alpha,vda)/(np.sqrt(sda) + epsilon))\n",
    "\n",
    "    # ai = np.array(ai)\n",
    "    return ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take the point (-6,10) as our starting point and begin the process of moving down the curve using the optimization algorithms discussed before.\n",
    "\n",
    "We receieve the following as outputs:\n",
    "- `a`: The latest set of x and y values obtained\n",
    "- `f`: The output of the function at each step taken\n",
    "- `da`: The derivatives of the matyas function at each point\n",
    "- `vda`: The exponential average of the weight gradients (only for Momentum and ADAM)\n",
    "- `sda`: The exponential average of the square of the weight gradients (only for RMSProp and ADAM)\n",
    "-  `updated learning rate`: This, as the name suggests, is the updated learning rate that is obtained as a result of the adaptive learning rate effect induced in RMSProp and ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = descend(np.array([-6,10]), iter=4000)\n",
    "momentum = descend(np.array([-6,10]),beta1=0.9, optimizer='momentum', iter=4000)\n",
    "rmsprop = descend(np.array([-6,10]), optimizer='rmsprop', iter=4000)\n",
    "adam = descend(np.array([-6,10]),beta1=0.9, optimizer='adam', iter=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Momentum on the loss function\n",
    "\n",
    "For simplicity's sake, we will consider the matyas function here to be a cost function for which we want to find the global minimum.\n",
    "\n",
    "Now that we have the outputs of our optimizing functions, we want to compare the weighted average values of the parameters (x and y here), with the original gradient we would normally receive from gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_gradients = np.array([point[2] for point in gd])\n",
    "momentum_gradients = np.array([point[2] for point in momentum])\n",
    "\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(4, figsize=(8,10))\n",
    "\n",
    "ax1.set_title(\"X updates\")\n",
    "ax1.plot(momentum_gradients[:,0], label='Momentum x Updates (due to V_dx)')\n",
    "ax1.plot(gd_gradients[:,0], label='Gradient Descent x Updates (due to dx)')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('dx')\n",
    "ax1.legend(fontsize='small')\n",
    "\n",
    "ax2.set_title(\"Y updates\")\n",
    "ax2.plot(momentum_gradients[:,1], label='Momentum y Updates (due to V_dy)')\n",
    "ax2.plot(gd_gradients[:,1], label='Gradient Descent y Updates (due to dy)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('dy')\n",
    "ax2.legend(fontsize='small')\n",
    "\n",
    "ax3.set_title(\"X updates (Log Scale)\")\n",
    "ax3.plot(momentum_gradients[:,0], label='Momentum x Updates (due to V_dx)')\n",
    "ax3.plot(gd_gradients[:,0], label='Gradient Descent x Updates (due to dx)')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('dx')\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend(fontsize='small')\n",
    "\n",
    "ax4.set_title(\"Y updates (Log Scale)\")\n",
    "ax4.plot(momentum_gradients[:,1], label='Momentum y Updates (due to V_dy)')\n",
    "ax4.plot(gd_gradients[:,1], label='Gradient Descent y Updates (due to dy)')\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('dy')\n",
    "ax4.set_yscale('log')\n",
    "ax4.legend(fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, if we focus on the normal scale plots, it would seem like the parameter updates in the case of gradient descent seem more straightforward and converge to zero faster than in the case of momentum. However this is not necessarily the case as we will see soon.\n",
    "\n",
    "Here, we can also clearly see that the weighted average updates (V_dx and v_dy) are more resilient to sudden changes in the gradient value due to which they don't react as quickly to changes in the value of the gradient updates in the opposite direction as they normally would if considering just the gradients. This leads to the momentum updates occuring in a meandering manner as opposed to hyperfocusing towards the minimum.\n",
    "\n",
    "Now, converting the y-scale to a logarithmic scale allows us to take a closer look at the updates as they approach zero. Here we see that the parameter updates for momentum eventually go lower than the updates for normal gradient descent. This in contrast to what we notice during the initial analysis of the curves in normal scale.\n",
    "\n",
    "To understand the overall effect these parameter updates have on the root finding process we have to see how they affect the cost function at each iteration, which is given by the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "gd_updates = np.array([point[1] for point in gd])\n",
    "momentum_updates = np.array([point[1] for point in momentum])\n",
    "plt.plot(momentum_updates[:], label='Momentum Updates')\n",
    "plt.plot(gd_updates[:], label='Gradient Descent Updates')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the momentum updates to f(x), though initially erratic, gradually end up moving faster towards the minimum value of 0 than normal gradient descent as the weighted average preserves its momentum, even if subsequent gradient values get extremely smaller.\n",
    "\n",
    "## Effects of RMSProp on the loss function\n",
    "\n",
    "Similar to the previous section, we will now have a look at how RMSProp affects our root finding process starting with the parameter updates.\n",
    "\n",
    "Here, RMSProp focuses on the learning rate during the parameter updating process as opposed to affecting the actual parameters in the case of Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_gradients = np.array([point[2] for point in gd])\n",
    "rmsprop_gradients = np.array([point[2] for point in rmsprop])\n",
    "\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(4, figsize=(8,10))\n",
    "\n",
    "ax1.set_title(\"X updates\")\n",
    "ax1.plot(rmsprop_gradients[:,0], label='RMSProp x Updates (due to alpha*dx/sqrt(S_dx))')\n",
    "ax1.plot(gd_gradients[:,0], label='Gradient Descent x Updates (due to dx)')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('dx')\n",
    "ax1.legend(fontsize='x-small')\n",
    "\n",
    "ax2.set_title(\"Y updates\")\n",
    "ax2.plot(rmsprop_gradients[:,1], label='RMSProp y Updates (due to alpha*dx/sqrt(S_dy))')\n",
    "ax2.plot(gd_gradients[:,1], label='Gradient Descent y Updates (due to dy)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('dy')\n",
    "ax2.legend(fontsize='x-small')\n",
    "\n",
    "ax3.set_title(\"X updates (Log Scale)\")\n",
    "ax3.plot(rmsprop_gradients[:,0], label='RMSProp x Updates (due to alpha*dx/sqrt(S_dx))')\n",
    "ax3.plot(gd_gradients[:,0], label='Gradient Descent x Updates (due to dx)')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('dx')\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend(fontsize='x-small')\n",
    "\n",
    "ax4.set_title(\"Y updates (Log Scale)\")\n",
    "ax4.plot(rmsprop_gradients[:,1], label='RMSProp y Updates (due to alpha*dx/sqrt(S_dy))')\n",
    "ax4.plot(gd_gradients[:,1], label='Gradient Descent y Updates (due to dy)')\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('dy')\n",
    "ax4.set_yscale('log')\n",
    "ax4.legend(fontsize='x-small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note a significant drop in the value of the updates occuring with each iteration. However, this doesn't tell us much about what is happening with the loss function.\n",
    "\n",
    "Instead, we need to focus on the value of the learning rate with each iteration. To be more precise, we need to focus on how the learning rate changes throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(3,figsize=(8,8))\n",
    "rmsprop_lrs = np.array([point[4] for point in rmsprop])\n",
    "rmsprop_sda = np.array([point[3] for point in rmsprop])\n",
    "\n",
    "ax1.set_title(\"RMSE gradients vs Iteration\")\n",
    "ax1.plot(rmsprop_sda[:,0], label='S_dx')\n",
    "ax1.plot(rmsprop_sda[:,1], label='S_dy')\n",
    "ax1.set_xlabel(r'Iteration')\n",
    "ax1.set_ylabel(r'f(x)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title(\"1/sqrt(RMSE gradients) vs Iteration\")\n",
    "ax2.plot(1/(np.sqrt(rmsprop_sda[:,0]) + 10**(-7)), label='1/sqrt(S_dx)')\n",
    "ax2.plot(1/(np.sqrt(rmsprop_sda[:,1]) + 10**(-7)), label='1/sqrt(S_dy)')\n",
    "ax2.set_xlabel(r'Iteration')\n",
    "ax2.set_ylabel(r'f(x)')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_title(\"Adaptive Learning Rate Value vs Iteration\")\n",
    "ax3.plot(rmsprop_lrs[:,0], label='learning rate updates for x')\n",
    "ax3.plot(rmsprop_lrs[:,1], label='learning rate updates for y')\n",
    "ax3.set_xlabel(r'Iteration')\n",
    "ax3.set_ylabel(r'f(x)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the learning rate increases to values much higher than the default value that has been set initially as the optimization algorithm approaches 250 iterations.\n",
    "\n",
    "This amplifies the size of the steps taken by the gradients greatly as the values of the gradients themselves approach the minimum.\n",
    "\n",
    "The overall effect this on the cost function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "gd_updates = np.array([point[1] for point in gd])\n",
    "rmsprop_updates = np.array([point[1] for point in rmsprop])\n",
    "plt.plot(rmsprop_updates[:], label='RMSProp Updates')\n",
    "plt.plot(gd_updates[:], label='Gradient Descent Updates')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice an extreme drop in the loss function past 100 iterations as opposed to gradient descent. This shows how effective RMSProp is in comparison to normal gradient descent.\n",
    "\n",
    "## The ADAM Optimizer\n",
    "\n",
    "The ADAM Optimizer implements concepts from both Momentum and ADAMProp, changing both, the learning rate and the gradient updates with every iteration. This hence leads to an implementation of momentum alongside an effective dampening of the updates occuring in directions other than towards the minimum.\n",
    "\n",
    "The curve below shows how the updates in the loss function looks in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "gd_updates = np.array([point[1] for point in gd])\n",
    "adam_updates = np.array([point[1] for point in adam])\n",
    "plt.plot(adam_updates[:], label='ADAM Updates')\n",
    "plt.plot(gd_updates[:], label='Gradient Descent Updates')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the loss function for the ADAM optimizer possesses the momemtum-like characteristics from gradient descent with momentum. In addition, the effect of the RMSProp part ensures that the learning rate compounds as the learning curve approaches the minimum.\n",
    "\n",
    "The effect here is that the jumps in the learning process attributed to the momentum mechanism are constrained by the adaptive learning rate which is constantly monitoring the RMS gradient value. This effectively dampens the oscillations of the loss curve, focusing it to learn faster in the direction of the root of the loss function.\n",
    "\n",
    "Finally, we can compare all of the loss function values for each of the curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(gd_updates[:], label='Gradient Descent Updates')\n",
    "plt.plot(momentum_updates[:], label='Momentum Updates')\n",
    "plt.plot(rmsprop_updates[:], label='RMSProp Updates')\n",
    "plt.plot(adam_updates[:], label='ADAM Updates')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally pit these algorithms against one another to see how they behave on the actual loss functions' surface.\n",
    "\n",
    "The code below generates animations showing us the paths followed by each of the optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange (-10,10,1)\n",
    "y = np.arange (-10,12,1)\n",
    "\n",
    "x,y = np.meshgrid(x,y)\n",
    "\n",
    "z = 0.26*(x**2 + y**2) - 0.48*x*y\n",
    "\n",
    "fig= plt.figure(figsize = (5,5), dpi=300)\n",
    "\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "ax.set(xlim=[-3.5, 3.5], ylim=[-3.5, 3.5], xlabel='x', ylabel='y')\n",
    "ax.view_init(50,-130)\n",
    "# ax.set_box_aspect(None, zoom=3)\n",
    "gd_points = np.array([(point[0][0],point[0][1],point[1]) for point in gd])\n",
    "momentum_points = np.array([(point[0][0],point[0][1],point[1]) for point in momentum])\n",
    "rmsprop_points = np.array([(point[0][0],point[0][1],point[1]) for point in rmsprop])\n",
    "adam_points = np.array([(point[0][0],point[0][1],point[1]) for point in adam])\n",
    "ax.plot_surface(x,y,z, cmap=cm.Wistia, alpha =0.4)\n",
    "ax.scatter(0, 0, 0, color='black', s=10, marker='*', label='Minimum')\n",
    "gd_line, = ax.plot(gd_points[:,0],gd_points[:,1],gd_points[:,2], label = \"GD\", color=\"crimson\", zorder = 3, linewidth = 1, alpha = 0.6)\n",
    "momentum_line, = ax.plot(momentum_points[:,0],momentum_points[:,1],momentum_points[:,2], label = \"Momentum\", color=\"blue\", zorder = 2, linewidth = 1, alpha = 0.7)\n",
    "rmsprop_line, = ax.plot(rmsprop_points[:,0],rmsprop_points[:,1],rmsprop_points[:,2], label = \"RmsProp\", color=\"orange\", zorder = 1, linewidth = 1, alpha = 0.8)\n",
    "adam_line, = ax.plot(adam_points[:,0],adam_points[:,1],adam_points[:,2], label = \"ADAM\", color=\"green\", zorder = 1, linewidth = 1, alpha = 0.8)\n",
    "gd_value = ax.text2D(0.85, 0.97, f\"GD f(x) = {gd_points[0][2]}\", transform=ax.transAxes, ha='left', fontsize=5)\n",
    "momentum_value = ax.text2D(0.85, 0.94, f\"Momentum f(x) = {momentum_points[0][2]}\", transform=ax.transAxes, ha='left', fontsize=5)\n",
    "rmsprop_value = ax.text2D(0.85, 0.91, f\"RMSProp f(x) = {rmsprop_points[0][2]}\", transform=ax.transAxes, ha='left', fontsize=5)\n",
    "adam_value = ax.text2D(0.85, 0.88, f\"ADAM f(x) = {adam_points[0][2]}\", transform=ax.transAxes, ha='left', fontsize=5)\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "\n",
    "    gd_line.set_xdata(gd_points[:frame,0])\n",
    "    gd_line.set_ydata(gd_points[:frame,1])\n",
    "    gd_line.set_3d_properties(gd_points[:frame,2])\n",
    "    momentum_line.set_xdata(momentum_points[:frame,0])\n",
    "    momentum_line.set_ydata(momentum_points[:frame,1])\n",
    "    momentum_line.set_3d_properties(momentum_points[:frame,2])\n",
    "    rmsprop_line.set_xdata(rmsprop_points[:frame,0])\n",
    "    rmsprop_line.set_ydata(rmsprop_points[:frame,1])\n",
    "    rmsprop_line.set_3d_properties(rmsprop_points[:frame,2])\n",
    "    adam_line.set_xdata(adam_points[:frame,0])\n",
    "    adam_line.set_ydata(adam_points[:frame,1])\n",
    "    adam_line.set_3d_properties(adam_points[:frame,2])\n",
    "\n",
    "    gd_value.set_text( f\"GD f(x) = {gd_points[frame][2]:.4f}\")\n",
    "    momentum_value.set_text( f\"Momentum f(x) = {momentum_points[frame][2]:.4f}\")\n",
    "    rmsprop_value.set_text( f\"RMSProp f(x) = {rmsprop_points[frame][2]:.4f}\")\n",
    "    adam_value.set_text( f\"ADAM f(x) = {adam_points[frame][2]:.4f}\")\n",
    "\n",
    "    plt.legend(loc=3, fontsize=5)\n",
    "\n",
    "    return (gd_line, momentum_line, rmsprop_line, adam_line)\n",
    "\n",
    "anim = animation.FuncAnimation(fig=fig, func=update, frames=len(adam), interval=100)\n",
    "# anim.save(filename=\"../video/gradients_2.gif\", writer=\"pillow\")\n",
    "\n",
    "# plt.legend(loc=3, fontsize=5)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
